{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"batching_strategies/","title":"Batching Strategies","text":""},{"location":"batching_strategies/#1-why-batching-matters","title":"1. Why Batching Matters","text":"<p>GPU Utilization Problem:  - Single request uses &lt;10% of GPU compute capacity - Memory bandwidth underutilized - Expensive hardware sitting idle</p> <p>Batching Solution:  - Process multiple requests simultaneously - Amortize memory access costs - Achieve 5-10x throughput improvement</p> <p>Key Constraint: GPU memory limits batch size (primarily KV cache)</p>"},{"location":"batching_strategies/#2-static-batching","title":"2. Static Batching","text":""},{"location":"batching_strategies/#mechanism","title":"Mechanism","text":"<ul> <li>Accumulate N requests</li> <li>Process entire batch together</li> <li>Wait for ALL sequences to complete before accepting new requests</li> </ul>"},{"location":"batching_strategies/#characteristics","title":"Characteristics","text":"<pre><code>Batch: [Req1: 100 tokens, Req2: 20 tokens, Req3: 80 tokens]\nAll requests wait until Req1 (longest) completes\nGPU idle during Req2, Req3 completion\n</code></pre> <p>Throughput: Improved vs single request Latency: High variance (tail latency problem) GPU Utilization: Poor (bubbles after short sequences finish)</p>"},{"location":"batching_strategies/#when-to-use","title":"When to Use","text":"<ul> <li>Offline batch processing</li> <li>Known sequence lengths</li> <li>Latency not critical</li> </ul>"},{"location":"batching_strategies/#3-dynamic-batching","title":"3. Dynamic Batching","text":""},{"location":"batching_strategies/#mechanism_1","title":"Mechanism","text":"<ul> <li>Accumulate requests up to max_batch_size OR max_delay</li> <li>Whichever comes first triggers batch execution</li> <li>Still waits for full batch completion before next batch</li> </ul>"},{"location":"batching_strategies/#key-parameters","title":"Key Parameters","text":"<pre><code>max_batch_size = 32      # Maximum requests per batch\nmax_delay_ms = 100       # Maximum wait time\n</code></pre> <p>Improvement over Static: - Balances latency and throughput - Reduces wait time for batch formation</p> <p>Still Limited: - Batch-level scheduling (not iteration-level) - GPU idle time after short sequences complete</p>"},{"location":"batching_strategies/#when-to-use_1","title":"When to Use","text":"<ul> <li>Services with moderate traffic</li> <li>Simpler implementation than continuous batching</li> <li>Good for non-LLM models (CV, audio)</li> </ul>"},{"location":"batching_strategies/#4-continuous-batching-iteration-level-scheduling","title":"4. Continuous Batching (Iteration-Level Scheduling)","text":""},{"location":"batching_strategies/#core-innovation","title":"Core Innovation","text":"<p>Iteration-Level Scheduling: Schedule at each decode step, not batch level</p>"},{"location":"batching_strategies/#mechanism_2","title":"Mechanism","text":"<pre><code>Initial Batch: [Req1, Req2, Req3, Req4]\n\nIteration 1: All generate token 1\nIteration 2: All generate token 2\nIteration 3: Req2 completes \u2192 Req5 joins \u2192 [Req1, Req3, Req4, Req5]\nIteration 4: Req5, Req3 complete \u2192 Req6, Req7 join \u2192 [Req1, Req4, Req6, Req7]\n...\n</code></pre> <p>Key Benefit: No GPU idle time, slots filled immediately</p>"},{"location":"batching_strategies/#implementation-details","title":"Implementation Details","text":"<p>Sequence Completion Detection:  - Monitor for EOS tokens - Max length reached - User cancellation</p> <p>Slot Management:  - Free KV cache blocks immediately - Add waiting request to batch - Update attention masks</p> <p>Memory Efficiency:  - Works best with PagedAttention (vLLM) - Non-contiguous memory allocation - Independent per-sequence management</p>"},{"location":"batching_strategies/#performance-impact","title":"Performance Impact","text":"<ul> <li>Throughput: +20-30% vs dynamic batching</li> <li>Latency: Lower average, more consistent tail latency</li> <li>GPU Utilization: 80-95% (vs 50-70% for static)</li> </ul>"},{"location":"batching_strategies/#trade-offs","title":"Trade-offs","text":"<ul> <li>Complex implementation</li> <li>Variable batch size per iteration</li> <li>Requires sophisticated memory management</li> </ul>"},{"location":"batching_strategies/#5-chunked-prefill","title":"5. Chunked Prefill","text":""},{"location":"batching_strategies/#problem","title":"Problem","text":"<p>Long prompts block decode: <pre><code>Prompt: 10,000 tokens (prefill) \u2192 50 iterations\nShort prompts: waiting in queue\nDecode operations: starved\n</code></pre></p>"},{"location":"batching_strategies/#solution","title":"Solution","text":"<p>Break prefill into chunks, interleave with decode</p>"},{"location":"batching_strategies/#mechanism_3","title":"Mechanism","text":"<pre><code>Iteration 1: Chunk 1 of long prompt (512 tokens)\nIteration 2: Decode for ongoing requests\nIteration 3: Chunk 2 of long prompt (512 tokens)\nIteration 4: Decode for ongoing requests\n...\n</code></pre> <p>Parameters: - <code>max_prefill_tokens</code>: Tokens to prefill per iteration - Balances prefill throughput vs decode latency</p>"},{"location":"batching_strategies/#benefits","title":"Benefits","text":"<ul> <li>Prevents large prompts from causing latency spikes</li> <li>Better tail latency for decode operations</li> <li>More predictable service times</li> </ul>"},{"location":"batching_strategies/#implementation-challenges","title":"Implementation Challenges","text":"<ul> <li>Partial KV cache management</li> <li>Attention mask complexity</li> <li>Scheduling overhead</li> </ul>"},{"location":"batching_strategies/#6-speculative-batching","title":"6. Speculative Batching","text":""},{"location":"batching_strategies/#concept","title":"Concept","text":"<p>Combine speculative decoding with batching</p>"},{"location":"batching_strategies/#mechanism_4","title":"Mechanism","text":"<ol> <li>Draft Phase: Small model predicts k tokens for all requests in batch</li> <li>Verification Phase: Large model verifies all k tokens in single pass</li> <li>Accept/Reject: Keep correct tokens, retry from first error</li> </ol>"},{"location":"batching_strategies/#batch-level-optimization","title":"Batch-Level Optimization","text":"<pre><code>Without Speculation: 5 iterations for 5 tokens\nWith Speculation (k=5): 1 iteration (if all accepted)\nEffective Speedup: 2-3x for batch\n</code></pre>"},{"location":"batching_strategies/#challenges","title":"Challenges","text":"<ul> <li>Variable acceptance rates across requests</li> <li>Synchronization points</li> <li>Draft model overhead</li> </ul>"},{"location":"batching_strategies/#7-prefix-caching-in-batching","title":"7. Prefix Caching in Batching","text":""},{"location":"batching_strategies/#problem_1","title":"Problem","text":"<p>Repeated prefixes waste computation</p> <pre><code>User1: [System Prompt] + User Query 1\nUser2: [System Prompt] + User Query 2\nUser3: [System Prompt] + User Query 3\n</code></pre>"},{"location":"batching_strategies/#solution_1","title":"Solution","text":"<p>Share KV cache blocks for common prefixes</p>"},{"location":"batching_strategies/#automatic-prefix-caching-apc","title":"Automatic Prefix Caching (APC)","text":"<ul> <li>Hash prompt prefixes</li> <li>Reuse physical memory blocks</li> <li>Multiple requests point to same KV cache</li> </ul>"},{"location":"batching_strategies/#batch-level-benefits","title":"Batch-Level Benefits","text":"<pre><code>Without APC:\n  3 requests \u00d7 1000-token system prompt = 3000 tokens in KV cache\n\nWith APC:\n  3 requests share 1000-token KV cache = 1000 tokens total\n  Effective batch size: 3x larger\n</code></pre>"},{"location":"batching_strategies/#use-cases","title":"Use Cases","text":"<ul> <li>Multi-turn chat (common history)</li> <li>RAG (shared context documents)</li> <li>Agent frameworks (repeated tool descriptions)</li> </ul>"},{"location":"batching_strategies/#8-mixed-batch-scheduling","title":"8. Mixed Batch Scheduling","text":""},{"location":"batching_strategies/#challenge","title":"Challenge","text":"<p>Different request types have different characteristics:</p> <ul> <li>Prefill: Compute-bound, parallel</li> <li>Decode: Memory-bound, sequential</li> </ul>"},{"location":"batching_strategies/#naive-approach","title":"Naive Approach","text":"<p>Separate batches for prefill and decode \u2192 Underutilization</p>"},{"location":"batching_strategies/#optimized-approach","title":"Optimized Approach","text":"<p>Mixed batches with resource allocation: </p> <pre><code>GPU Resources:\n  80% compute \u2192 Prefill operations\n  20% memory bandwidth \u2192 Decode operations\n\nSingle iteration:\n  Process prefill chunks (compute-heavy)\n  Process decode steps (memory-heavy)\n  Overlap when possible\n</code></pre>"},{"location":"batching_strategies/#splitfuse-deepspeed-fastgen","title":"SplitFuse (DeepSpeed-FastGen)","text":"<p>Dynamic algorithm that: 1. Monitors compute vs memory utilization 2. Adjusts prefill chunk size 3. Balances prefill/decode in each iteration</p>"},{"location":"batching_strategies/#9-batch-size-selection","title":"9. Batch Size Selection","text":""},{"location":"batching_strategies/#factors","title":"Factors","text":"<p>Memory Constraint: </p> <pre><code>Available Memory = Model Weights + KV Cache + Activations\nKV Cache = batch_size \u00d7 seq_len \u00d7 kv_memory_per_token\n</code></pre> <p>Optimal Batch Size:  - Too small \u2192 GPU underutilized - Too large \u2192 OOM, increased latency</p>"},{"location":"batching_strategies/#heuristics","title":"Heuristics","text":"<p>For Decode: <pre><code>optimal_batch_size = GPU_memory / (model_size + max_seq_len \u00d7 kv_per_token)\n</code></pre></p> <p>For Prefill: <pre><code>Limited by compute, not memory\nLarger batches better (up to memory limit)\n</code></pre></p>"},{"location":"batching_strategies/#adaptive-batching","title":"Adaptive Batching","text":"<p>Monitor queue depth and adjust:</p> <ul> <li>High queue \u2192 Increase batch size (if memory allows)</li> <li>Low latency requirement \u2192 Decrease batch size</li> <li>Balance throughput vs latency SLO</li> </ul>"},{"location":"batching_strategies/#10-multi-query-batching-mqagqa-context","title":"10. Multi-Query Batching (MQA/GQA Context)","text":""},{"location":"batching_strategies/#relevance-to-batching","title":"Relevance to Batching","text":"<p>Multi-Query Attention (MQA): - Fewer KV heads \u2192 Smaller KV cache - Enables larger batch sizes</p> <p>Grouped-Query Attention (GQA): - Middle ground between MHA and MQA - Llama 3, Mistral use GQA</p>"},{"location":"batching_strategies/#impact","title":"Impact","text":"<pre><code>Llama 2 (MHA): 32 heads for K, 32 for V\nLlama 3 (GQA): 8 KV heads, 32 Q heads\n\nKV Cache Reduction: 4x\nBatch Size Increase: ~4x at same memory\n</code></pre>"},{"location":"batching_strategies/#11-interview-qa","title":"11. Interview Q&amp;A","text":"<p>Q: Why is continuous batching better than dynamic batching?  A: Dynamic batching schedules at batch level, causing GPU idle time when short sequences complete. Continuous batching schedules at iteration level, immediately filling freed slots with new requests. This eliminates bubbles and improves throughput by 20-30%.</p> <p>Q: What's the trade-off between batch size and latency?  A: Larger batches increase throughput but also increase latency per request (more compute per iteration, longer queue wait times). Optimal batch size depends on SLO requirements and whether you're optimizing for throughput or latency.</p> <p>Q: How does chunked prefill prevent latency spikes?  A: Without chunking, a 10k token prefill blocks the GPU for many iterations, starving decode operations. Chunked prefill breaks it into smaller pieces (e.g., 512 tokens) and interleaves with decode, keeping decode latency predictable.</p> <p>Q: Why does prefix caching improve effective batch size?  A: Common prefixes (system prompts, RAG contexts) are stored once in memory and shared across requests. If 10 requests share a 1k token prefix, you save 9k tokens of KV cache, allowing 9 more requests to fit in the same memory.</p> <p>Q: When would you use static batching instead of continuous batching?  A: Offline batch processing with known sequence lengths, where latency doesn't matter and implementation simplicity is valued. For production online serving, continuous batching is almost always better.</p> <p>Q: How does speculative decoding interact with batching?  A: Draft model generates k tokens for entire batch, then target model verifies all in parallel. Benefit multiplies across batch. Challenge is handling variable acceptance rates\u2014some sequences may accept all k tokens while others reject after first token.</p> <p>Q: What determines the optimal prefill chunk size?  A: Balance between prefill throughput and decode latency. Smaller chunks (256-512 tokens) minimize decode latency impact but reduce prefill efficiency. Larger chunks (1024-2048) maximize prefill throughput but can cause latency spikes. Typically tuned based on workload mix.</p> <p>Q: Why does continuous batching require PagedAttention or similar?  A: Variable batch sizes per iteration need dynamic memory allocation. Traditional contiguous allocation can't handle requests joining/leaving mid-batch efficiently. Paged memory allows independent per-sequence management without fragmentation.</p>"},{"location":"attention_optimization/flash_attention/","title":"Flash Attention","text":""},{"location":"attention_optimization/flash_attention/#1-overview","title":"1. Overview","text":"<p>FlashAttention is a fast and memory-efficient attention algorithm that computes exact attention without materializing the full \\(N \\times N\\) attention matrix. It's especially critical for long sequences (4k+ tokens) in modern LLMs.</p> <p>Key insight: The bottleneck in attention isn't compute\u2014it's memory bandwidth (moving data between GPU memory hierarchies).</p>"},{"location":"attention_optimization/flash_attention/#2-why-standard-attention-is-slow","title":"2. Why Standard Attention is Slow","text":"<p>Standard attention formula: $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$</p>"},{"location":"attention_optimization/flash_attention/#problem-1-quadratic-memory-growth","title":"Problem 1: Quadratic Memory Growth","text":"<p>For sequence length \\(N = 16{,}384\\) in FP16: - Attention matrix: \\(N^2 = 268M\\) elements - Memory: \\(268M \\times 2\\) bytes \\(\u2248 512\\) MB (per layer, per head!)</p>"},{"location":"attention_optimization/flash_attention/#problem-2-excessive-memory-traffic","title":"Problem 2: Excessive Memory Traffic","text":"<p>Standard attention performs multiple memory-heavy steps: 1. Compute \\(QK^T\\) \u2192 write to global memory 2. Read \\(QK^T\\) \u2192 apply softmax \u2192 write back 3. Read softmax output \u2192 compute with \\(V\\) \u2192 write output</p> <p>Result: GPUs become memory-bound, not compute-bound.</p>"},{"location":"attention_optimization/flash_attention/#problem-3-numerical-instability-in-fp16","title":"Problem 3: Numerical Instability in FP16","text":"<ul> <li>Large values in \\(QK^T\\) cause overflow in \\(e^x\\)</li> <li>Small values underflow to zero</li> <li>Standard attention often requires FP32, increasing memory usage</li> </ul>"},{"location":"attention_optimization/flash_attention/#3-how-flashattention-works","title":"3. How FlashAttention Works","text":"<p>FlashAttention uses three key techniques:</p>"},{"location":"attention_optimization/flash_attention/#31-tiling","title":"3.1 Tiling","text":"<p>Split Q, K, V into small tiles that fit in GPU shared memory (SRAM).</p> <p>Example: - Sequence length: \\(N = 16{,}384\\) - Tile size: \\(B = 128\\) - Memory per tile: \\(128 \\times 128 = 16{,}384\\) elements (vs. \\(268M\\) for full matrix)</p> <pre><code># Conceptual tiling\nfor q_tile in Q_tiles:\n    for k_tile, v_tile in zip(K_tiles, V_tiles):\n        partial_scores = q_tile @ k_tile.T\n        # accumulate incrementally\n</code></pre>"},{"location":"attention_optimization/flash_attention/#32-kernel-fusion","title":"3.2 Kernel Fusion","text":"<p>Fuse all operations into a single kernel to keep intermediate results in fast shared memory: 1. Matrix multiplication (\\(Q \\cdot K^T\\)) 2. Scaling (\\(1/\\sqrt{d}\\)) 3. Softmax 4. Weighted sum with \\(V\\)</p> <p>Standard attention writes/reads from global memory between each step. FlashAttention does everything in one pass.</p>"},{"location":"attention_optimization/flash_attention/#33-online-softmax","title":"3.3 Online Softmax","text":"<p>Compute softmax incrementally across tiles without storing the full attention matrix.</p> <p>Numerically stable approach: 1. Maintain running maximum \\(m\\) across tiles    - Compute: \\(e^{x_i - m}\\) (prevents overflow) 2. Maintain running sum of exponentials 3. Accumulate weighted output incrementally</p> <p>Example with 2 tiles:</p> <p>Tile 1: <code>[0.1, 0.5, 0.3]</code>, Tile 2: <code>[0.2, 0.4, 0.1]</code></p> <p>Processing: 1. Tile 1: \\(m = 0.5\\), shifted exps: \\([e^{-0.4}, e^{0}, e^{-0.2}]\\), running sum \\(s_1\\) 2. Tile 2: update \\(m\\), reweight previous results, add new exps, update sum \\(s_2\\) 3. Final: divide accumulated output by \\(s_2\\)</p> <p>Result: Exact same output as standard attention, but in FP16/BF16 without overflow.</p>"},{"location":"attention_optimization/flash_attention/#4-performance-impact","title":"4. Performance Impact","text":""},{"location":"attention_optimization/flash_attention/#memory-complexity","title":"Memory Complexity","text":"<ul> <li>Standard: \\(O(N^2)\\)</li> <li>FlashAttention: \\(O(N \\cdot B)\\) where \\(B\\) is tile size</li> </ul>"},{"location":"attention_optimization/flash_attention/#speedup","title":"Speedup","text":"<ul> <li>2\u20134x faster for long sequences on modern GPUs</li> <li>Enables 2\u20134x longer sequences or larger batch sizes</li> </ul>"},{"location":"attention_optimization/flash_attention/#usage","title":"Usage","text":"<pre><code>from flash_attn import flash_attn_func\n\n# q, k, v shape: (batch, seq_len, num_heads, head_dim)\noutput = flash_attn_func(q, k, v, dropout_p=0.0, causal=False)\n</code></pre>"},{"location":"attention_optimization/flash_attention/#when-flashattention-helps-most","title":"When FlashAttention Helps Most","text":"<p>\u2705 Long sequences (2k+ tokens) \u2705 FP16/BF16 precision \u2705 Modern NVIDIA GPUs with fast shared memory  </p> <p>\u274c Very short sequences \u274c CPU-based inference \u274c Custom attention patterns not supported by the kernels  </p>"},{"location":"attention_optimization/flash_attention/#5-interview-questions","title":"5. Interview Questions","text":""},{"location":"attention_optimization/flash_attention/#q1-why-is-flashattention-faster-than-standard-attention","title":"Q1: Why is FlashAttention faster than standard attention?","text":"<p>Answer: The bottleneck is memory bandwidth, not compute. Standard attention writes intermediate results (attention matrix, softmax output) to slow GPU global memory and reads them back multiple times. FlashAttention uses tiling and kernel fusion to keep all intermediate computations in fast shared memory, drastically reducing memory traffic.</p>"},{"location":"attention_optimization/flash_attention/#q2-does-flashattention-approximate-attention","title":"Q2: Does FlashAttention approximate attention?","text":"<p>Answer: No, it computes exact attention. It produces identical results to standard attention by using online softmax to correctly compute the softmax normalization across tiles without storing the full attention matrix.</p>"},{"location":"attention_optimization/flash_attention/#q3-explain-online-softmax-why-is-it-needed","title":"Q3: Explain online softmax. Why is it needed?","text":"<p>Answer: When processing tiles, we can't store the full \\(N \\times N\\) attention matrix. Online softmax maintains a running maximum and running sum across tiles to compute the exact softmax incrementally. This also provides numerical stability in FP16/BF16 by shifting scores before exponentiation to prevent overflow: \\(e^{x_i - m}\\) instead of \\(e^{x_i}\\).</p>"},{"location":"attention_optimization/flash_attention/#q4-what-is-the-memory-complexity-of-flashattention-vs-standard-attention","title":"Q4: What is the memory complexity of FlashAttention vs standard attention?","text":"<p>Answer: - Standard attention: \\(O(N^2)\\) to store full attention matrix - FlashAttention: \\(O(N \\cdot B)\\) where \\(B\\) is tile size (typically 128-256) - For \\(N=16k\\), this reduces memory from ~512MB to ~4-8MB per head</p>"},{"location":"attention_optimization/flash_attention/#q5-can-flashattention-be-used-for-any-attention-mechanism","title":"Q5: Can FlashAttention be used for any attention mechanism?","text":"<p>Answer: FlashAttention works best for standard scaled dot-product attention. Variants exist for: - Causal (autoregressive) attention \u2705 - Cross-attention \u2705 - Sparse attention patterns \u26a0\ufe0f (limited support, depends on sparsity structure)</p> <p>Custom attention patterns may require specialized kernels.</p>"},{"location":"attention_optimization/flash_attention/#q6-why-does-flashattention-require-modern-gpus","title":"Q6: Why does FlashAttention require modern GPUs?","text":"<p>Answer: FlashAttention relies on: 1. Fast shared memory (SRAM) - to store tiles and perform fused operations 2. High memory bandwidth - to maximize benefit from reduced memory traffic 3. Tensor cores - for fast matrix multiplications</p> <p>Older GPUs or CPUs don't have the same memory hierarchy, so the benefits are minimal.</p>"},{"location":"attention_optimization/flash_attention/#q7-walk-through-how-flashattention-processes-a-single-tile-pair","title":"Q7: Walk through how FlashAttention processes a single tile pair.","text":"<p>Answer: 1. Load \\(Q_{tile}\\) and \\(K_{tile}\\) into shared memory 2. Compute scores: \\(S = Q_{tile} \\cdot K_{tile}^T / \\sqrt{d}\\) 3. Track running max \\(m\\) for numerical stability 4. Compute: \\(e^{S - m}\\) (stays in shared memory) 5. Update running sum for normalization 6. Load \\(V_{tile}\\), compute weighted sum, accumulate to output 7. Move to next tile, repeat</p> <p>All intermediate values stay in fast SRAM, not global memory.</p>"},{"location":"attention_optimization/flash_attention/#q8-what-trade-offs-does-flashattention-make","title":"Q8: What trade-offs does FlashAttention make?","text":"<p>Answer: - \u2705 Gains: 2-4x speedup, drastically reduced memory - \u26a0\ufe0f Complexity: More complex implementation than standard attention - \u26a0\ufe0f Flexibility: Limited support for custom sparse attention patterns - \u26a0\ufe0f Hardware: Requires modern GPUs to realize full benefits</p> <p>The trade-offs are generally worth it for production LLM serving and training.</p>"},{"location":"attention_optimization/flash_attention/#q9-how-does-tiling-affect-the-computational-complexity","title":"Q9: How does tiling affect the computational complexity?","text":"<p>Answer: Tiling doesn't change the computational complexity (still \\(O(N^2)\\) FLOPs), but it changes the I/O complexity: - Standard: \\(O(N^2)\\) memory reads/writes - FlashAttention: \\(O(N^2/B)\\) memory reads/writes, where \\(B\\) is tile size</p> <p>Since memory bandwidth is the bottleneck, this provides significant speedup.</p>"},{"location":"attention_optimization/flash_attention/#q10-can-you-explain-the-difference-between-shared-memory-and-global-memory","title":"Q10: Can you explain the difference between shared memory and global memory?","text":"<p>Answer: - Shared memory (SRAM): Fast (~20 TB/s), small (~100 KB per SM), explicitly managed - Global memory (HBM): Slower (~1-2 TB/s), large (16-80 GB), high latency</p> <p>FlashAttention keeps working data in shared memory to minimize expensive global memory accesses. This is the key to its performance gains.</p>"},{"location":"attention_optimization/flash_attention/#6-key-takeaways-for-interviews","title":"6. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Avoid materializing the \\(N \\times N\\) attention matrix by using tiling and kernel fusion</li> <li>Core techniques: Tiling + Kernel Fusion + Online Softmax</li> <li>Why it's fast: Reduces memory bandwidth usage (the real bottleneck)</li> <li>Memory savings: \\(O(N^2) \u2192 O(N \\cdot B)\\)</li> <li>Exact computation: Not an approximation\u2014produces identical results to standard attention</li> <li>Numerical stability: Online softmax enables stable FP16/BF16 computation for long sequences</li> </ol>"},{"location":"attention_optimization/flash_attention_2/","title":"Flash Attention 2","text":""},{"location":"attention_optimization/flash_attention_2/#1-overview","title":"1. Overview","text":"<p>FlashAttention-2 is an improved version of FlashAttention that achieves 2x speedup over FlashAttention-1 through better GPU utilization. It maintains the same exact attention computation while being even faster and more efficient.</p> <p>Key improvement: Better work partitioning across GPU threads to reduce idle time and maximize hardware utilization.</p>"},{"location":"attention_optimization/flash_attention_2/#2-what-was-wrong-with-flashattention-1","title":"2. What Was Wrong with FlashAttention-1?","text":"<p>Despite being much faster than standard attention, FlashAttention-1 had suboptimal GPU utilization:</p>"},{"location":"attention_optimization/flash_attention_2/#problem-1-poor-work-partitioning","title":"Problem 1: Poor Work Partitioning","text":"<ul> <li>Each thread block processed one query tile across all key/value tiles</li> <li>Led to unbalanced workload and thread block idle time</li> <li>Didn't fully saturate GPU compute resources</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#problem-2-non-coalesced-memory-accesses","title":"Problem 2: Non-Coalesced Memory Accesses","text":"<ul> <li>Memory accesses weren't optimally aligned for GPU memory coalescing</li> <li>Caused unnecessary memory bandwidth waste</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#problem-3-limited-parallelism","title":"Problem 3: Limited Parallelism","text":"<ul> <li>Parallelism was only across batch, heads, and query sequence</li> <li>Didn't parallelize across key/value sequence dimension</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#3-key-improvements-in-flashattention-2","title":"3. Key Improvements in FlashAttention-2","text":""},{"location":"attention_optimization/flash_attention_2/#31-better-parallelism-strategy","title":"3.1 Better Parallelism Strategy","text":"<p>FlashAttention-1: Parallelize over <code>(batch, heads, query_tiles)</code> <pre><code>Thread Block 1 \u2192 processes Q_tile_1 across all K,V tiles\nThread Block 2 \u2192 processes Q_tile_2 across all K,V tiles\n</code></pre></p> <p>FlashAttention-2: Parallelize over <code>(batch, heads, query_tiles, kv_tiles)</code> <pre><code>Thread Block 1 \u2192 processes (Q_tile_1, K_tile_1, V_tile_1)\nThread Block 2 \u2192 processes (Q_tile_1, K_tile_2, V_tile_2)\nThread Block 3 \u2192 processes (Q_tile_2, K_tile_1, V_tile_1)\n</code></pre></p> <p>Benefit: More thread blocks doing work simultaneously \u2192 better GPU occupancy \u2192 less idle time</p>"},{"location":"attention_optimization/flash_attention_2/#32-improved-work-partitioning-within-thread-blocks","title":"3.2 Improved Work Partitioning Within Thread Blocks","text":"<p>FlashAttention-1: Each warp handled different queries within a tile - Led to imbalanced work when softmax required different amounts of computation</p> <p>FlashAttention-2: Each warp handles same query, split across K dimension - More balanced work distribution - Better load balancing across warps</p>"},{"location":"attention_optimization/flash_attention_2/#33-memory-access-optimizations","title":"3.3 Memory Access Optimizations","text":"<ul> <li>Improved memory coalescing patterns</li> <li>Better cache utilization</li> <li>Reduced redundant memory loads</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#4-performance-impact","title":"4. Performance Impact","text":""},{"location":"attention_optimization/flash_attention_2/#speedup-over-flashattention-1","title":"Speedup Over FlashAttention-1","text":"<ul> <li>~2x faster on average for typical sequence lengths</li> <li>Up to 2.3x on A100 GPUs for long sequences</li> <li>Better scaling with sequence length</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#gpu-utilization","title":"GPU Utilization","text":"<ul> <li>FlashAttention-1: ~35-50% of peak FLOPS</li> <li>FlashAttention-2: ~50-70% of peak FLOPS</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Same \\(O(N \\cdot B)\\) memory complexity</li> <li>Better bandwidth utilization due to improved access patterns</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#5-implementation-details","title":"5. Implementation Details","text":""},{"location":"attention_optimization/flash_attention_2/#thread-block-structure","title":"Thread Block Structure","text":"<pre><code># Conceptual partitioning\nfor batch_idx in batches:\n    for head_idx in heads:\n        for q_tile_idx in query_tiles:\n            for kv_tile_idx in kv_tiles:  # NEW: also parallelize here\n                # Each (q_tile, kv_tile) pair gets its own thread block\n                thread_block.process(Q[q_tile_idx], K[kv_tile_idx], V[kv_tile_idx])\n                # Accumulate partial results\n</code></pre>"},{"location":"attention_optimization/flash_attention_2/#synchronization","title":"Synchronization","text":"<ul> <li>Requires careful synchronization when accumulating partial outputs</li> <li>Uses atomic operations or reduction trees to combine results from different KV tiles</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#6-interview-questions","title":"6. Interview Questions","text":""},{"location":"attention_optimization/flash_attention_2/#q1-whats-the-main-difference-between-flashattention-1-and-flashattention-2","title":"Q1: What's the main difference between FlashAttention-1 and FlashAttention-2?","text":"<p>Answer: FlashAttention-2 improves parallelism by also parallelizing across the key/value sequence dimension, not just query sequence. This means more thread blocks work simultaneously, reducing idle time and achieving ~2x speedup while computing the exact same result.</p>"},{"location":"attention_optimization/flash_attention_2/#q2-does-flashattention-2-change-the-algorithm-or-just-the-implementation","title":"Q2: Does FlashAttention-2 change the algorithm or just the implementation?","text":"<p>Answer: It's purely an implementation improvement. The algorithm (tiling, online softmax, kernel fusion) remains the same. FlashAttention-2 just distributes work more efficiently across GPU threads to maximize hardware utilization.</p>"},{"location":"attention_optimization/flash_attention_2/#q3-why-does-parallelizing-across-kv-tiles-improve-performance","title":"Q3: Why does parallelizing across KV tiles improve performance?","text":"<p>Answer: In FlashAttention-1, each thread block processes one Q tile sequentially across all KV tiles. This limits parallelism. FlashAttention-2 launches separate thread blocks for each (Q_tile, KV_tile) pair, enabling many more blocks to run concurrently, better saturating the GPU's compute resources.</p>"},{"location":"attention_optimization/flash_attention_2/#q4-whats-the-trade-off-with-this-increased-parallelism","title":"Q4: What's the trade-off with this increased parallelism?","text":"<p>Answer: More synchronization overhead. Since multiple thread blocks now compute partial outputs for the same query tile (from different KV tiles), we need to carefully accumulate and normalize these partial results. However, the performance gain from parallelism far outweighs this cost.</p>"},{"location":"attention_optimization/flash_attention_2/#q5-how-does-flashattention-2-handle-the-accumulation-of-partial-results","title":"Q5: How does FlashAttention-2 handle the accumulation of partial results?","text":"<p>Answer: Each thread block computes a partial attention output for its (Q_tile, KV_tile) pair along with partial softmax statistics (max, sum). These partials are then combined using: - Atomic operations, or - Reduction trees, or - Final pass to accumulate stored partials</p> <p>The online softmax technique ensures correct normalization.</p>"},{"location":"attention_optimization/flash_attention_2/#q6-what-gpu-features-does-flashattention-2-rely-on-more-heavily","title":"Q6: What GPU features does FlashAttention-2 rely on more heavily?","text":"<p>Answer: - High thread block occupancy - needs many concurrent blocks - Fast atomic operations - for accumulating partials - Shared memory bandwidth - still crucial like FA-1 - Warp-level primitives - for efficient intra-block communication</p> <p>Modern GPUs (A100, H100) have better support for these, maximizing FA-2's benefits.</p>"},{"location":"attention_optimization/flash_attention_2/#q7-why-doesnt-flashattention-2-achieve-100-of-peak-flops","title":"Q7: Why doesn't FlashAttention-2 achieve 100% of peak FLOPS?","text":"<p>Answer: Several factors: - Memory bandwidth still matters (can't fully hide all memory latency) - Synchronization overhead from accumulating partials - Load imbalance across thread blocks (some finish before others) - Non-uniform work per tile (softmax computation varies)</p> <p>50-70% utilization is actually quite good for memory-intensive operations.</p>"},{"location":"attention_optimization/flash_attention_2/#q8-how-does-sequence-length-affect-fa-2s-speedup-over-fa-1","title":"Q8: How does sequence length affect FA-2's speedup over FA-1?","text":"<p>Answer: FlashAttention-2's advantage increases with sequence length: - Longer sequences \u2192 more tiles \u2192 more parallelism opportunities - Better amortization of synchronization overhead - At very short sequences, FA-1 and FA-2 are similar (not enough parallelism to exploit)</p>"},{"location":"attention_optimization/flash_attention_2/#q9-can-flashattention-2s-techniques-be-applied-to-other-operations","title":"Q9: Can FlashAttention-2's techniques be applied to other operations?","text":"<p>Answer: Yes! The key insight\u2014parallelizing over both input and computation dimensions\u2014applies to any operation that processes tiles/blocks: - Other attention variants (sparse, local attention) - Convolutions with tiling - Matrix multiplications with block processing</p> <p>The principle is: maximize parallel work to reduce GPU idle time.</p>"},{"location":"attention_optimization/flash_attention_2/#q10-whats-the-memory-complexity-of-flashattention-2","title":"Q10: What's the memory complexity of FlashAttention-2?","text":"<p>Answer: Same as FlashAttention-1: \\(O(N \\cdot B)\\) where \\(B\\) is tile size. The improvement is in speed (better parallelism and memory access patterns), not memory usage. Both avoid materializing the full \\(N \\times N\\) attention matrix.</p>"},{"location":"attention_optimization/flash_attention_2/#7-flashattention-1-vs-flashattention-2-summary","title":"7. FlashAttention-1 vs FlashAttention-2 Summary","text":"Aspect FlashAttention-1 FlashAttention-2 Parallelism Over batch, heads, query tiles Over batch, heads, query tiles, KV tiles Work per block One Q tile \u00d7 all KV tiles One (Q tile, KV tile) pair GPU utilization 35-50% of peak FLOPS 50-70% of peak FLOPS Speedup vs standard 2-4x 4-8x Speedup vs FA-1 - ~2x Memory complexity \\(O(N \\cdot B)\\) \\(O(N \\cdot B)\\) Algorithm Tiling + online softmax + fusion Same Synchronization Simpler More complex (atomic/reduction)"},{"location":"attention_optimization/flash_attention_2/#8-key-takeaways-for-interviews","title":"8. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Same algorithm, better parallelism by also parallelizing across KV dimension</li> <li>Performance: ~2x faster than FA-1 through better GPU utilization</li> <li>Trade-off: More synchronization overhead, but worth it for the speedup</li> <li>Memory: Same \\(O(N \\cdot B)\\) complexity, just faster execution</li> <li>When it matters most: Long sequences where more parallelism can be exploited</li> <li>Hardware dependency: Benefits scale with GPU's ability to run many thread blocks concurrently</li> </ol>"},{"location":"attention_optimization/kv_caching/","title":"KV Caching","text":""},{"location":"attention_optimization/kv_caching/#1-self-attention-recap","title":"\ud83d\udce61. Self Attention Recap","text":"<p>Given hidden states \\(X \\in \\mathbb{R}^{T \\times d}\\):</p> \\[ Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V \\] <p>Per head attention:</p> \\[ \\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V \\] <p>Autoregressive decoding generates one token at a time with causal masking.</p>"},{"location":"attention_optimization/kv_caching/#2-why-kv-cache-is-needed","title":"\ud83d\udce62. Why KV Cache Is Needed","text":"<p>At decoding step \\(t\\), keys and values for tokens \\(1 \\ldots t-1\\) are unchanged but would be recomputed without caching.</p> <p>This repeated computation dominates inference latency and wastes FLOPs.</p>"},{"location":"attention_optimization/kv_caching/#3-kv-cache-mechanism","title":"\ud83d\udce63. KV Cache Mechanism","text":"<p>For each transformer layer \\(\\ell\\):</p> \\[ \\text{KVCache}_\\ell = \\{K_\\ell^{1:t}, V_\\ell^{1:t}\\} \\] <p>At decoding step \\(t\\):</p> <ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to the cache</li> <li>Attend over all cached keys and values</li> </ul> \\[ \\text{Attn}_t = \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Only the current token requires new computation.</p>"},{"location":"attention_optimization/kv_caching/#4-toy-example","title":"\ud83d\udce64. Toy Example","text":"<p>Prompt: \"I like neural\"</p> <p>Step 1: generate <code>\"networks\"</code></p> <ul> <li>Compute and cache keys and values for the prompt</li> <li>Attend to all cached tokens</li> </ul> <p>Step 2: generate <code>\"models\"</code></p> <ul> <li>Reuse cached keys and values</li> <li>Compute keys and values only for <code>\"networks\"</code></li> </ul> <p>Previously generated tokens are never recomputed.</p>"},{"location":"attention_optimization/kv_caching/#5-complexity-analysis","title":"\ud83d\udce65. Complexity Analysis","text":""},{"location":"attention_optimization/kv_caching/#notation","title":"Notation","text":"<ul> <li>\\(T\\): number of generated tokens</li> <li>\\(L\\): number of transformer layers</li> <li>\\(H\\): number of attention heads</li> <li>\\(d_h\\): head dimension</li> </ul>"},{"location":"attention_optimization/kv_caching/#without-kv-cache","title":"Without KV Cache","text":"<p>At each decoding step, attention is recomputed for all previous tokens:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^3) \\]"},{"location":"attention_optimization/kv_caching/#with-kv-cache","title":"With KV Cache","text":"<p>Only attention against cached keys and values is computed:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^2) \\] <p>KV caching removes one full factor of \\(T\\) from decoding complexity.</p>"},{"location":"attention_optimization/kv_caching/#6-memory-cost","title":"\ud83d\udce66. Memory Cost","text":"<p>Each layer stores:</p> \\[ K, V \\in \\mathbb{R}^{H \\times T \\times d_h} \\] <p>Total KV cache memory across all layers:</p> \\[ O(L \\cdot H \\cdot T \\cdot d_h) \\] <p>For long context inference, KV cache memory is often the dominant bottleneck.</p>"},{"location":"attention_optimization/kv_caching/#7-inference-vs-training-usage","title":"\ud83d\udce67. Inference v/s Training Usage","text":""},{"location":"attention_optimization/kv_caching/#71-during-inference","title":"7.1 During Inference","text":"<p>This is the most common and important usage.</p> <p>Inference Workflow</p> <ul> <li>Encode prompt</li> <li>Initialize empty KV cache per layer</li> <li>For each generated token:<ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to cache</li> </ul> </li> <li>Compute attention using cached tensors</li> </ul> <p>Practical Benefits</p> <ul> <li>Faster decoding</li> <li>Lower FLOPs</li> <li>Enables long context generation</li> <li>Essential for streaming and chat systems</li> </ul>"},{"location":"attention_optimization/kv_caching/#72-during-training","title":"7.2 During Training","text":"<p>KV caching is not used in standard full sequence training.</p> <p>Why?</p> <ul> <li>Training processes full sequences in parallel</li> <li>All tokens attend to each other simultaneously</li> <li>No repeated computation across steps</li> </ul>"},{"location":"attention_optimization/kv_caching/#8-scaling-kv-cache-for-long-context","title":"\ud83d\udce68. Scaling KV Cache for Long Context","text":"<p>Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.</p>"},{"location":"attention_optimization/kv_caching/#81-sliding-window-attention","title":"8.1 Sliding Window Attention","text":"<p>Only retain keys and values for the most recent \\(W\\) tokens:</p> \\[ K_{t-W:t}, V_{t-W:t} \\] <p>This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.</p>"},{"location":"attention_optimization/kv_caching/#82-kv-cache-quantization","title":"8.2 KV Cache Quantization","text":"<p>KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.</p>"},{"location":"attention_optimization/kv_caching/#what-gets-quantized","title":"What Gets Quantized","text":"<p>Both keys and values can be quantized, but they have different sensitivity:</p> <ul> <li>Keys (K) directly affect attention scores \\(QK^T\\)</li> <li>Values (V) affect the weighted sum after softmax</li> </ul> <p>As a result:</p> <ul> <li>Keys usually require higher precision</li> <li>Values tolerate more aggressive quantization</li> </ul>"},{"location":"attention_optimization/kv_caching/#common-quantization-schemes","title":"Common Quantization Schemes","text":"Component Typical Format Notes Keys FP16 / BF16 Preserves attention score stability Values INT8 Large memory reduction with minimal quality loss Both INT8 or INT4 Used for extreme long context scenarios <p>Mixed precision KV cache is widely used in practice.</p>"},{"location":"attention_optimization/kv_caching/#quantization-granularity","title":"Quantization Granularity","text":"<p>KV cache quantization can be applied at different levels:</p> <ul> <li>Per tensor: One scale for entire K or V tensor</li> <li>Per head: Separate scale per attention head</li> <li>Per channel: Separate scale per head dimension</li> </ul> <p>Finer granularity improves accuracy but increases metadata and compute overhead.</p>"},{"location":"attention_optimization/kv_caching/#dequantization-during-attention","title":"Dequantization During Attention","text":"<p>At decoding step \\(t\\):</p> <ol> <li>Load quantized \\(K, V\\) from cache</li> <li>Dequantize to FP16 or BF16</li> <li>Compute attention normally:</li> </ol> \\[ \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Dequantization cost is small compared to memory bandwidth savings.</p>"},{"location":"attention_optimization/kv_caching/#impact-on-performance","title":"Impact on Performance","text":"<p>Benefits:</p> <ul> <li>2x to 4x KV memory reduction</li> <li>Higher batch size and longer context</li> <li>Improved inference throughput due to reduced memory traffic</li> </ul> <p>Tradeoffs:</p> <ul> <li>Slight loss in generation quality</li> <li>Additional dequantization overhead</li> </ul> <p>In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.</p>"},{"location":"attention_optimization/kv_caching/#interaction-with-other-optimizations","title":"Interaction with Other Optimizations","text":"<ul> <li>GQA further reduces KV cache size and works well with quantization</li> <li>Paged KV cache benefits from smaller KV blocks</li> <li>FlashAttention amortizes dequantization overhead inside fused kernels</li> </ul>"},{"location":"attention_optimization/kv_caching/#83-prefix-caching","title":"8.3 Prefix Caching","text":"<p>When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.</p>"},{"location":"attention_optimization/kv_caching/#84-paged-kv-cache","title":"8.4 Paged KV Cache","text":"<p>KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.</p>"},{"location":"attention_optimization/kv_caching/#9-grouped-query-attention-gqa","title":"\ud83d\udce69. Grouped Query Attention (GQA)","text":"<p>Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.</p>"},{"location":"attention_optimization/kv_caching/#91-head-configuration","title":"9.1 Head Configuration","text":"\\[ H_q &gt; H_k = H_v \\] <p>Example:</p> <ul> <li>Query heads \\(H_q = 32\\)</li> <li>Key value heads \\(H_k = 8\\)</li> </ul> <p>This reduces KV cache memory by a factor of \\(H_q / H_k\\).</p>"},{"location":"attention_optimization/kv_caching/#92-qk-computation-with-mismatched-heads","title":"9.2 QK Computation with Mismatched Heads","text":"<p>Each key value head is shared by a fixed group of query heads.</p> <p>Let:</p> \\[ g = \\frac{H_q}{H_k} \\] <p>Each key value head serves \\(g\\) query heads.</p> <p>For query head \\(i\\), the corresponding key value head index is:</p> \\[ \\left\\lfloor \\frac{i}{g} \\right\\rfloor \\] <p>The attention computation becomes:</p> \\[ \\text{Attn}_i = \\text{softmax}\\left(\\frac{Q_i K_{\\left\\lfloor i/g \\right\\rfloor}^T}{\\sqrt{d_h}}\\right)V_{\\left\\lfloor i/g \\right\\rfloor} \\] <p>Keys and values are reused directly without additional projection or averaging.</p>"},{"location":"attention_optimization/kv_caching/#93-why-gqa-is-effective","title":"9.3 Why GQA Is Effective","text":"<ul> <li>Query heads retain expressive power</li> <li>Keys and values capture shared context</li> <li>KV cache size and memory bandwidth are significantly reduced</li> </ul> <p>GQA is widely used in production LLMs.</p>"},{"location":"attention_optimization/kv_caching/#10-other-common-optimizations","title":"\ud83d\udce610. Other Common Optimizations","text":""},{"location":"attention_optimization/kv_caching/#flashattention","title":"FlashAttention","text":"<p>FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.</p>"},{"location":"attention_optimization/kv_caching/#chunked-prefill","title":"Chunked Prefill","text":"<p>Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.</p>"},{"location":"attention_optimization/kv_caching/#speculative-decoding","title":"Speculative Decoding","text":"<p>Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.</p>"},{"location":"attention_optimization/paged_attention/","title":"Paged Attention","text":""},{"location":"attention_optimization/paged_attention/#1-overview","title":"1. Overview","text":"<p>PagedAttention is a memory management technique for efficient LLM serving that stores KV cache in non-contiguous memory blocks (pages). It's the core innovation behind vLLM, enabling near-zero waste in KV cache memory and higher throughput.</p> <p>Key insight: Treat KV cache like virtual memory in operating systems\u2014use paging to eliminate fragmentation and enable flexible memory sharing.</p>"},{"location":"attention_optimization/paged_attention/#2-the-kv-cache-memory-problem","title":"2. The KV Cache Memory Problem","text":""},{"location":"attention_optimization/paged_attention/#what-is-kv-cache","title":"What is KV Cache?","text":"<p>In autoregressive generation, transformers reuse Key and Value tensors from previous tokens: - Without cache: Recompute K, V for all previous tokens at each step (wasteful) - With cache: Store K, V tensors and only compute for new token</p> <p>For a sequence of length \\(N\\) with \\(L\\) layers, \\(H\\) heads, and dimension \\(d\\): $$ \\text{KV cache size} = 2 \\times N \\times L \\times H \\times d $$</p> <p>Example: LLaMA-13B with 2048 tokens \u2248 800 MB per sequence</p>"},{"location":"attention_optimization/paged_attention/#problems-with-traditional-kv-cache","title":"Problems with Traditional KV Cache","text":""},{"location":"attention_optimization/paged_attention/#problem-1-memory-fragmentation","title":"Problem 1: Memory Fragmentation","text":"<p>Issue: Must pre-allocate contiguous memory for maximum sequence length</p> <pre><code>Sequence 1: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591] (800 tokens, allocated for 2048)\nSequence 2: [\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] (300 tokens, allocated for 2048)\nSequence 3: [\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591] (600 tokens, allocated for 2048)\n</code></pre> <ul> <li>Actual usage: 1700 tokens</li> <li>Allocated: 6144 slots (3 \u00d7 2048)</li> <li>Waste: 72% of memory unused!</li> </ul>"},{"location":"attention_optimization/paged_attention/#problem-2-no-memory-sharing","title":"Problem 2: No Memory Sharing","text":"<ul> <li>Cannot share KV cache across sequences (even with identical prompts)</li> <li>Parallel sampling requires duplicating entire cache</li> <li>Beam search creates multiple copies</li> </ul>"},{"location":"attention_optimization/paged_attention/#problem-3-static-allocation","title":"Problem 3: Static Allocation","text":"<ul> <li>Must allocate for worst case (max sequence length)</li> <li>Can't dynamically adjust based on actual needs</li> <li>Limits batch size and throughput</li> </ul>"},{"location":"attention_optimization/paged_attention/#3-how-pagedattention-works","title":"3. How PagedAttention Works","text":""},{"location":"attention_optimization/paged_attention/#31-core-concept-paging","title":"3.1 Core Concept: Paging","text":"<p>Divide KV cache into fixed-size blocks (pages), similar to OS virtual memory:</p> <pre><code>Logical sequence: [Token 0, Token 1, ..., Token N]\n                         \u2193\nPhysical memory:  [Block 0] \u2192 [Block 5] \u2192 [Block 2] (non-contiguous)\n</code></pre> <p>Key properties: - Block size: Typically 16-64 tokens - Blocks can be anywhere in physical memory - Mapping tracked via block table (like OS page table)</p>"},{"location":"attention_optimization/paged_attention/#32-block-table","title":"3.2 Block Table","text":"<p>Each sequence has a block table mapping logical blocks to physical blocks:</p> <pre><code>Sequence 1:\n  Logical Block 0 \u2192 Physical Block 3\n  Logical Block 1 \u2192 Physical Block 7\n  Logical Block 2 \u2192 Physical Block 1\n\nSequence 2:\n  Logical Block 0 \u2192 Physical Block 3  (shared with Seq 1!)\n  Logical Block 1 \u2192 Physical Block 9\n</code></pre>"},{"location":"attention_optimization/paged_attention/#33-dynamic-allocation","title":"3.3 Dynamic Allocation","text":"<p>Blocks are allocated on-demand as sequences grow:</p> <pre><code># Conceptual allocation\ndef generate_token(sequence):\n    if sequence.last_block_is_full():\n        new_block = allocate_free_block()\n        sequence.block_table.append(new_block)\n\n    # Compute attention using block table\n    output = paged_attention(Q, sequence.block_table)\n    return output\n</code></pre> <p>Benefits: - Only allocate what's actually used - No pre-allocation for max length - Memory freed immediately when sequence completes</p>"},{"location":"attention_optimization/paged_attention/#34-memory-sharing-via-copy-on-write","title":"3.4 Memory Sharing via Copy-on-Write","text":"<p>Multiple sequences can share blocks (read-only):</p> <pre><code>Prompt: \"Translate to French: \"\n         \u2193\n[Block 0: \"Translate to French: \"] \u2190 Shared by all sequences\n\nSeq 1: [Block 0] \u2192 [Block 3: \"Hello \u2192 \"]\nSeq 2: [Block 0] \u2192 [Block 5: \"Goodbye \u2192 \"]\n</code></pre> <p>When modifying a shared block \u2192 copy-on-write: 1. Allocate new physical block 2. Copy contents 3. Update block table 4. Modify the copy</p>"},{"location":"attention_optimization/paged_attention/#4-attention-computation-with-paging","title":"4. Attention Computation with Paging","text":"<p>Standard attention accesses KV cache contiguously. PagedAttention accesses via block table:</p> <pre><code># Simplified PagedAttention\ndef paged_attention(Q, block_table, K_blocks, V_blocks):\n    output = 0\n    for logical_idx, physical_idx in enumerate(block_table):\n        # Fetch K, V from physical block\n        K_block = K_blocks[physical_idx]\n        V_block = V_blocks[physical_idx]\n\n        # Compute attention for this block\n        scores = Q @ K_block.T / sqrt(d)\n        attn = softmax(scores)\n        output += attn @ V_block\n\n    return output\n</code></pre> <p>Key insight: The indirection (block table lookup) has minimal overhead compared to memory savings.</p>"},{"location":"attention_optimization/paged_attention/#5-performance-impact","title":"5. Performance Impact","text":""},{"location":"attention_optimization/paged_attention/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Traditional: 20-40% KV cache utilization (60-80% waste)</li> <li>PagedAttention: 80-95% utilization (5-20% waste)</li> <li>2-3x more sequences in same memory</li> </ul>"},{"location":"attention_optimization/paged_attention/#throughput-improvement","title":"Throughput Improvement","text":"<ul> <li>vLLM with PagedAttention: 2-4x higher throughput vs traditional serving</li> <li>Batch size limited by memory \u2192 bigger batches with less waste</li> </ul>"},{"location":"attention_optimization/paged_attention/#latency","title":"Latency","text":"<ul> <li>Minimal overhead from block table lookups (&lt;5%)</li> <li>Often better latency due to higher batch efficiency</li> </ul>"},{"location":"attention_optimization/paged_attention/#6-interview-questions","title":"6. Interview Questions","text":""},{"location":"attention_optimization/paged_attention/#q1-what-problem-does-pagedattention-solve","title":"Q1: What problem does PagedAttention solve?","text":"<p>Answer: PagedAttention solves memory fragmentation and waste in KV cache management. Traditional approaches pre-allocate contiguous memory for max sequence length, wasting 60-80% of memory. PagedAttention uses non-contiguous blocks allocated on-demand, achieving 80-95% utilization and enabling 2-4x higher throughput.</p>"},{"location":"attention_optimization/paged_attention/#q2-how-is-pagedattention-similar-to-os-virtual-memory","title":"Q2: How is PagedAttention similar to OS virtual memory?","text":"<p>Answer: Both use paging: - Virtual memory: Maps virtual addresses to physical pages via page table - PagedAttention: Maps logical KV cache positions to physical blocks via block table</p> <p>Both enable non-contiguous allocation, on-demand paging, and copy-on-write sharing.</p>"},{"location":"attention_optimization/paged_attention/#q3-whats-a-typical-block-size-and-why","title":"Q3: What's a typical block size and why?","text":"<p>Answer: Typically 16-64 tokens. Trade-offs: - Too small: High block table overhead, more lookups during attention - Too large: Internal fragmentation (wasted space within partially-filled blocks) - Sweet spot: 16-64 balances overhead vs. fragmentation (similar to OS page sizes like 4KB)</p>"},{"location":"attention_optimization/paged_attention/#q4-how-does-pagedattention-enable-memory-sharing","title":"Q4: How does PagedAttention enable memory sharing?","text":"<p>Answer: Multiple sequences can point to the same physical blocks (read-only). Common use cases: - Shared prompts: All sequences share blocks containing the same prompt - Parallel sampling: Multiple outputs from same prompt share prefix blocks - Beam search: Different beams share common prefix</p> <p>When a shared block needs modification \u2192 copy-on-write: allocate new block, copy contents, update that sequence's block table.</p>"},{"location":"attention_optimization/paged_attention/#q5-whats-the-overhead-of-block-table-lookups","title":"Q5: What's the overhead of block table lookups?","text":"<p>Answer: Minimal (&lt;5% typically) because: - Block tables are small (fits in cache) - Lookups are simple integer indexing - Attention computation dominates (matrix ops on blocks) - Modern GPUs handle indirection efficiently</p> <p>The memory savings far outweigh this small overhead.</p>"},{"location":"attention_optimization/paged_attention/#q6-how-does-pagedattention-improve-throughput","title":"Q6: How does PagedAttention improve throughput?","text":"<p>Answer: By reducing memory waste: 1. Traditional: Can fit 10 sequences (60% waste) 2. PagedAttention: Can fit 25 sequences (10% waste) in same memory 3. Bigger batches \u2192 better GPU utilization \u2192 higher throughput</p> <p>Typical improvement: 2-4x more requests/second.</p>"},{"location":"attention_optimization/paged_attention/#q7-what-happens-when-we-run-out-of-physical-blocks","title":"Q7: What happens when we run out of physical blocks?","text":"<p>Answer: Memory management strategies: - Preemption: Evict lower-priority sequences, save their state - Swapping: Move blocks to CPU memory (like OS swap) - Recomputation: Drop blocks and recompute if needed - Blocking: Wait until blocks free up</p> <p>vLLM typically uses preemption for fairness and efficiency.</p>"},{"location":"attention_optimization/paged_attention/#q8-can-pagedattention-work-with-flashattention","title":"Q8: Can PagedAttention work with FlashAttention?","text":"<p>Answer: Yes! They're complementary: - FlashAttention: Optimizes attention computation (tiling, kernel fusion) - PagedAttention: Optimizes KV cache memory management (paging, sharing)</p> <p>You can use both together: FlashAttention for fast computation, PagedAttention for efficient memory. vLLM does exactly this.</p>"},{"location":"attention_optimization/paged_attention/#q9-whats-the-difference-between-block-size-and-tile-size","title":"Q9: What's the difference between block size and tile size?","text":"<p>Answer: - Block size (PagedAttention): Memory management granularity (16-64 tokens)   - Determines allocation unit for KV cache storage - Tile size (FlashAttention): Computation granularity (128-256 tokens)   - Determines how much data loads into shared memory at once</p> <p>They're independent concepts operating at different levels (memory management vs computation).</p>"},{"location":"attention_optimization/paged_attention/#q10-what-are-the-limitations-of-pagedattention","title":"Q10: What are the limitations of PagedAttention?","text":"<p>Answer: - Complexity: More complex implementation than contiguous allocation - Indirection overhead: Small cost from block table lookups - GPU kernel changes: Requires custom attention kernels that understand block tables - Internal fragmentation: Last block in sequence may be partially empty</p> <p>Despite these, benefits (2-4x throughput) far outweigh costs for LLM serving.</p>"},{"location":"attention_optimization/paged_attention/#7-pagedattention-in-practice-vllm","title":"7. PagedAttention in Practice (vLLM)","text":""},{"location":"attention_optimization/paged_attention/#key-features","title":"Key Features","text":"<pre><code># vLLM with PagedAttention\nfrom vllm import LLM\n\nllm = LLM(model=\"meta-llama/Llama-2-7b\")\n\n# Automatic memory management\noutputs = llm.generate(prompts, sampling_params)\n# - Blocks allocated on-demand\n# - Shared prompts reuse blocks\n# - Memory freed automatically\n</code></pre>"},{"location":"attention_optimization/paged_attention/#use-cases-where-it-shines","title":"Use Cases Where It Shines","text":"<p>\u2705 High-throughput serving (many concurrent requests) \u2705 Long sequences (less pre-allocation waste) \u2705 Parallel sampling / beam search (shared prefixes) \u2705 Shared system prompts across requests  </p> <p>\u274c Single-sequence inference (no sharing benefits) \u274c Very short sequences (overhead not amortized)  </p>"},{"location":"attention_optimization/paged_attention/#8-key-takeaways-for-interviews","title":"8. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Treat KV cache like OS virtual memory\u2014use paging for efficient, flexible allocation</li> <li>Problem solved: Memory fragmentation (60-80% waste \u2192 5-20% waste)</li> <li>Mechanism: Block table maps logical positions to physical memory blocks</li> <li>Sharing: Copy-on-write enables multiple sequences to share read-only blocks</li> <li>Performance: 2-4x throughput improvement in LLM serving</li> <li>Complementary: Works alongside FlashAttention (computation vs memory optimization)</li> </ol>"},{"location":"attention_optimization/paged_attention/#9-comparison-table","title":"9. Comparison Table","text":"Aspect Traditional KV Cache PagedAttention Allocation Contiguous, pre-allocated Non-contiguous, on-demand Memory waste 60-80% 5-20% Max sequences Limited by pre-allocation 2-4x more in same memory Sharing No sharing Copy-on-write sharing Complexity Simple More complex Overhead None &lt;5% (block lookups) Throughput Baseline 2-4x higher"},{"location":"decoding_strategies/beam_search/","title":"Beam Search - Interview Prep Guide","text":""},{"location":"decoding_strategies/beam_search/#1-overview","title":"1. Overview","text":"<p>Beam search maintains K candidate sequences (beams) at each decoding step and selects the sequence with the highest cumulative probability. It balances between greedy decoding (K=1) and exhaustive search (K=vocab_size).</p> <p>Key insight: Explore multiple promising paths simultaneously to avoid getting stuck in locally optimal but globally suboptimal sequences.</p>"},{"location":"decoding_strategies/beam_search/#2-how-it-works","title":"2. How It Works","text":""},{"location":"decoding_strategies/beam_search/#algorithm","title":"Algorithm","text":"<pre><code>def beam_search(model, prompt, beam_width=5, max_length=50):\n    # Initialize: One beam with the prompt\n    beams = [(prompt, 0.0)]  # (sequence, cumulative_log_prob)\n\n    for step in range(max_length):\n        candidates = []\n\n        # Expand each beam\n        for seq, score in beams:\n            if seq[-1] == EOS:  # Completed sequence\n                candidates.append((seq, score))\n                continue\n\n            logits = model(seq)\n            probs = softmax(logits)\n\n            # Consider all tokens\n            for token_id, prob in enumerate(probs):\n                new_seq = seq + [token_id]\n                new_score = score + log(prob)  # Cumulative log probability\n                candidates.append((new_seq, new_score))\n\n        # Keep top K beams\n        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Stop if all beams are complete\n        if all(seq[-1] == EOS for seq, _ in beams):\n            break\n\n    return beams[0][0]  # Return best sequence\n</code></pre>"},{"location":"decoding_strategies/beam_search/#core-concepts","title":"Core Concepts","text":"<p>Beam width (K): - K=1: Greedy decoding - K=5-10: Typical for translation - K=50+: Exhaustive (expensive)</p> <p>Cumulative log probability: - Use log probabilities to avoid numerical underflow - Score = log(P\u2081) + log(P\u2082) + ... + log(P\u2099) - Equivalent to log(P\u2081 \u00d7 P\u2082 \u00d7 ... \u00d7 P\u2099)</p>"},{"location":"decoding_strategies/beam_search/#3-example-walkthrough","title":"3. Example Walkthrough","text":"<p>Prompt: \"The cat sat on the\"</p> <p>Beam width: 3</p>"},{"location":"decoding_strategies/beam_search/#step-1-first-token","title":"Step 1: First Token","text":"<p>Model probabilities: | Token | Probability | Log Prob | |-------|-------------|----------| | mat   | 0.40        | -0.92    | | floor | 0.25        | -1.39    | | sofa  | 0.15        | -1.90    | | bed   | 0.10        | -2.30    |</p> <p>Top 3 beams: 1. \"The cat sat on the mat\" \u2192 score: -0.92 2. \"The cat sat on the floor\" \u2192 score: -1.39 3. \"The cat sat on the sofa\" \u2192 score: -1.90</p>"},{"location":"decoding_strategies/beam_search/#step-2-second-token","title":"Step 2: Second Token","text":"<p>Expand each beam:</p> <p>Beam 1: \"mat\" + next token - \"mat .\" \u2192 -0.92 + (-0.51) = -1.43 - \"mat and\" \u2192 -0.92 + (-1.61) = -2.53</p> <p>Beam 2: \"floor\" + next token - \"floor .\" \u2192 -1.39 + (-0.36) = -1.75 - \"floor while\" \u2192 -1.39 + (-1.20) = -2.59</p> <p>Beam 3: \"sofa\" + next token - \"sofa .\" \u2192 -1.90 + (-0.41) = -2.31 - \"sofa when\" \u2192 -1.90 + (-0.51) = -2.41</p> <p>New top 3 beams: 1. \"The cat sat on the mat .\" \u2192 -1.43 2. \"The cat sat on the floor .\" \u2192 -1.75 3. \"The cat sat on the sofa when\" \u2192 -2.41</p> <p>Final output: \"The cat sat on the mat.\"</p>"},{"location":"decoding_strategies/beam_search/#4-key-characteristics","title":"4. Key Characteristics","text":""},{"location":"decoding_strategies/beam_search/#explores-multiple-paths","title":"Explores Multiple Paths","text":"<p>Unlike greedy, beam search maintains K hypotheses: - Can recover from locally suboptimal choices - Considers alternative continuations - Better global optimization</p>"},{"location":"decoding_strategies/beam_search/#length-bias-problem","title":"Length Bias Problem","text":"<p>Longer sequences accumulate more negative log probabilities: <pre><code>Seq 1 (length 5): -0.5 + -0.6 + -0.4 + -0.5 + -0.3 = -2.3\nSeq 2 (length 3): -0.5 + -0.6 + -0.4 = -1.5\n</code></pre></p> <p>Seq 2 has higher score despite being incomplete!</p> <p>Solution: Length normalization <pre><code>normalized_score = score / length^\u03b1\n# \u03b1 = 0.6-0.8 typical\n# \u03b1 = 0: no normalization\n# \u03b1 = 1: full normalization\n</code></pre></p>"},{"location":"decoding_strategies/beam_search/#5-common-problems","title":"5. Common Problems","text":""},{"location":"decoding_strategies/beam_search/#problem-1-reduced-diversity","title":"Problem 1: Reduced Diversity","text":"<p>All beams often converge to similar outputs:</p> <pre><code>Prompt: \"I think that\"\n\nBeam 1: \"I think that we should focus on...\"\nBeam 2: \"I think that we need to consider...\"\nBeam 3: \"I think that this is important...\"\n</code></pre> <p>All beams start with safe, high-probability tokens \u2192 similar continuations.</p> <p>Why: Beam search favors safe, high-probability paths over diverse, creative paths.</p>"},{"location":"decoding_strategies/beam_search/#problem-2-generic-outputs","title":"Problem 2: Generic Outputs","text":"<p>In open-ended generation: <pre><code>Prompt: \"Tell me a story about\"\n\nGreedy: \"a boy who lived in\"\nBeam (K=5): \"a young boy who lived in a small town\"\n</code></pre></p> <p>Beam search produces grammatically perfect but boring text.</p>"},{"location":"decoding_strategies/beam_search/#problem-3-computational-cost","title":"Problem 3: Computational Cost","text":"<ul> <li>Memory: O(K \u00d7 T \u00d7 V) for storing beam candidates</li> <li>Time: O(K \u00d7 V) per step (vs O(V) for greedy)</li> <li>K=10 \u2192 10\u00d7 slower than greedy</li> </ul>"},{"location":"decoding_strategies/beam_search/#6-length-normalization","title":"6. Length Normalization","text":""},{"location":"decoding_strategies/beam_search/#without-normalization","title":"Without Normalization","text":"<pre><code># Shorter sequences preferred\nbeams = sorted(candidates, key=lambda x: x[1], reverse=True)\n</code></pre>"},{"location":"decoding_strategies/beam_search/#with-normalization","title":"With Normalization","text":"<pre><code>def length_penalty(length, alpha=0.6):\n    return ((5 + length) / 6) ** alpha  # Google NMT formula\n\n# Normalized score\nnormalized = score / length_penalty(len(seq), alpha=0.6)\nbeams = sorted(candidates, key=lambda x: normalized_score(x), reverse=True)\n</code></pre> <p>Effect: - Encourages longer, more complete sequences - Prevents premature termination - Essential for translation and summarization</p>"},{"location":"decoding_strategies/beam_search/#7-when-to-use-beam-search","title":"7. When to Use Beam Search","text":""},{"location":"decoding_strategies/beam_search/#good-use-cases","title":"\u2705 Good Use Cases","text":"<p>Structured tasks with clear objectives: - Machine translation - Automatic speech recognition (ASR) - Image captioning - Summarization - Question answering (extractive)</p> <p>When correctness matters more than creativity: - Technical documentation generation - Code comment generation - Medical report generation</p>"},{"location":"decoding_strategies/beam_search/#poor-use-cases","title":"\u274c Poor Use Cases","text":"<p>Creative or conversational tasks: - Story writing - Dialogue systems - Chatbots - Poetry generation</p> <p>Tasks requiring diversity: - Brainstorming - Multiple solution generation - Creative writing</p>"},{"location":"decoding_strategies/beam_search/#8-variants-and-improvements","title":"8. Variants and Improvements","text":""},{"location":"decoding_strategies/beam_search/#diverse-beam-search","title":"Diverse Beam Search","text":"<p>Force beams to be dissimilar using diversity penalty: <pre><code>diversity_penalty = 0.5\nfor beam_group in groups:\n    # Penalize tokens already chosen by other groups\n    adjusted_score = score - diversity_penalty * overlap_count\n</code></pre></p>"},{"location":"decoding_strategies/beam_search/#constrained-beam-search","title":"Constrained Beam Search","text":"<p>Force inclusion of specific tokens/phrases: <pre><code># E.g., must include \"climate change\" in summary\nconstraints = [\"climate\", \"change\"]\n# Only keep beams that satisfy constraints\n</code></pre></p>"},{"location":"decoding_strategies/beam_search/#stochastic-beam-search","title":"Stochastic Beam Search","text":"<p>Add randomness to beam selection for more diversity.</p>"},{"location":"decoding_strategies/beam_search/#9-interview-questions","title":"9. Interview Questions","text":""},{"location":"decoding_strategies/beam_search/#q1-what-is-beam-search-and-how-does-it-differ-from-greedy-decoding","title":"Q1: What is beam search and how does it differ from greedy decoding?","text":"<p>Answer: Beam search maintains K candidate sequences (beams) instead of just one. At each step, it expands all beams, scores all possible continuations, and keeps the top K. This allows exploration of multiple paths and can recover from locally suboptimal choices, unlike greedy which commits to a single path.</p>"},{"location":"decoding_strategies/beam_search/#q2-why-use-log-probabilities-instead-of-regular-probabilities","title":"Q2: Why use log probabilities instead of regular probabilities?","text":"<p>Answer: Two reasons: 1. Numerical stability: Multiplying many small probabilities (0.3 \u00d7 0.4 \u00d7 0.2...) quickly underflows to zero in floating point 2. Computational efficiency: Log transforms products to sums: log(P\u2081 \u00d7 P\u2082) = log(P\u2081) + log(P\u2082), which is more stable and efficient</p>"},{"location":"decoding_strategies/beam_search/#q3-what-is-the-length-bias-problem-in-beam-search","title":"Q3: What is the length bias problem in beam search?","text":"<p>Answer: Longer sequences accumulate more negative log probabilities, making them score lower than shorter sequences even if they're more complete. For example, a 10-token sequence might score -8.5 while a 5-token incomplete sequence scores -3.2. Length normalization (dividing by sequence length raised to \u03b1) addresses this by favoring complete sentences.</p>"},{"location":"decoding_strategies/beam_search/#q4-why-does-beam-search-produce-less-diverse-outputs-than-sampling","title":"Q4: Why does beam search produce less diverse outputs than sampling?","text":"<p>Answer: Beam search is deterministic and risk-averse\u2014it keeps the K highest-probability sequences. This means all beams tend to follow safe, high-probability paths, leading to similar outputs. Rare but creative continuations are discarded early. Sampling methods can explore lower-probability tokens, leading to more diversity.</p>"},{"location":"decoding_strategies/beam_search/#q5-whats-the-computational-complexity-of-beam-search","title":"Q5: What's the computational complexity of beam search?","text":"<p>Answer: - Time per step: O(K \u00d7 V) where K=beam width, V=vocab size   - Greedy: O(V), so beam is K\u00d7 slower - Memory: O(K \u00d7 T) to store K beams of length T - Total: For T steps, O(K \u00d7 V \u00d7 T) time</p> <p>Typical K=5-10 for translation, but this 5-10\u00d7 slowdown is significant.</p>"},{"location":"decoding_strategies/beam_search/#q6-how-do-you-choose-the-optimal-beam-width-k","title":"Q6: How do you choose the optimal beam width K?","text":"<p>Answer: Trade-off between quality and speed: - K=1: Greedy (fast, low quality) - K=5-10: Standard for translation (good balance) - K=50+: Diminishing returns, very slow</p> <p>Empirically, quality plateaus around K=10 for most tasks. Beyond that, you get marginal gains for significant computational cost.</p>"},{"location":"decoding_strategies/beam_search/#q7-can-beam-search-guarantee-finding-the-optimal-sequence","title":"Q7: Can beam search guarantee finding the optimal sequence?","text":"<p>Answer: No. Beam search is a heuristic that prunes the search space. It only explores the top K paths at each step, potentially discarding paths that could lead to the globally optimal sequence later. Full exhaustive search (K=|V|^T) is computationally infeasible, so beam search is a practical approximation.</p>"},{"location":"decoding_strategies/beam_search/#q8-what-is-diverse-beam-search-and-when-is-it-useful","title":"Q8: What is diverse beam search and when is it useful?","text":"<p>Answer: Diverse beam search forces different beam groups to explore different areas of the search space by penalizing similarity. It's useful when you need multiple distinct outputs (e.g., generating K different translations or summaries). Standard beam search often produces K very similar sequences, which isn't helpful for diversity.</p>"},{"location":"decoding_strategies/beam_search/#q9-when-would-you-use-beam-search-over-sampling-methods-like-top-p","title":"Q9: When would you use beam search over sampling methods like top-p?","text":"<p>Answer: Use beam search for: - Objective quality metrics (BLEU, ROUGE) that correlate with likelihood - Structured outputs (translation, ASR) with one correct answer - Deterministic requirements (reproducibility)</p> <p>Use sampling for: - Creative tasks requiring diversity - Conversational AI needing personality - Open-ended generation where many good answers exist</p>"},{"location":"decoding_strategies/beam_search/#q10-how-does-beam-search-handle-the-eos-token","title":"Q10: How does beam search handle the EOS token?","text":"<p>Answer: When a beam generates EOS (end-of-sequence), it's marked as complete: 1. Complete beams stop expanding 2. They remain in the candidate pool with their final score 3. Active beams continue generating 4. Search terminates when all K beams complete or max_length reached 5. Return the complete beam with highest normalized score</p> <p>Some implementations use length normalization to fairly compare complete sequences of different lengths.</p>"},{"location":"decoding_strategies/beam_search/#10-code-example","title":"10. Code Example","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom typing import List, Tuple\n\ndef beam_search(\n    model,\n    input_ids: torch.Tensor,\n    beam_width: int = 5,\n    max_length: int = 50,\n    length_penalty: float = 0.6,\n    eos_token_id: int = 2\n) -&gt; torch.Tensor:\n    \"\"\"\n    Beam search decoding.\n\n    Args:\n        model: Language model\n        input_ids: Starting tokens [1, seq_len]\n        beam_width: Number of beams\n        max_length: Maximum sequence length\n        length_penalty: Alpha for length normalization\n        eos_token_id: End of sequence token\n\n    Returns:\n        Best sequence found\n    \"\"\"\n    device = input_ids.device\n    batch_size = input_ids.size(0)\n\n    # Initialize beams: (sequence, score, finished)\n    beams = [(input_ids[0].tolist(), 0.0, False)]\n\n    for _ in range(max_length):\n        candidates = []\n\n        for seq, score, finished in beams:\n            if finished:\n                candidates.append((seq, score, finished))\n                continue\n\n            # Get next token probabilities\n            input_tensor = torch.tensor([seq], device=device)\n            with torch.no_grad():\n                outputs = model(input_tensor)\n                logits = outputs.logits[0, -1, :]\n                log_probs = F.log_softmax(logits, dim=-1)\n\n            # Expand beam with top-K tokens\n            top_k_probs, top_k_ids = torch.topk(log_probs, beam_width)\n\n            for prob, token_id in zip(top_k_probs, top_k_ids):\n                new_seq = seq + [token_id.item()]\n                new_score = score + prob.item()\n                is_finished = (token_id.item() == eos_token_id)\n                candidates.append((new_seq, new_score, is_finished))\n\n        # Apply length penalty and keep top beams\n        def normalized_score(item):\n            seq, score, _ = item\n            penalty = ((5 + len(seq)) / 6) ** length_penalty\n            return score / penalty\n\n        beams = sorted(candidates, key=normalized_score, reverse=True)[:beam_width]\n\n        # Early stopping if all beams finished\n        if all(finished for _, _, finished in beams):\n            break\n\n    # Return best sequence\n    best_seq, _, _ = beams[0]\n    return torch.tensor([best_seq], device=device)\n\n# Usage\n# output = beam_search(model, prompt_ids, beam_width=5)\n</code></pre>"},{"location":"decoding_strategies/beam_search/#11-key-takeaways-for-interviews","title":"11. Key Takeaways for Interviews","text":"<ol> <li>Definition: Maintains K candidate sequences, keeps top-K by cumulative probability</li> <li>Beam width K: Trade-off between quality (higher K) and speed (lower K)</li> <li>Length normalization: Essential to prevent bias toward shorter sequences</li> <li>Pros: Better than greedy, explores alternatives, good for structured tasks</li> <li>Cons: Computationally expensive (K\u00d7 slower), low diversity, generic outputs</li> <li>Best for: Translation, ASR, captioning, summarization</li> <li>Worst for: Creative writing, dialogue, brainstorming</li> <li>Complexity: O(K \u00d7 V \u00d7 T) time, O(K \u00d7 T) space</li> </ol>"},{"location":"decoding_strategies/beam_search/#references","title":"References","text":"<ul> <li>Google's Neural Machine Translation System - Length normalization formula</li> <li>The Curious Case of Neural Text Degeneration - Discusses diversity issues</li> <li>Diverse Beam Search</li> </ul>"},{"location":"decoding_strategies/greedy_decoding/","title":"Greedy Decoding - Interview Prep Guide","text":""},{"location":"decoding_strategies/greedy_decoding/#1-overview","title":"1. Overview","text":"<p>Greedy decoding selects the token with the highest probability at each step during autoregressive text generation. It's the simplest and fastest decoding strategy but lacks diversity.</p> <p>Key insight: Local optimization (best token at each step) doesn't guarantee global optimality (best overall sequence).</p>"},{"location":"decoding_strategies/greedy_decoding/#2-how-it-works","title":"2. How It Works","text":""},{"location":"decoding_strategies/greedy_decoding/#algorithm","title":"Algorithm","text":"<pre><code>def greedy_decode(model, prompt, max_length):\n    tokens = prompt\n    for _ in range(max_length):\n        logits = model(tokens)  # Shape: (vocab_size,)\n        next_token = argmax(logits)  # Pick highest probability\n        tokens.append(next_token)\n        if next_token == EOS:\n            break\n    return tokens\n</code></pre>"},{"location":"decoding_strategies/greedy_decoding/#example","title":"Example","text":"<p>Given probability distribution after \"The cat sat on the\":</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 <p>Greedy choice: <code>mat</code> (highest probability)</p> <p>Output: \"The cat sat on the mat\"</p>"},{"location":"decoding_strategies/greedy_decoding/#3-key-characteristics","title":"3. Key Characteristics","text":""},{"location":"decoding_strategies/greedy_decoding/#deterministic","title":"Deterministic","text":"<ul> <li>Same input \u2192 always same output</li> <li>No randomness involved</li> <li>Fully reproducible</li> </ul>"},{"location":"decoding_strategies/greedy_decoding/#myopic-short-sighted","title":"Myopic (Short-sighted)","text":"<p>Each decision is locally optimal but may lead to suboptimal sequences:</p> <pre><code>Step 1: \"I think\" \u2192 model assigns:\n  - \"that\" (0.6)\n  - \"the\" (0.4)\n\nGreedy picks: \"that\"\n\nStep 2: \"I think that\" \u2192 model assigns:\n  - \"is\" (0.3)\n  - \"maybe\" (0.25)\n\nBut if we had picked \"the\" at step 1:\n  \"I think the\" \u2192 \"best\" (0.7)\n\nFinal: \"I think that is...\" (score: 0.6 \u00d7 0.3 = 0.18)\nBetter: \"I think the best...\" (score: 0.4 \u00d7 0.7 = 0.28)\n</code></pre> <p>Greedy can't recover from early suboptimal choices.</p>"},{"location":"decoding_strategies/greedy_decoding/#4-common-problems","title":"4. Common Problems","text":""},{"location":"decoding_strategies/greedy_decoding/#problem-1-repetition-loops","title":"Problem 1: Repetition Loops","text":"<pre><code>Prompt: \"To be or not to\"\nOutput: \"be or not to be or not to be or not to be...\"\n</code></pre> <p>Why: If \"be\" has slightly higher probability than alternatives at each step, greedy gets stuck.</p>"},{"location":"decoding_strategies/greedy_decoding/#problem-2-generic-outputs","title":"Problem 2: Generic Outputs","text":"<pre><code>Prompt: \"Write a creative story about\"\nGreedy: \"a boy who lived in a small town and went to school...\"\n</code></pre> <p>Always picks safe, high-probability continuations \u2192 boring text.</p>"},{"location":"decoding_strategies/greedy_decoding/#problem-3-early-mistakes-propagate","title":"Problem 3: Early Mistakes Propagate","text":"<pre><code>Prompt: \"The capital of France is\"\nIf model assigns:\n  - \"Paris\" (0.45)\n  - \"Lyon\" (0.46)  \u2190 greedy picks this (wrong!)\n\nOutput: \"Lyon, which is known for...\"\n</code></pre> <p>Once the wrong token is chosen, subsequent tokens try to justify it.</p>"},{"location":"decoding_strategies/greedy_decoding/#5-when-to-use-greedy-decoding","title":"5. When to Use Greedy Decoding","text":""},{"location":"decoding_strategies/greedy_decoding/#good-use-cases","title":"\u2705 Good Use Cases","text":"<p>Deterministic tasks: - Math problem solving - Code generation (when exact output matters) - Factual Q&amp;A with clear answers - Translation of standardized text</p> <p>Debugging: - Baseline comparisons - Reproducible testing - Fastest inference for quick experiments</p> <p>Confident models: - When model probability distributions are very peaked - Single obvious correct answer</p>"},{"location":"decoding_strategies/greedy_decoding/#poor-use-cases","title":"\u274c Poor Use Cases","text":"<p>Creative tasks: - Story writing - Poetry generation - Brainstorming</p> <p>Conversational AI: - Chatbots - Dialogue systems - Personality-driven responses</p> <p>Long-form generation: - Articles, essays - Open-ended content</p>"},{"location":"decoding_strategies/greedy_decoding/#6-practical-considerations","title":"6. Practical Considerations","text":""},{"location":"decoding_strategies/greedy_decoding/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Time: O(T \u00d7 V) where T = sequence length, V = vocab size</li> <li>Space: O(1) for decoding state (minimal memory)</li> <li>Fastest among all decoding strategies</li> </ul>"},{"location":"decoding_strategies/greedy_decoding/#modifications","title":"Modifications","text":"<p>Greedy + repetition penalty: <pre><code>def greedy_with_penalty(logits, previous_tokens, penalty=1.2):\n    for token in previous_tokens:\n        logits[token] /= penalty  # Reduce probability of repeated tokens\n    return argmax(logits)\n</code></pre></p> <p>Greedy + length normalization: Useful when comparing sequences of different lengths (not during decoding itself).</p>"},{"location":"decoding_strategies/greedy_decoding/#7-interview-questions","title":"7. Interview Questions","text":""},{"location":"decoding_strategies/greedy_decoding/#q1-what-is-greedy-decoding-and-how-does-it-work","title":"Q1: What is greedy decoding and how does it work?","text":"<p>Answer: Greedy decoding selects the token with the highest probability at each generation step. It's deterministic and fast but locally optimal\u2014it picks the best next token without considering whether this leads to the best overall sequence.</p>"},{"location":"decoding_strategies/greedy_decoding/#q2-why-is-greedy-decoding-called-greedy","title":"Q2: Why is greedy decoding called \"greedy\"?","text":"<p>Answer: It's \"greedy\" because it makes the locally optimal choice at each step (highest probability token) without considering future consequences. Like the greedy algorithm paradigm, it optimizes immediate reward rather than global optimality.</p>"},{"location":"decoding_strategies/greedy_decoding/#q3-whats-the-main-disadvantage-of-greedy-decoding","title":"Q3: What's the main disadvantage of greedy decoding?","text":"<p>Answer: Lack of diversity and repetition. Greedy often produces repetitive, generic text because it can't explore alternative paths. Once it makes a suboptimal choice, it can't backtrack, leading to error propagation and repetitive loops.</p>"},{"location":"decoding_strategies/greedy_decoding/#q4-how-does-greedy-decoding-differ-from-beam-search","title":"Q4: How does greedy decoding differ from beam search?","text":"<p>Answer: - Greedy: Keeps only 1 hypothesis (best token at each step) - Beam search: Keeps K hypotheses (top-K sequences), explores multiple paths</p> <p>Beam search can recover from locally suboptimal choices by considering alternative sequences. Greedy cannot.</p>"},{"location":"decoding_strategies/greedy_decoding/#q5-can-greedy-decoding-produce-the-optimal-sequence","title":"Q5: Can greedy decoding produce the optimal sequence?","text":"<p>Answer: Not necessarily. Greedy finds a locally optimal sequence but not necessarily globally optimal. The highest-probability sequence might require choosing a lower-probability token early on that leads to much higher probabilities later.</p>"},{"location":"decoding_strategies/greedy_decoding/#q6-why-does-greedy-decoding-cause-repetition","title":"Q6: Why does greedy decoding cause repetition?","text":"<p>Answer: If a token or phrase has slightly higher probability than alternatives, greedy will keep selecting it. For example, in \"to be or not to be or not to be...\", if \"be\" consistently has a small probability advantage, the model gets stuck in a loop with no mechanism to escape.</p>"},{"location":"decoding_strategies/greedy_decoding/#q7-how-can-you-reduce-repetition-in-greedy-decoding","title":"Q7: How can you reduce repetition in greedy decoding?","text":"<p>Answer: Common techniques: 1. Repetition penalty: Divide logits of previously generated tokens by penalty factor (e.g., 1.2) 2. N-gram blocking: Prevent repeating same N-grams 3. Switch to sampling: Use temperature/top-p for diversity 4. Beam search: Explore multiple paths to avoid local minima</p>"},{"location":"decoding_strategies/greedy_decoding/#q8-whats-the-computational-cost-of-greedy-decoding","title":"Q8: What's the computational cost of greedy decoding?","text":"<p>Answer: Very efficient: - Per step: O(V) to find argmax over vocabulary - Total: O(T \u00d7 V) for T tokens - Memory: O(1) for decoding state - Fastest decoding strategy, no beam/sample storage overhead</p>"},{"location":"decoding_strategies/greedy_decoding/#q9-when-would-you-prefer-greedy-over-beam-search","title":"Q9: When would you prefer greedy over beam search?","text":"<p>Answer: - Latency-critical applications (greedy is much faster) - Deterministic outputs required (reproducibility) - Single clear answer (factual Q&amp;A, simple math) - Resource-constrained environments (minimal memory)</p> <p>Beam search is overkill when diversity isn't needed and the model is confident.</p>"},{"location":"decoding_strategies/greedy_decoding/#q10-how-does-greedy-decoding-interact-with-temperature","title":"Q10: How does greedy decoding interact with temperature?","text":"<p>Answer: Greedy decoding ignores temperature because it always picks argmax. Temperature only affects sampling-based methods. You can think of greedy as temperature \u2192 0 (infinitely peaked distribution where highest probability \u2192 1.0, others \u2192 0).</p>"},{"location":"decoding_strategies/greedy_decoding/#8-comparison-with-other-methods","title":"8. Comparison with Other Methods","text":"Aspect Greedy Beam Search Sampling Speed Fastest Slow (K\u00d7 cost) Fast Diversity None Low High Determinism Yes Yes No Repetition High risk Medium risk Low risk Quality Variable Higher Variable Memory Minimal O(K \u00d7 T) Minimal"},{"location":"decoding_strategies/greedy_decoding/#9-code-example","title":"9. Code Example","text":"<pre><code>import torch\n\ndef greedy_decode(model, input_ids, max_length=50):\n    \"\"\"\n    Simple greedy decoding implementation.\n\n    Args:\n        model: Language model with forward() method\n        input_ids: Starting token IDs (tensor)\n        max_length: Maximum sequence length\n\n    Returns:\n        Generated token IDs\n    \"\"\"\n    generated = input_ids.clone()\n\n    for _ in range(max_length):\n        # Get logits for next token\n        with torch.no_grad():\n            outputs = model(generated)\n            logits = outputs.logits[:, -1, :]  # Last token logits\n\n        # Greedy selection\n        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n        # Append to sequence\n        generated = torch.cat([generated, next_token], dim=1)\n\n        # Check for EOS\n        if next_token.item() == model.config.eos_token_id:\n            break\n\n    return generated\n\n# Usage\n# output = greedy_decode(model, prompt_tokens, max_length=100)\n</code></pre>"},{"location":"decoding_strategies/greedy_decoding/#10-key-takeaways-for-interviews","title":"10. Key Takeaways for Interviews","text":"<ol> <li>Definition: Always picks the highest probability token at each step</li> <li>Pros: Fast, deterministic, simple, reproducible</li> <li>Cons: No diversity, repetitive, locally optimal only</li> <li>Main problem: Repetition loops and generic outputs</li> <li>Best for: Deterministic tasks, debugging, factual Q&amp;A</li> <li>Worst for: Creative writing, conversations, long-form generation</li> <li>Complexity: O(T \u00d7 V) time, O(1) space for decoding</li> </ol>"},{"location":"decoding_strategies/greedy_decoding/#references","title":"References","text":"<ul> <li>The Curious Case of Neural Text Degeneration - Discusses repetition issues</li> <li>Hugging Face: Generation Strategies</li> </ul>"},{"location":"decoding_strategies/sampling_methods/","title":"Sampling Methods - Interview Prep Guide","text":""},{"location":"decoding_strategies/sampling_methods/#1-overview","title":"1. Overview","text":"<p>Sampling methods introduce controlled randomness into text generation by probabilistically selecting tokens rather than deterministically choosing the highest-probability token. This enables diverse, creative outputs while maintaining coherence.</p> <p>Key insight: The best sequence isn't always the highest-probability one\u2014controlled randomness can produce more natural, interesting text.</p> <p>Three main techniques: 1. Temperature sampling - Controls randomness via scaling 2. Top-k sampling - Samples from top K tokens 3. Top-p sampling (nucleus) - Samples from smallest set with cumulative probability \u2265 p</p>"},{"location":"decoding_strategies/sampling_methods/#2-temperature-sampling","title":"2. Temperature Sampling","text":""},{"location":"decoding_strategies/sampling_methods/#21-how-it-works","title":"2.1 How It Works","text":"<p>Temperature scales logits before applying softmax, controlling distribution \"sharpness\":</p> \\[\\text{P}_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\\] <p>Where: - \\(z_i\\) = logit for token i - \\(T\\) = temperature (T &gt; 0) - \\(T = 1\\) \u2192 original distribution - \\(T &lt; 1\\) \u2192 sharper (more deterministic) - \\(T &gt; 1\\) \u2192 flatter (more random)</p>"},{"location":"decoding_strategies/sampling_methods/#22-example","title":"2.2 Example","text":"<p>Original probabilities after \"The cat sat on the\":</p> Token Prob (T=1) Prob (T=0.5) Prob (T=2.0) mat 0.40 0.58 0.28 floor 0.25 0.26 0.23 sofa 0.15 0.10 0.18 bed 0.10 0.04 0.14 roof 0.05 0.01 0.09 moon 0.03 0.00 0.05 pizza 0.02 0.00 0.03 <p>T=0.5 (Low): Distribution sharpens \u2192 \"mat\" dominates \u2192 near-greedy behavior T=2.0 (High): Distribution flattens \u2192 \"moon\", \"pizza\" become viable \u2192 more random</p>"},{"location":"decoding_strategies/sampling_methods/#23-extreme-cases","title":"2.3 Extreme Cases","text":"<p>T \u2192 0: - Distribution becomes one-hot (probability \u2192 1.0 for argmax) - Equivalent to greedy decoding - Zero randomness</p> <p>T \u2192 \u221e: - Uniform distribution (all tokens equally likely) - Maximum randomness - Often produces gibberish</p> <p>Typical values: - T=0.7: Focused, coherent (factual Q&amp;A) - T=1.0: Balanced (default) - T=1.2-1.5: Creative, diverse (story writing)</p>"},{"location":"decoding_strategies/sampling_methods/#24-code-example","title":"2.4 Code Example","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\ndef temperature_sampling(logits, temperature=1.0):\n    \"\"\"\n    Sample token with temperature scaling.\n\n    Args:\n        logits: Raw model outputs [vocab_size]\n        temperature: Scaling factor (T &gt; 0)\n\n    Returns:\n        Sampled token ID\n    \"\"\"\n    # Scale logits\n    scaled_logits = logits / temperature\n\n    # Convert to probabilities\n    probs = F.softmax(scaled_logits, dim=-1)\n\n    # Sample\n    token_id = torch.multinomial(probs, num_samples=1)\n    return token_id.item()\n\n# Usage\n# next_token = temperature_sampling(model_logits, temperature=1.2)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#3-top-k-sampling","title":"3. Top-k Sampling","text":""},{"location":"decoding_strategies/sampling_methods/#31-how-it-works","title":"3.1 How It Works","text":"<p>Top-k restricts sampling to the k most probable tokens:</p> <ol> <li>Sort tokens by probability (descending)</li> <li>Keep only top-k tokens</li> <li>Set all other probabilities to zero</li> <li>Renormalize the top-k probabilities</li> <li>Sample from renormalized distribution</li> </ol>"},{"location":"decoding_strategies/sampling_methods/#32-example","title":"3.2 Example","text":"<p>Probabilities after \"The cat sat on the\":</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-k with k=3:</p> <p>Kept tokens: - mat: 0.40 - floor: 0.25 - sofa: 0.15</p> <p>Renormalized: - mat: 0.40/0.80 = 0.50 - floor: 0.25/0.80 = 0.31 - sofa: 0.15/0.80 = 0.19</p> <p>Removed: bed, roof, moon, pizza</p> <p>Output: One of {mat, floor, sofa} with renormalized probabilities</p>"},{"location":"decoding_strategies/sampling_methods/#33-key-limitation-fixed-k","title":"3.3 Key Limitation: Fixed K","text":"<p>Top-k doesn't adapt to distribution shape:</p> <p>Case 1: Confident model - Probabilities: [0.85, 0.07, 0.03, 0.02, 0.02, 0.01] - k=5 \u2192 keeps 5 tokens even though model is very confident - Inefficient: forces sampling from low-quality tokens</p> <p>Case 2: Uncertain model - Probabilities: [0.20, 0.20, 0.20, 0.20, 0.20] - k=3 \u2192 keeps only 3 tokens, excludes equally valid options - Too restrictive: removes valid choices</p> <p>Problem: Same k value for different distribution shapes.</p>"},{"location":"decoding_strategies/sampling_methods/#34-typical-values","title":"3.4 Typical Values","text":"<ul> <li>k=10-20: Conservative, relatively safe outputs</li> <li>k=40-50: More diverse, creative</li> <li>k=100+: Very random, may include low-quality tokens</li> </ul>"},{"location":"decoding_strategies/sampling_methods/#35-code-example","title":"3.5 Code Example","text":"<pre><code>import torch\n\ndef top_k_sampling(logits, k=50, temperature=1.0):\n    \"\"\"\n    Sample from top-k tokens.\n\n    Args:\n        logits: Raw model outputs [vocab_size]\n        k: Number of top tokens to consider\n        temperature: Optional temperature scaling\n\n    Returns:\n        Sampled token ID\n    \"\"\"\n    # Apply temperature\n    scaled_logits = logits / temperature\n\n    # Get top-k\n    top_k_logits, top_k_indices = torch.topk(scaled_logits, k)\n\n    # Softmax over top-k\n    probs = F.softmax(top_k_logits, dim=-1)\n\n    # Sample from top-k\n    sampled_idx = torch.multinomial(probs, num_samples=1)\n\n    # Map back to original vocabulary\n    token_id = top_k_indices[sampled_idx]\n    return token_id.item()\n\n# Usage\n# next_token = top_k_sampling(model_logits, k=40, temperature=0.9)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#4-top-p-sampling-nucleus-sampling","title":"4. Top-p Sampling (Nucleus Sampling)","text":""},{"location":"decoding_strategies/sampling_methods/#41-how-it-works","title":"4.1 How It Works","text":"<p>Top-p selects the smallest set of tokens whose cumulative probability \u2265 p:</p> <ol> <li>Sort tokens by probability (descending)</li> <li>Compute cumulative probability</li> <li>Keep tokens until cumulative \u2265 p</li> <li>Renormalize and sample</li> </ol> <p>Key advantage: Adaptive\u2014number of tokens varies based on distribution.</p>"},{"location":"decoding_strategies/sampling_methods/#42-example","title":"4.2 Example","text":"<p>Probabilities after \"The cat sat on the\":</p> Token Probability Cumulative mat 0.40 0.40 floor 0.25 0.65 sofa 0.15 0.80 bed 0.10 0.90 roof 0.05 0.95 moon 0.03 0.98 pizza 0.02 1.00 <p>Top-p with p=0.9: - Keep: mat, floor, sofa, bed (cumulative = 0.90) - Remove: roof, moon, pizza - Effective k = 4</p>"},{"location":"decoding_strategies/sampling_methods/#43-adaptive-behavior","title":"4.3 Adaptive Behavior","text":"<p>Case 1: Confident model <pre><code>Probabilities: [0.85, 0.07, 0.03, 0.02, 0.02, 0.01]\np=0.9 \u2192 keeps 2 tokens [0.85, 0.07]\nEffective k = 2 (adaptive reduction)\n</code></pre></p> <p>Case 2: Uncertain model <pre><code>Probabilities: [0.20, 0.20, 0.20, 0.20, 0.20]\np=0.9 \u2192 keeps 5 tokens\nEffective k = 5 (adaptive expansion)\n</code></pre></p> <p>Advantage over top-k: Automatically adjusts to model confidence.</p>"},{"location":"decoding_strategies/sampling_methods/#44-typical-values","title":"4.4 Typical Values","text":"<ul> <li>p=0.9: Standard for most applications (OpenAI default)</li> <li>p=0.95: More diverse, creative outputs</li> <li>p=0.75-0.85: More focused, conservative</li> </ul>"},{"location":"decoding_strategies/sampling_methods/#45-code-example","title":"4.5 Code Example","text":"<pre><code>import torch\n\ndef top_p_sampling(logits, p=0.9, temperature=1.0):\n    \"\"\"\n    Nucleus sampling (top-p).\n\n    Args:\n        logits: Raw model outputs [vocab_size]\n        p: Cumulative probability threshold (0 &lt; p \u2264 1)\n        temperature: Optional temperature scaling\n\n    Returns:\n        Sampled token ID\n    \"\"\"\n    # Apply temperature\n    scaled_logits = logits / temperature\n    probs = F.softmax(scaled_logits, dim=-1)\n\n    # Sort probabilities\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n\n    # Compute cumulative probabilities\n    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n\n    # Find cutoff: first position where cumulative &gt; p\n    # Include that position (so cumulative &gt;= p)\n    sorted_indices_to_remove = cumulative_probs &gt; p\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = False\n\n    # Mask out tokens to remove\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        0, sorted_indices, sorted_indices_to_remove\n    )\n    filtered_logits = scaled_logits.clone()\n    filtered_logits[indices_to_remove] = float('-inf')\n\n    # Sample from filtered distribution\n    filtered_probs = F.softmax(filtered_logits, dim=-1)\n    token_id = torch.multinomial(filtered_probs, num_samples=1)\n    return token_id.item()\n\n# Usage\n# next_token = top_p_sampling(model_logits, p=0.9, temperature=1.0)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#5-combining-sampling-methods","title":"5. Combining Sampling Methods","text":"<p>In practice, sampling methods are often combined:</p>"},{"location":"decoding_strategies/sampling_methods/#common-combination-temperature-top-p","title":"Common Combination: Temperature + Top-p","text":"<pre><code>def sample_token(logits, temperature=0.9, top_p=0.9):\n    # 1. Apply temperature first\n    scaled_logits = logits / temperature\n\n    # 2. Then apply top-p filtering\n    token = top_p_sampling(scaled_logits, p=top_p)\n    return token\n</code></pre> <p>Why combine: - Temperature controls overall randomness - Top-p prevents sampling from the very long tail - Together: controlled creativity with safety</p>"},{"location":"decoding_strategies/sampling_methods/#other-combinations","title":"Other Combinations","text":"<p>Temperature + Top-k: <pre><code># Used in some older systems\ntoken = top_k_sampling(logits, k=40, temperature=0.8)\n</code></pre></p> <p>Top-k + Top-p: <pre><code># Apply top-k first as a hard cutoff\n# Then apply top-p for adaptive filtering\n# Less common, top-p usually sufficient\n</code></pre></p>"},{"location":"decoding_strategies/sampling_methods/#6-when-to-use-each-method","title":"6. When to Use Each Method","text":""},{"location":"decoding_strategies/sampling_methods/#temperature-sampling","title":"Temperature Sampling","text":"<p>\u2705 Use when: - Need simple randomness control - Working with other filtering methods (top-k, top-p) - Want smooth transition between deterministic and random</p> <p>\u274c Avoid when: - Used alone (no filtering from tail) - Need adaptive behavior</p>"},{"location":"decoding_strategies/sampling_methods/#top-k-sampling","title":"Top-k Sampling","text":"<p>\u2705 Use when: - Need simple, predictable diversity control - Fixed computational budget (always k tokens) - Legacy systems (older standard)</p> <p>\u274c Avoid when: - Distribution shape varies significantly - Need adaptive behavior - Modern systems (top-p preferred)</p>"},{"location":"decoding_strategies/sampling_methods/#top-p-sampling","title":"Top-p Sampling","text":"<p>\u2705 Use when: - Need adaptive diversity - Model confidence varies - General-purpose text generation - Conversational AI, creative writing</p> <p>\u274c Avoid when: - Need strict determinism - Computational constraints (slightly more complex)</p>"},{"location":"decoding_strategies/sampling_methods/#7-interview-questions","title":"7. Interview Questions","text":""},{"location":"decoding_strategies/sampling_methods/#q1-what-problem-do-sampling-methods-solve","title":"Q1: What problem do sampling methods solve?","text":"<p>Answer: Greedy and beam search produce repetitive, generic text by always choosing high-probability tokens. Sampling methods introduce controlled randomness, enabling diverse, creative outputs while preventing the model from getting stuck in repetitive loops. They balance coherence with variety.</p>"},{"location":"decoding_strategies/sampling_methods/#q2-how-does-temperature-affect-the-probability-distribution","title":"Q2: How does temperature affect the probability distribution?","text":"<p>Answer: Temperature scales logits before softmax. Low temperature (T&lt;1) sharpens the distribution\u2014high-probability tokens become more dominant (near-greedy). High temperature (T&gt;1) flattens the distribution\u2014low-probability tokens become more likely (more random). At T\u21920, it becomes greedy; at T\u2192\u221e, it becomes uniform.</p>"},{"location":"decoding_strategies/sampling_methods/#q3-whats-the-main-limitation-of-top-k-sampling","title":"Q3: What's the main limitation of top-k sampling?","text":"<p>Answer: Fixed k doesn't adapt to distribution shape. When the model is very confident, k=50 wastes computation on unlikely tokens. When uncertain with many valid options, k=50 might exclude good alternatives. Top-k treats all distributions the same, ignoring the model's confidence level.</p>"},{"location":"decoding_strategies/sampling_methods/#q4-how-is-top-p-better-than-top-k","title":"Q4: How is top-p better than top-k?","text":"<p>Answer: Top-p is adaptive\u2014it automatically adjusts the number of candidate tokens based on distribution shape: - Confident model \u2192 keeps fewer tokens (smaller effective k) - Uncertain model \u2192 keeps more tokens (larger effective k)</p> <p>This makes top-p more robust across different contexts without hyperparameter tuning.</p>"},{"location":"decoding_strategies/sampling_methods/#q5-can-you-use-temperature05-with-top-p09-together","title":"Q5: Can you use temperature=0.5 with top-p=0.9 together?","text":"<p>Answer: Yes, and this is common in practice: 1. Apply temperature=0.5 first \u2192 sharpen distribution (reduce randomness) 2. Apply top-p=0.9 \u2192 filter out low-probability tail 3. Sample from filtered distribution</p> <p>Temperature controls overall randomness, top-p prevents sampling gibberish. Together they provide controlled, safe creativity.</p>"},{"location":"decoding_strategies/sampling_methods/#q6-why-not-just-use-temperature-alone","title":"Q6: Why not just use temperature alone?","text":"<p>Answer: Temperature alone doesn't filter out low-probability tokens\u2014it just reduces their probability. Even with low temperature, there's still a tiny chance of sampling nonsense tokens from the very long tail. Top-p/top-k provide a hard cutoff, ensuring we never sample from clearly bad options.</p>"},{"location":"decoding_strategies/sampling_methods/#q7-what-happens-with-very-high-temperature-t5","title":"Q7: What happens with very high temperature (T=5)?","text":"<p>Answer: The distribution becomes nearly uniform\u2014all tokens have similar probability regardless of model's original confidence. This produces incoherent gibberish:</p> <pre><code>Input: \"The capital of France is\"\nOutput: \"banana quantum seventh pencil\"\n</code></pre> <p>High temperature destroys the model's learned knowledge. Typical max: T=1.5-2.0.</p>"},{"location":"decoding_strategies/sampling_methods/#q8-how-do-you-choose-between-p09-vs-p095","title":"Q8: How do you choose between p=0.9 vs p=0.95?","text":"<p>Answer: - p=0.9: More focused, coherent (default for most applications) - p=0.95: More diverse, creative (for storytelling, brainstorming)</p> <p>Trade-off: Higher p \u2192 more diversity but higher risk of incoherence. Start with 0.9; increase for creativity, decrease for safety. Depends on task requirements.</p>"},{"location":"decoding_strategies/sampling_methods/#q9-whats-the-computational-cost-of-top-p-vs-top-k","title":"Q9: What's the computational cost of top-p vs top-k?","text":"<p>Answer: - Top-k: O(V log k) \u2014 partial sort for top-k elements - Top-p: O(V log V) \u2014 full sort to compute cumulative probabilities</p> <p>Top-p is slightly more expensive, but the difference is negligible compared to model inference cost. The adaptive benefits of top-p outweigh the small computational overhead.</p>"},{"location":"decoding_strategies/sampling_methods/#q10-why-do-modern-llms-gpt-4-claude-prefer-top-p-over-top-k","title":"Q10: Why do modern LLMs (GPT-4, Claude) prefer top-p over top-k?","text":"<p>Answer: Adaptivity and robustness. Top-p automatically adjusts to: - Different contexts (formal vs casual) - Varying model confidence - Different domains (technical vs creative)</p> <p>This makes it more reliable across diverse use cases without manual tuning. Top-k requires choosing k for each scenario, while top-p with p=0.9 works well universally.</p>"},{"location":"decoding_strategies/sampling_methods/#8-comparison-table","title":"8. Comparison Table","text":"Method Randomness Control Adaptive Prevents Tail Sampling Typical Usage Complexity Temperature Continuous (T) No No + top-p/top-k O(V) Top-k Discrete (k) No Yes Legacy, simple control O(V log k) Top-p Continuous (p) Yes Yes Modern LLMs, general use O(V log V) Greedy None N/A N/A Debugging, deterministic O(V) Beam None No N/A Translation, ASR O(K\u00d7V)"},{"location":"decoding_strategies/sampling_methods/#9-practical-guidelines","title":"9. Practical Guidelines","text":""},{"location":"decoding_strategies/sampling_methods/#for-most-applications","title":"For Most Applications","text":"<pre><code># Recommended defaults\ntemperature = 0.8-1.0\ntop_p = 0.9\n# Don't use top-k with top-p\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#for-creative-writing","title":"For Creative Writing","text":"<pre><code>temperature = 1.0-1.5\ntop_p = 0.95\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#for-factual-qa","title":"For Factual Q&amp;A","text":"<pre><code>temperature = 0.3-0.7\ntop_p = 0.9\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#for-code-generation","title":"For Code Generation","text":"<pre><code>temperature = 0.2-0.5\ntop_p = 0.9\n# Or just use greedy (temperature=0)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#10-key-takeaways-for-interviews","title":"10. Key Takeaways for Interviews","text":"<ol> <li>Temperature: Controls randomness by scaling logits; T&lt;1 sharper, T&gt;1 flatter</li> <li>Top-k: Fixed number of candidate tokens; simple but not adaptive</li> <li>Top-p: Adaptive cumulative probability threshold; modern standard</li> <li>Combination: Use temperature + top-p together for best results</li> <li>Top-p advantage: Automatically adjusts to model confidence</li> <li>Common values: temperature=0.8-1.0, top-p=0.9</li> <li>Use case: Sampling for creative/diverse tasks; greedy/beam for deterministic tasks</li> </ol>"},{"location":"decoding_strategies/sampling_methods/#references","title":"References","text":"<ul> <li>The Curious Case of Neural Text Degeneration - Introduces nucleus (top-p) sampling</li> <li>Hugging Face: Generation Strategies</li> <li>OpenAI API: Sampling Parameters</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/","title":"Speculative Decoding - Interview Prep Guide","text":""},{"location":"decoding_strategies/speculative_decoding/#1-overview","title":"1. Overview","text":"<p>Speculative decoding reduces inference latency in autoregressive LLMs by using a small draft model to propose multiple tokens that a large target model verifies in parallel. It produces exact same output distribution as standard decoding\u2014not an approximation.</p> <p>Key insight: Instead of 1 token per expensive model call, verify K tokens in one call by having a cheap model guess ahead.</p> <p>Typical speedup: 2-3x latency reduction with no quality loss</p>"},{"location":"decoding_strategies/speculative_decoding/#2-the-problem-with-standard-decoding","title":"2. The Problem with Standard Decoding","text":""},{"location":"decoding_strategies/speculative_decoding/#sequential-bottleneck","title":"Sequential Bottleneck","text":"<p>Autoregressive generation is inherently sequential:</p> \\[ P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{&lt;t}) \\] <p>Standard decoding loop: <pre><code>for _ in range(max_length):\n    logits = large_model(tokens)      # Expensive!\n    next_token = sample(logits[-1])   # Only use last position\n    tokens.append(next_token)         # Generate 1 token\n</code></pre></p> <p>Problems: - Generate 1 token per forward pass - Model sits mostly idle (memory-bound, not compute-bound) - Latency grows linearly with output length - KV cache helps computation but doesn't break sequential dependency</p> <p>Example: For 100 tokens, need 100 expensive forward passes of the large model.</p>"},{"location":"decoding_strategies/speculative_decoding/#3-how-speculative-decoding-works","title":"3. How Speculative Decoding Works","text":""},{"location":"decoding_strategies/speculative_decoding/#core-idea","title":"Core Idea","text":"<p>Separate token proposal from verification:</p> <ol> <li>Draft model (small, fast): Proposes K tokens autoregressively</li> <li>Target model (large, expensive): Verifies all K tokens in one parallel forward pass</li> <li>Accept/reject: Use rejection sampling to maintain correct distribution</li> </ol>"},{"location":"decoding_strategies/speculative_decoding/#visual-example","title":"Visual Example","text":"<pre><code>Prompt: \"The capital of France is\"\n\nDraft model proposes:  \"Paris , which is\"  (K=4 tokens)\n                        \u2193\nTarget model verifies all 4 tokens in one pass\n                        \u2193\nAccept? [Paris: \u2713] [,: \u2713] [which: \u2713] [is: \u2717]\n                        \u2193\nOutput: \"The capital of France is Paris , which\"\nContinue from \"which\" (3 tokens in 1 target pass instead of 3!)\n</code></pre>"},{"location":"decoding_strategies/speculative_decoding/#4-step-by-step-algorithm","title":"4. Step-by-Step Algorithm","text":""},{"location":"decoding_strategies/speculative_decoding/#setup","title":"Setup","text":"<ul> <li>Draft model \\(q\\): Small, fast (e.g., 1B params)</li> <li>Target model \\(p\\): Large, accurate (e.g., 70B params)</li> <li>Draft length \\(K\\): Number of tokens to propose (typically 4-8)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#step-1-draft-proposes-k-tokens","title":"Step 1: Draft Proposes K Tokens","text":"<p>Draft model generates K tokens autoregressively:</p> \\[ y_i \\sim q(\\cdot \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1, \\dots, K \\] <p>Important: Sample tokens (don't use greedy) and record \\(q(y_i \\mid x, y_{&lt;i})\\) for each.</p> <pre><code># Draft phase (cheap, K sequential calls to small model)\ndraft_tokens = []\ndraft_probs = []\n\nfor i in range(K):\n    logits = draft_model(prompt + draft_tokens)\n    probs = softmax(logits[-1])\n\n    token = sample(probs)  # Sample, not greedy!\n    draft_tokens.append(token)\n    draft_probs.append(probs[token])  # Record q(y_i)\n</code></pre>"},{"location":"decoding_strategies/speculative_decoding/#step-2-target-verifies-all-tokens","title":"Step 2: Target Verifies All Tokens","text":"<p>Target model runs once on the entire sequence:</p> \\[ \\text{Input: } [x, y_1, y_2, \\dots, y_K] \\] <p>This produces probabilities for all draft positions:</p> \\[ p(y_i \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1, \\dots, K \\] <pre><code># Verification phase (1 parallel call to large model)\nfull_sequence = prompt + draft_tokens\ntarget_logits = target_model(full_sequence)  # Shape: [seq_len, vocab_size]\n\n# Extract logits for draft positions only\ntarget_probs = []\nfor i in range(K):\n    pos = len(prompt) + i - 1  # Position before token i\n    probs = softmax(target_logits[pos])\n    target_probs.append(probs[draft_tokens[i]])  # p(y_i)\n</code></pre> <p>Key: Transformer naturally computes logits for all positions\u2014speculative decoding just uses them.</p>"},{"location":"decoding_strategies/speculative_decoding/#step-3-accept-or-reject-each-token","title":"Step 3: Accept or Reject Each Token","text":"<p>Use rejection sampling to maintain correct distribution:</p> \\[ \\alpha_i = \\min\\left(1, \\frac{p(y_i \\mid x, y_{&lt;i})}{q(y_i \\mid x, y_{&lt;i})}\\right) \\] <pre><code>accepted = []\n\nfor i in range(K):\n    # Acceptance probability\n    acceptance_prob = min(1.0, target_probs[i] / draft_probs[i])\n\n    # Random test\n    if random.random() &lt; acceptance_prob:\n        accepted.append(draft_tokens[i])\n    else:\n        # First rejection: stop here\n        break\n</code></pre> <p>Stop at first rejection (can't accept token 3 if token 2 was rejected).</p>"},{"location":"decoding_strategies/speculative_decoding/#step-4-handle-rejection","title":"Step 4: Handle Rejection","text":"<p>If token \\(j\\) is rejected: - Discard \\(y_j, y_{j+1}, \\dots, y_K\\) - Sample replacement from target model at position \\(j\\):</p> \\[ x_{\\text{next}} \\sim p(\\cdot \\mid x, y_{&lt;j}) \\] <pre><code>if len(accepted) &lt; K:  # Some token was rejected\n    # Sample next token from target model\n    rejection_pos = len(accepted)\n    next_token = sample(target_logits[rejection_pos])\n    accepted.append(next_token)\n\n# Continue with accepted tokens\nprompt = prompt + accepted\n</code></pre> <p>If all K tokens accepted, continue with next draft batch.</p>"},{"location":"decoding_strategies/speculative_decoding/#5-why-its-faster","title":"5. Why It's Faster","text":""},{"location":"decoding_strategies/speculative_decoding/#speedup-analysis","title":"Speedup Analysis","text":"<p>Standard decoding for N tokens: - N forward passes of target model - Cost: \\(N \\times C_{\\text{target}}\\)</p> <p>Speculative decoding: - Draft proposes K tokens: \\(K \\times C_{\\text{draft}}\\) (cheap) - Target verifies in 1 pass: \\(1 \\times C_{\\text{target}}\\) - If acceptance rate = \\(\\alpha\\), generate \\(\\alpha \\times K\\) tokens per cycle</p> <p>Expected tokens per target call: $$ E[\\text{tokens}] = 1 + \\sum_{i=1}^{K} \\alpha^i \\approx \\frac{1 - \\alpha^{K+1}}{1 - \\alpha} $$</p> <p>Example: - \\(K=4\\), \\(\\alpha=0.7\\) \u2192 ~2.4 tokens per target call - 2.4\u00d7 speedup (vs 1 token per call in standard decoding)</p>"},{"location":"decoding_strategies/speculative_decoding/#why-parallel-verification-works","title":"Why Parallel Verification Works","text":"<p>Transformers compute logits for all positions in one pass: <pre><code>Input:  [\"The\", \"cat\", \"sat\"]\nOutput: [logits_0, logits_1, logits_2]\n         \u2193         \u2193         \u2193\n      P(\u00b7|\"The\") P(\u00b7|\"The cat\") P(\u00b7|\"The cat sat\")\n</code></pre></p> <p>Standard decoding only uses <code>logits_2</code> (last position). Speculative decoding uses <code>logits_0</code>, <code>logits_1</code>, <code>logits_2</code> to verify multiple tokens.</p> <p>This is not new capability\u2014it's how Transformers always work during training.</p>"},{"location":"decoding_strategies/speculative_decoding/#6-why-its-exact-not-approximate","title":"6. Why It's Exact (Not Approximate)","text":""},{"location":"decoding_strategies/speculative_decoding/#rejection-sampling-proof","title":"Rejection Sampling Proof","text":"<p>The acceptance rule: $$ \\alpha_i = \\min\\left(1, \\frac{p(y_i)}{q(y_i)}\\right) $$</p> <p>is standard rejection sampling:</p> <ul> <li>If \\(p(y_i) \\geq q(y_i)\\): always accept (draft underestimated)</li> <li>If \\(p(y_i) &lt; q(y_i)\\): accept with probability \\(p/q\\) (draft overestimated)</li> </ul> <p>Result: Accepted tokens have distribution \\(p\\), not \\(q\\).</p> <p>Mathematical guarantee: Output distribution is identical to sampling from target model directly.</p>"},{"location":"decoding_strategies/speculative_decoding/#when-rejection-happens","title":"When Rejection Happens","text":"<pre><code>Draft assigns:  q(\"Paris\") = 0.8\nTarget assigns: p(\"Paris\") = 0.9\n\u2192 Accept always (\u03b1 = min(1, 0.9/0.8) = 1.0)\n\nDraft assigns:  q(\"London\") = 0.6\nTarget assigns: p(\"London\") = 0.3\n\u2192 Accept 50% of time (\u03b1 = 0.3/0.6 = 0.5)\n</code></pre> <p>Rejection corrects for draft model errors while maintaining exact target distribution.</p>"},{"location":"decoding_strategies/speculative_decoding/#7-practical-considerations","title":"7. Practical Considerations","text":""},{"location":"decoding_strategies/speculative_decoding/#draft-model-selection","title":"Draft Model Selection","text":"<p>Options: 1. Smaller version of target (e.g., 1B vs 70B parameters) 2. Quantized target model (INT8 vs FP16) 3. Distilled model trained to match target 4. Early-exit from target (use intermediate layers)</p> <p>Requirements: - Fast enough (\u226510\u00d7 faster than target) - Similar enough to target (high acceptance rate)</p>"},{"location":"decoding_strategies/speculative_decoding/#draft-length-k","title":"Draft Length K","text":"<p>Trade-off: - Small K (2-4): Lower overhead, guaranteed speedup - Large K (8-16): Higher potential speedup, but more likely to reject</p> <p>Optimal K depends on: - Acceptance rate (higher \u03b1 \u2192 larger K beneficial) - Draft/target speed ratio - Memory constraints</p> <p>Typical: K=4-5 works well in practice</p>"},{"location":"decoding_strategies/speculative_decoding/#acceptance-rate","title":"Acceptance Rate","text":"<p>Factors affecting \u03b1: - Draft-target model similarity - Task complexity (simple text \u2192 higher \u03b1) - Prompt context (more context \u2192 better draft predictions)</p> <p>Typical rates: - Good draft: \u03b1=0.7-0.9 - Poor draft: \u03b1=0.3-0.5</p> <p>Below \u03b1\u22480.3: Speculative decoding may be slower than standard (overhead dominates).</p>"},{"location":"decoding_strategies/speculative_decoding/#8-interaction-with-kv-cache","title":"8. Interaction with KV Cache","text":""},{"location":"decoding_strategies/speculative_decoding/#kv-cache-in-both-models","title":"KV Cache in Both Models","text":"<p>Draft model: - Maintains its own KV cache - Generates K tokens autoregressively (K cache updates)</p> <p>Target model: - Computes KV cache for entire draft window in one pass - Accepted tokens' KV states are kept - Rejected tokens' KV states are discarded</p>"},{"location":"decoding_strategies/speculative_decoding/#memory-considerations","title":"Memory Considerations","text":"<pre><code>Standard decoding: 1 model's KV cache\nSpeculative decoding: 2 models' KV cache (draft + target)\n</code></pre> <p>Memory cost: Minimal\u2014draft model is small, so its cache is negligible.</p>"},{"location":"decoding_strategies/speculative_decoding/#9-interview-questions","title":"9. Interview Questions","text":""},{"location":"decoding_strategies/speculative_decoding/#q1-what-is-speculative-decoding-and-why-is-it-faster","title":"Q1: What is speculative decoding and why is it faster?","text":"<p>Answer: Speculative decoding uses a small draft model to propose K tokens cheaply, then a large target model verifies all K tokens in one parallel forward pass. Since Transformers compute logits for all positions naturally, we can check multiple draft tokens together. If most are accepted (high acceptance rate), we generate multiple tokens per expensive target model call instead of 1, reducing latency by 2-3\u00d7 while maintaining exact output distribution.</p>"},{"location":"decoding_strategies/speculative_decoding/#q2-does-speculative-decoding-produce-exact-or-approximate-results","title":"Q2: Does speculative decoding produce exact or approximate results?","text":"<p>Answer: Exact. It uses rejection sampling to correct for draft model errors. The acceptance probability \u03b1 = min(1, p(y)/q(y)) ensures accepted tokens have the exact target distribution p, not the draft distribution q. The output is statistically identical to standard decoding with the target model\u2014it's a latency optimization, not an approximation.</p>"},{"location":"decoding_strategies/speculative_decoding/#q3-why-cant-you-just-run-the-target-model-in-parallel-for-k-tokens-directly","title":"Q3: Why can't you just run the target model in parallel for K tokens directly?","text":"<p>Answer: Autoregressive models have a causal dependency\u2014token t depends on all tokens before it (t-1, t-2, ...). You cannot predict token t and token t+1 independently because t+1 needs t as input. Speculative decoding works around this by having the draft model make guesses (which may be wrong), then the target model verifies them all at once (which is valid because verification only requires a single forward pass).</p>"},{"location":"decoding_strategies/speculative_decoding/#q4-how-does-the-target-model-verify-k-tokens-in-one-pass","title":"Q4: How does the target model verify K tokens in one pass?","text":"<p>Answer: Transformers naturally compute logits for all token positions in a forward pass, not just the last one. Given input [x, y\u2081, y\u2082, y\u2083], the model outputs logits for positions 0, 1, 2, 3. Standard decoding only uses the last position. Speculative decoding uses all positions to compute p(y\u2081|x), p(y\u2082|x,y\u2081), p(y\u2083|x,y\u2081,y\u2082) and compare them with the draft probabilities. This is how Transformers work during training\u2014speculative decoding just reuses this during inference.</p>"},{"location":"decoding_strategies/speculative_decoding/#q5-what-happens-when-a-draft-token-is-rejected","title":"Q5: What happens when a draft token is rejected?","text":"<p>Answer: Stop immediately at the first rejection: 1. Discard the rejected token and all subsequent draft tokens 2. Sample a replacement token from the target model at that position 3. Restart speculation from the new sequence</p> <p>For example, if tokens 1,2 are accepted but token 3 is rejected, keep 1,2, sample a new token 3 from target, discard draft tokens 4,5,...,K. This maintains causality and correctness.</p>"},{"location":"decoding_strategies/speculative_decoding/#q6-what-determines-the-acceptance-rate","title":"Q6: What determines the acceptance rate?","text":"<p>Answer: How well the draft model matches the target model's distribution: - High similarity (e.g., quantized version) \u2192 \u03b1=0.8-0.9 \u2192 2-3\u00d7 speedup - Moderate similarity (e.g., smaller architecture) \u2192 \u03b1=0.5-0.7 \u2192 1.5-2\u00d7 speedup - Low similarity \u2192 \u03b1&lt;0.3 \u2192 overhead dominates, possibly slower</p> <p>Also affected by task (simple text has higher \u03b1) and context (more prompt context helps draft predict better).</p>"},{"location":"decoding_strategies/speculative_decoding/#q7-how-do-you-choose-the-draft-length-k","title":"Q7: How do you choose the draft length K?","text":"<p>Answer: Trade-off between potential speedup and overhead: - Higher acceptance rate \u03b1 \u2192 can use larger K (more tokens likely accepted) - Lower \u03b1 \u2192 use smaller K (avoid wasting computation on rejections) - Typical: K=4-5 works well across scenarios</p> <p>Formula: Expected tokens per cycle \u2248 (1-\u03b1^(K+1))/(1-\u03b1). Diminishing returns beyond K=5-8 for most acceptance rates.</p>"},{"location":"decoding_strategies/speculative_decoding/#q8-does-speculative-decoding-reduce-total-flops","title":"Q8: Does speculative decoding reduce total FLOPs?","text":"<p>Answer: No, it increases total FLOPs slightly (due to draft model overhead and potential rejections). The speedup comes from reducing latency\u2014fewer sequential calls to the expensive target model. It's memory-bound optimization: better GPU utilization by verifying multiple tokens in parallel rather than one at a time. Wall-clock time decreases even though total compute increases.</p>"},{"location":"decoding_strategies/speculative_decoding/#q9-can-you-use-greedy-decoding-for-the-draft-model","title":"Q9: Can you use greedy decoding for the draft model?","text":"<p>Answer: No, you must sample from the draft model and record probabilities q(y_i). The rejection sampling formula requires q(y_i) to compute \u03b1 = min(1, p(y_i)/q(y_i)). Greedy decoding doesn't provide a probability distribution over sampled tokens\u2014it just picks argmax. Sampling is essential for the mathematical correctness guarantee.</p>"},{"location":"decoding_strategies/speculative_decoding/#q10-how-does-speculative-decoding-interact-with-kv-cache","title":"Q10: How does speculative decoding interact with KV cache?","text":"<p>Answer: Both models use KV cache: - Draft model: Maintains its own cache, generates K tokens autoregressively - Target model: Computes cache for entire draft window in one pass; keeps cache for accepted tokens, discards for rejected ones</p> <p>Memory overhead is minimal since the draft model is small. KV cache improves efficiency (avoids recomputing attention) but doesn't change the algorithm\u2014it's an orthogonal optimization.</p>"},{"location":"decoding_strategies/speculative_decoding/#10-variants-and-extensions","title":"10. Variants and Extensions","text":""},{"location":"decoding_strategies/speculative_decoding/#medusa-multi-head-speculation","title":"Medusa / Multi-Head Speculation","text":"<ul> <li>Add multiple prediction heads to draft model</li> <li>Each head predicts different future positions in parallel</li> <li>Tree-based verification (multiple candidate paths)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#lookahead-decoding","title":"Lookahead Decoding","text":"<ul> <li>Uses n-gram matching and Jacobi iterations</li> <li>Doesn't require separate draft model</li> <li>Lower speedup but no extra model needed</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#staged-speculative-decoding","title":"Staged Speculative Decoding","text":"<ul> <li>Multiple draft models of increasing size</li> <li>First draft proposes, second refines, target verifies</li> <li>Can achieve higher acceptance rates</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#11-key-takeaways-for-interviews","title":"11. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Draft model proposes K tokens, target verifies all in one pass</li> <li>Speedup source: Multiple tokens per expensive model call (not reduced FLOPs)</li> <li>Exactness: Rejection sampling ensures identical distribution to target model</li> <li>Acceptance rate: Critical metric\u2014need \u03b1&gt;0.5 for practical speedup</li> <li>Parallel verification: Uses existing Transformer capability (logits for all positions)</li> <li>Draft model: Must be 10\u00d7+ faster, doesn't need to be very accurate</li> <li>Typical gain: 2-3\u00d7 latency reduction in production systems</li> <li>Memory: Minimal overhead (draft model is small)</li> </ol>"},{"location":"decoding_strategies/speculative_decoding/#12-when-to-use-speculative-decoding","title":"12. When to Use Speculative Decoding","text":""},{"location":"decoding_strategies/speculative_decoding/#good-use-cases","title":"\u2705 Good Use Cases","text":"<ul> <li>Latency-critical serving (chatbots, real-time applications)</li> <li>Long-form generation (more tokens \u2192 more opportunities for speedup)</li> <li>Good draft model available (smaller version, quantized, distilled)</li> <li>Inference-bound workloads (not training)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#less-beneficial","title":"\u274c Less Beneficial","text":"<ul> <li>Batch inference (already parallelized across sequences)</li> <li>Very short outputs (overhead not amortized)</li> <li>No suitable draft model (acceptance rate too low)</li> <li>Memory-constrained systems (need to load 2 models)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#references","title":"References","text":"<ul> <li>Fast Inference from Transformers via Speculative Decoding - Original paper</li> <li>Accelerating Large Language Model Decoding with Speculative Sampling - DeepMind's version</li> <li>Medusa: Simple LLM Inference Acceleration with Multiple Decoding Heads</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/","title":"Bottleneck Analysis","text":""},{"location":"fundamentals/bottleneck_analysis/#1-understanding-bottlenecks","title":"1. Understanding Bottlenecks","text":""},{"location":"fundamentals/bottleneck_analysis/#the-three-primary-bottlenecks","title":"The Three Primary Bottlenecks","text":"<p>1. Compute-Bound - GPU cores underutilized - Not enough arithmetic operations - Common in: Prefill phase, large batches</p> <p>2. Memory-Bound - GPU cores waiting for data - Memory bandwidth saturated - Common in: Decode phase, small batches</p> <p>3. Overhead-Bound - Framework/system overhead dominates - Kernel launch latency - Common in: Very small models, batch=1</p>"},{"location":"fundamentals/bottleneck_analysis/#2-roofline-model","title":"2. Roofline Model","text":"<pre><code>Attainable Performance = min(Peak Compute, Arithmetic Intensity \u00d7 Memory Bandwidth)\n\nArithmetic Intensity = FLOPs / Bytes Transferred\n\nIf Arithmetic Intensity &lt; Compute/Bandwidth ratio \u2192 Memory-Bound\nIf Arithmetic Intensity &gt; Compute/Bandwidth ratio \u2192 Compute-Bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#example-h100-gpu","title":"Example: H100 GPU","text":"<pre><code>Peak FP16 Compute: 989 TFLOPS\nMemory Bandwidth: 3,350 GB/s\nRatio: 295 FLOP/Byte\n\nOperation with AI=100 FLOP/Byte \u2192 Memory-bound\nOperation with AI=500 FLOP/Byte \u2192 Compute-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#3-identifying-bottlenecks","title":"3. Identifying Bottlenecks","text":""},{"location":"fundamentals/bottleneck_analysis/#method-1-gpu-utilization-metrics","title":"Method 1: GPU Utilization Metrics","text":"<p>Compute Utilization</p> <pre><code>nvidia-smi dmon -s u\n# SM (Streaming Multiprocessor) utilization\n\nHigh SM% (&gt;80%) \u2192 Compute-bound\nLow SM% (&lt;40%) \u2192 Memory or overhead-bound\n</code></pre> <p>Memory Utilization</p> <pre><code>nvidia-smi dmon -s m\n# Memory bandwidth utilization\n\nHigh Mem% (&gt;80%) \u2192 Memory-bound\nLow Mem% (&lt;40%) \u2192 Compute or overhead-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#method-2-profiling-tools","title":"Method 2: Profiling Tools","text":"<p>NVIDIA Nsight Compute</p> <p><pre><code>ncu --set full -o profile python inference.py\n</code></pre> - Shows compute vs memory bottleneck per kernel - Identifies optimization opportunities</p> <p>PyTorch Profiler</p> <pre><code>from torch.profiler import profile, ProfilerActivity\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(input)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n</code></pre> <p>Key Metrics to Check:</p> <ul> <li>Kernel time distribution</li> <li>Memory copy overhead</li> <li>CPU-GPU sync points</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#method-3-microbenchmarks","title":"Method 3: Microbenchmarks","text":"<p>Isolate Operations</p> <pre><code># Test prefill vs decode separately\nprefill_time = benchmark_prefill(prompt_tokens)\ndecode_time = benchmark_decode(num_output_tokens)\n\n# Test different batch sizes\nfor batch_size in [1, 4, 8, 16, 32]:\n    throughput[batch_size] = benchmark(batch_size)\n</code></pre> <p>Expected Results:</p> <ul> <li>Decode: Throughput plateaus early \u2192 Memory-bound</li> <li>Prefill: Throughput scales with batch \u2192 Compute-bound</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#4-common-bottleneck-patterns","title":"4. Common Bottleneck Patterns","text":""},{"location":"fundamentals/bottleneck_analysis/#pattern-1-decode-phase-memory-bound","title":"Pattern 1: Decode Phase (Memory-Bound)","text":"<p>Symptoms:</p> <ul> <li>Low GPU compute utilization (20-40%)</li> <li>High memory bandwidth usage</li> <li>TPOT doesn't improve with smaller model quantization</li> </ul> <p>Root Cause:</p> <pre><code>Single token generation = Load entire weight matrix\nArithmetic Intensity \u2248 1-2 FLOP/Byte (very low)\n</code></pre> <p>Solutions:</p> <ul> <li>Weight quantization (INT8/INT4) \u2192 Reduce bytes transferred</li> <li>Increase batch size \u2192 Amortize weight loading</li> <li>Use higher memory bandwidth GPU (H100 vs A100)</li> <li>Speculative decoding \u2192 Generate multiple tokens</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-2-prefill-phase-compute-bound","title":"Pattern 2: Prefill Phase (Compute-Bound)","text":"<p>Symptoms:</p> <ul> <li>High GPU compute utilization (70-90%)</li> <li>Attention computation dominates</li> <li>Scales well with batch size</li> </ul> <p>Root Cause:</p> <pre><code>Attention: O(n\u00b2d) operations\nLong sequences = Quadratic compute growth\n</code></pre> <p>Solutions:</p> <ul> <li>FlashAttention \u2192 Fused kernel, reduce memory access</li> <li>Tensor parallelism \u2192 Split across GPUs</li> <li>Reduce sequence length if possible</li> <li>Use models with sliding window attention (Mistral)</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-3-kv-cache-transfer-memory-bound","title":"Pattern 3: KV Cache Transfer (Memory-Bound)","text":"<p>Symptoms:</p> <ul> <li>Performance degrades with sequence length</li> <li>Memory copy time visible in profiler</li> </ul> <p>Root Cause:</p> <pre><code>KV cache size = 2 \u00d7 seq_len \u00d7 layers \u00d7 heads \u00d7 dim \u00d7 bytes\nLong sequences = Large cache to copy\n</code></pre> <p>Solutions:</p> <ul> <li>GQA/MQA \u2192 Reduce KV cache size</li> <li>KV cache quantization (INT8) \u2192 2x reduction</li> <li>Paged attention (vLLM) \u2192 Better memory management</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-4-kernel-launch-overhead","title":"Pattern 4: Kernel Launch Overhead","text":"<p>Symptoms:</p> <ul> <li>Low utilization despite small workload</li> <li>Many small kernels in profiler</li> <li>Performance doesn't scale with model size</li> </ul> <p>Root Cause:</p> <pre><code>Each operation launches separate kernel\nOverhead: ~5-20\u03bcs per kernel launch\n</code></pre> <p>Solutions:</p> <ul> <li>Kernel fusion (FlashAttention, torch.compile)</li> <li>Larger batch sizes</li> <li>Use CUDA graphs \u2192 Eliminate launch overhead</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-5-cpu-gpu-synchronization","title":"Pattern 5: CPU-GPU Synchronization","text":"<p>Symptoms:</p> <ul> <li>GPU idle time between operations</li> <li>High \"cudaDeviceSynchronize\" time</li> <li>Low pipeline parallelism</li> </ul> <p>Root Cause:</p> <pre><code>Explicit sync points or implicit Python overhead\nGPU waits for CPU to issue next operation\n</code></pre> <p>Solutions:</p> <ul> <li>Asynchronous operations (CUDA streams)</li> <li>Reduce Python overhead (torch.compile, C++ inference)</li> <li>Pipeline parallelism</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#5-systematic-analysis-framework","title":"5. Systematic Analysis Framework","text":""},{"location":"fundamentals/bottleneck_analysis/#step-1-measure-baseline","title":"Step 1: Measure Baseline","text":"<pre><code>Metrics to collect:\n- Total latency (TTFT + decode time)\n- Tokens per second (throughput)\n- GPU utilization (SM%, Mem%)\n- Memory usage (weights, KV cache, activations)\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#step-2-profile-critical-path","title":"Step 2: Profile Critical Path","text":"<pre><code>Use profiler to identify:\n1. Which operations take most time?\n2. Are they compute or memory-bound?\n3. Where are sync points?\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#step-3-apply-targeted-optimizations","title":"Step 3: Apply Targeted Optimizations","text":"<pre><code>If memory-bound \u2192 Reduce data movement\nIf compute-bound \u2192 Optimize kernels or reduce ops\nIf overhead-bound \u2192 Fuse kernels or increase batch\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#step-4-validate-improvement","title":"Step 4: Validate Improvement","text":"<pre><code>Measure again and compare\nCheck for regressions in quality\nEnsure optimization applies to production workload\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#6-profiling-example-llama-2-7b","title":"6. Profiling Example: LLaMA-2-7B","text":""},{"location":"fundamentals/bottleneck_analysis/#baseline-batch1-seq512","title":"Baseline (Batch=1, Seq=512)","text":"<pre><code>Operation          | Time (ms) | % Total | Bottleneck\n-------------------|-----------|---------|------------\nAttention          | 8.2       | 45%     | Memory\nFFN                | 6.5       | 35%     | Memory\nLayer Norm         | 1.8       | 10%     | Overhead\nKV Cache Update    | 1.2       | 7%      | Memory\nMisc               | 0.5       | 3%      | -\n-------------------|-----------|---------|------------\nTotal              | 18.2      | 100%    | Memory-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#after-optimization","title":"After Optimization","text":"<pre><code>Applied: FlashAttention, INT8 quantization, kernel fusion\n\nOperation          | Time (ms) | % Total | Change\n-------------------|-----------|---------|--------\nAttention (Flash)  | 4.1       | 40%     | -50%\nFFN (INT8)         | 3.8       | 37%     | -42%\nLayer Norm (fused) | 0.9       | 9%      | -50%\nKV Cache Update    | 1.0       | 10%     | -17%\nMisc               | 0.4       | 4%      | -20%\n-------------------|-----------|---------|--------\nTotal              | 10.2      | 100%    | -44%\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#7-common-interview-questions","title":"7. Common Interview Questions","text":"<p>Q: How do you determine if inference is compute or memory-bound?</p> <pre><code>1. Check GPU metrics (SM% vs Mem%)\n2. Profile with Nsight Compute (SOL Compute vs SOL Memory)\n3. Test batch size scaling:\n   - Compute-bound: Scales well with batch\n   - Memory-bound: Plateaus quickly\n4. Calculate arithmetic intensity vs hardware ratio\n</code></pre> <p>Q: GPU shows 100% utilization but throughput is low. Why?</p> <ul> <li>Could be memory-bound (100% memory utilization)</li> <li>Check if memory bandwidth saturated</li> <li>Verify you're looking at the right metric (compute vs memory)</li> <li>Could be inefficient kernels (high utilization, low throughput)</li> </ul> <p>Q: Describe how you'd optimize a memory-bound decode phase</p> <pre><code>1. Profile to confirm bottleneck (low SM%, high Mem%)\n2. Quantize weights (INT8) \u2192 2x less data to transfer\n3. Increase batch size \u2192 Better memory bandwidth utilization\n4. Use H100 instead of A100 \u2192 1.7x more bandwidth\n5. Consider speculative decoding \u2192 Reduce number of decode steps\n</code></pre> <p>Q: What's the impact of FlashAttention on prefill vs decode?</p> <pre><code>Prefill (Compute-bound):\n- Reduces memory access (no full attention matrix)\n- Enables longer sequences without OOM\n- 2-4x speedup typical\n\nDecode (Memory-bound):\n- Smaller benefit (already memory-bound on weights)\n- Still helpful for very long context\n- ~20-30% speedup\n</code></pre> <p>Q: How do you profile Python overhead vs GPU computation?</p> <pre><code># Method 1: Compare with/without CUDA sync\nimport time\n\n# With sync (includes Python overhead)\nt0 = time.time()\noutput = model(input)\ntorch.cuda.synchronize()\nt1 = time.time()\n\n# With events (pure GPU time)\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\nstart.record()\noutput = model(input)\nend.record()\ntorch.cuda.synchronize()\ngpu_time = start.elapsed_time(end)\n\npython_overhead = (t1 - t0) - (gpu_time / 1000)\n</code></pre> <p>Q: Explain the roofline model for LLM inference</p> <pre><code>Roofline: Performance = min(Compute Peak, Bandwidth \u00d7 Arithmetic Intensity)\n\nExample: Decode single token on H100\n- Matmul: [1, 4096] \u00d7 [4096, 4096]\n- FLOPs: 2 \u00d7 1 \u00d7 4096 \u00d7 4096 \u2248 33M\n- Bytes: (4096\u00d74096 + 4096\u00d74096) \u00d7 2 (FP16) \u2248 67MB\n- AI: 33M / 67M \u2248 0.5 FLOP/Byte\n\nH100: 989 TFLOPS, 3350 GB/s \u2192 295 FLOP/Byte ratio\nAI (0.5) &lt;&lt; Ratio (295) \u2192 Memory-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#8-advanced-profiling","title":"8. Advanced Profiling","text":""},{"location":"fundamentals/bottleneck_analysis/#tensor-core-utilization","title":"Tensor Core Utilization","text":"<pre><code>ncu --metrics sm__sass_thread_inst_executed_op_dmma_inst,sm__sass_thread_inst_executed_op_hmma_inst\n\nCheck if matmuls use tensor cores (TC)\nLow TC usage \u2192 Not using FP16/BF16 or improper dims\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#memory-transaction-efficiency","title":"Memory Transaction Efficiency","text":"<pre><code>ncu --metrics l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum\n\nEfficiency = Sectors / (Requests \u00d7 4)\nLow efficiency \u2192 Uncoalesced memory access\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#9-key-takeaways","title":"9. Key Takeaways","text":"<ol> <li>Always profile before optimizing - Don't guess the bottleneck</li> <li>Different phases have different bottlenecks: Prefill (compute), Decode (memory)</li> <li>Use the right metric: SM% for compute, Mem% for memory</li> <li>Batch size is a key diagnostic: Scaling behavior reveals bottleneck</li> <li>Optimization must target the actual bottleneck: Memory optimization won't help compute-bound workload</li> <li>Modern GPUs shift bottlenecks: H100's higher compute/bandwidth ratio changes optimization strategy</li> </ol>"},{"location":"fundamentals/inference_basics/","title":"Inference Basics","text":""},{"location":"fundamentals/inference_basics/#1-core-concepts","title":"1. Core Concepts","text":""},{"location":"fundamentals/inference_basics/#autoregressive-generation","title":"Autoregressive Generation","text":"<ul> <li>LLMs generate tokens sequentially: P(token_t | token_1, ..., token_{t-1})</li> <li>Each token requires full model forward pass</li> <li>Output of step t becomes input for step t+1</li> </ul>"},{"location":"fundamentals/inference_basics/#two-phase-inference","title":"Two-Phase Inference","text":"<p>Prefill Phase (Prompt Processing)</p> <ul> <li>Process entire input prompt in parallel</li> <li>Compute KV cache for all input tokens</li> <li>Computationally intensive, compute-bound</li> <li>Time complexity: O(n\u00b2d) for n tokens, d dimensions</li> </ul> <p>Decode Phase (Token Generation)</p> <ul> <li>Generate one token at a time</li> <li>Reuse cached KV from previous tokens</li> <li>Memory-bound operation (fetching weights/KV cache)</li> <li>Continues until EOS token or max length</li> </ul>"},{"location":"fundamentals/inference_basics/#key-metrics","title":"Key Metrics","text":"<pre><code>Time to First Token (TTFT) = Prefill time\nTime Per Output Token (TPOT) = Average decode time per token\nTotal Latency = TTFT + (num_output_tokens \u00d7 TPOT)\n</code></pre>"},{"location":"fundamentals/inference_basics/#2-model-architecture-components","title":"2. Model Architecture Components","text":""},{"location":"fundamentals/inference_basics/#transformer-blocks","title":"Transformer Blocks","text":"<ul> <li>Multi-head self-attention: O(n\u00b2d) complexity</li> <li>Feed-forward network: O(nd_ff) where d_ff \u2248 4d</li> <li>Layer normalization</li> <li>Residual connections</li> </ul>"},{"location":"fundamentals/inference_basics/#kv-cache","title":"KV Cache","text":"<ul> <li>Stores key/value matrices from previous tokens</li> <li>Size per layer: 2 \u00d7 batch_size \u00d7 seq_len \u00d7 num_heads \u00d7 head_dim \u00d7 2 bytes (FP16)</li> <li>Example: LLaMA-2-7B with seq_len=2048, batch=1</li> <li>Per layer: 2 \u00d7 1 \u00d7 2048 \u00d7 32 \u00d7 128 \u00d7 2 \u2248 33 MB</li> <li>Total (32 layers): ~1 GB</li> </ul>"},{"location":"fundamentals/inference_basics/#3-memory-requirements","title":"3. Memory Requirements","text":"<pre><code>Total Memory = Model Weights + KV Cache + Activations + Overhead\n\nModel Weights = num_params \u00d7 bytes_per_param\n- FP32: 4 bytes, FP16: 2 bytes, INT8: 1 byte, INT4: 0.5 bytes\n\nActivations = temporary tensors during forward pass\nOverhead = CUDA context, fragmentation (~10-20%)\n</code></pre> <p>Example: LLaMA-2-7B (FP16) - Weights: 7B \u00d7 2 = 14 GB - KV cache (batch=1, seq=2048): ~1 GB - Activations: ~0.5-1 GB - Total: ~16-17 GB</p>"},{"location":"fundamentals/inference_basics/#4-common-interview-questions","title":"4. Common Interview Questions","text":"<p>Q: Why is prefill compute-bound and decode memory-bound?</p> <ul> <li>Prefill: Process many tokens in parallel \u2192 high arithmetic intensity, GPU cores saturated</li> <li>Decode: Generate 1 token \u2192 fetch entire model weights from memory, low compute utilization</li> </ul> <p>Q: How does batch size affect inference? - Prefill: Higher batch increases compute, remains compute-bound - Decode: Higher batch increases memory for KV cache, can become compute-bound with large batches - Sweet spot: Balance between throughput and latency</p> <p>Q: What limits maximum sequence length?</p> <ul> <li>KV cache memory grows linearly with sequence length</li> <li>Attention computation grows quadratically O(n\u00b2)</li> <li>GPU memory capacity is primary constraint</li> </ul> <p>Q: Calculate memory for Mistral-7B (FP16) with batch=4, seq=4096?</p> <pre><code>Weights: 7B \u00d7 2 = 14 GB\nKV cache: 2 \u00d7 4 \u00d7 4096 \u00d7 32 \u00d7 128 \u00d7 2 \u00d7 32 layers \u2248 8 GB\nTotal: ~22-24 GB\n</code></pre> <p>Q: Why can't we parallelize token generation?</p> <ul> <li>Each token depends on all previous tokens</li> <li>Autoregressive dependency prevents parallelization</li> <li>Speculative decoding attempts to work around this</li> </ul>"},{"location":"fundamentals/inference_basics/#5-modern-optimizations-2024-2025","title":"5. Modern Optimizations (2024-2025)","text":"<ul> <li>Grouped Query Attention (GQA): Reduce KV cache by sharing KV heads</li> <li>Multi-Query Attention (MQA): Single KV head for all queries</li> <li>FlashAttention-3: Fused attention kernel, 2x faster on H100</li> <li>Paged Attention (vLLM): Non-contiguous KV cache storage</li> <li>Continuous Batching: Dynamic batch assembly for throughput</li> </ul>"},{"location":"fundamentals/inference_basics/#6-key-takeaways","title":"6. Key Takeaways","text":"<ol> <li>Inference has distinct prefill (parallel) and decode (sequential) phases</li> <li>KV cache is crucial for avoiding recomputation but consumes significant memory</li> <li>Memory bandwidth is often the bottleneck during decode</li> <li>Model size, sequence length, and batch size determine memory requirements</li> <li>Understanding the compute vs memory bound distinction is critical</li> </ol>"},{"location":"fundamentals/latency_vs_throughput/","title":"Latency vs Throughput","text":""},{"location":"fundamentals/latency_vs_throughput/#1-core-concepts","title":"1. Core Concepts","text":""},{"location":"fundamentals/latency_vs_throughput/#latency","title":"Latency","text":"<ul> <li>Time to complete a single request</li> <li>Measured in seconds or milliseconds</li> <li>Critical for interactive applications (chatbots, code completion)</li> <li>Key metrics: TTFT, TPOT, E2E latency</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#throughput","title":"Throughput","text":"<ul> <li>Number of requests processed per unit time</li> <li>Measured in tokens/sec or requests/sec</li> <li>Critical for batch processing, high-traffic services</li> <li>Maximize GPU utilization</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#2-the-fundamental-tradeoff","title":"2. The Fundamental Tradeoff","text":"<pre><code>Latency \u2191 as Throughput \u2191\n\nHigher batch size \u2192 Higher throughput, Higher latency per request\nLower batch size \u2192 Lower latency, Lower throughput\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#why-they-conflict","title":"Why They Conflict","text":"<p>Batching Increases Throughput</p> <ul> <li>Process multiple requests simultaneously</li> <li>Better GPU utilization (more parallel work)</li> <li>Amortize weight loading overhead</li> </ul> <p>But Hurts Latency</p> <ul> <li>Requests wait for entire batch to complete</li> <li>Queueing delays increase</li> <li>Stragglers slow down entire batch</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#3-key-metrics","title":"3. Key Metrics","text":"<pre><code>Latency = Queue Time + Processing Time\nThroughput = Batch Size / Processing Time (ignoring queue)\n\nUtilization = (Actual Throughput) / (Max Theoretical Throughput)\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#4-optimization-strategies","title":"4. Optimization Strategies","text":""},{"location":"fundamentals/latency_vs_throughput/#for-low-latency-100ms","title":"For Low Latency (&lt; 100ms)","text":"<p>Batch Size = 1 or Small</p> <ul> <li>Minimize queueing delay</li> <li>Accept lower GPU utilization</li> <li>Use smaller models (7B vs 70B)</li> <li>Quantization (INT8/INT4) for faster decode</li> </ul> <p>Prefill Optimization</p> <ul> <li>FlashAttention for faster attention</li> <li>Tensor parallelism to split model across GPUs</li> </ul> <p>Infrastructure</p> <ul> <li>Low-latency network</li> <li>GPU with high memory bandwidth (H100 &gt; A100)</li> <li>Close to users (edge deployment)</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#for-high-throughput","title":"For High Throughput","text":"<p>Large Batch Sizes</p> <ul> <li>Batch 32-128+ requests</li> <li>Maximize GPU compute utilization</li> <li>Accept seconds of latency per request</li> </ul> <p>Continuous Batching</p> <ul> <li>Don't wait for all sequences to finish</li> <li>Insert new requests as others complete</li> <li>Used by vLLM, TensorRT-LLM</li> </ul> <p>Paged Attention (vLLM)</p> <ul> <li>Reduce memory fragmentation</li> <li>Pack more sequences in memory</li> <li>Enable larger effective batch size</li> </ul> <p>Chunked Prefill</p> <ul> <li>Split long prefills into chunks</li> <li>Interleave with decode steps</li> <li>Balance latency and throughput</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#5-request-level-batching-strategies","title":"5. Request-Level Batching Strategies","text":""},{"location":"fundamentals/latency_vs_throughput/#static-batching","title":"Static Batching","text":"<ul> <li>Wait for batch to fill before processing</li> <li>Simple but high latency variance</li> <li>Wasted time if batch doesn't fill</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#continuous-batching","title":"Continuous Batching","text":"<pre><code>t=0: Start batch [A, B, C]\nt=1: A finishes \u2192 add D \u2192 [B, C, D]\nt=2: B finishes \u2192 add E \u2192 [C, D, E]\n</code></pre> <ul> <li>Dynamic batch composition</li> <li>Much better GPU utilization</li> <li>Lower average latency</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#priority-queuing","title":"Priority Queuing","text":"<ul> <li>Process short/urgent requests first</li> <li>Separate queues for interactive vs batch</li> <li>SLO-aware scheduling</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#6-hardware-considerations","title":"6. Hardware Considerations","text":""},{"location":"fundamentals/latency_vs_throughput/#a100-80gb","title":"A100 (80GB)","text":"<ul> <li>1,935 GB/s memory bandwidth</li> <li>Good for batch inference</li> <li>Throughput: ~2000 tokens/sec (LLaMA-2-7B, batch=32)</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#h100-80gb","title":"H100 (80GB)","text":"<ul> <li>3,350 GB/s memory bandwidth (1.7x A100)</li> <li>Better for both latency and throughput</li> <li>FlashAttention-3 support</li> <li>Throughput: ~3500 tokens/sec (same setup)</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#l40s-l4","title":"L40S / L4","text":"<ul> <li>Lower cost, lower bandwidth</li> <li>Good for latency-optimized serving (small batch)</li> <li>Not ideal for high throughput</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#7-common-interview-questions","title":"7. Common Interview Questions","text":"<p>Q: You have 1000 QPS (queries per second). Optimize for p99 latency &lt; 200ms. How?</p> <ul> <li>Use continuous batching (vLLM)</li> <li>Target small effective batch (4-8)</li> <li>Replica scaling with load balancer</li> <li>Monitor queue depth, scale if needed</li> </ul> <p>Q: Batch size 1 vs 32: compare latency and throughput</p> <pre><code>Batch=1:\n- Latency: ~50ms\n- Throughput: ~20 tokens/sec\n- GPU utilization: ~15%\n\nBatch=32:\n- Latency: ~800ms (includes queueing)\n- Throughput: ~500 tokens/sec\n- GPU utilization: ~80%\n</code></pre> <p>Q: How does continuous batching improve over static?</p> <ul> <li>No waiting for batch to fill</li> <li>No wasted cycles when sequences finish at different times</li> <li>Typically, 2-3x better throughput at similar latency</li> </ul> <p>Q: When would you choose latency over throughput?</p> <ul> <li>Real-time chat applications</li> <li>Code completion (100-200ms target)</li> <li>Interactive agents</li> <li>Premium API tiers</li> </ul> <p>Q: When would you choose throughput over latency?</p> <ul> <li>Offline batch processing</li> <li>Data labeling/annotation</li> <li>Embedding generation</li> <li>Document summarization at scale</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#8-production-patterns-2024-2025","title":"8. Production Patterns (2024-2025)","text":""},{"location":"fundamentals/latency_vs_throughput/#multi-tier-serving","title":"Multi-Tier Serving","text":"<pre><code>Tier 1 (Latency): Small models, batch=1-4, edge deployment\nTier 2 (Balanced): Medium models, continuous batching, batch=8-16\nTier 3 (Throughput): Large models, large batches, datacenter\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#speculative-decoding","title":"Speculative Decoding","text":"<ul> <li>Draft model generates multiple tokens</li> <li>Target model verifies in parallel</li> <li>2-3x speedup with same latency</li> <li>Best for latency-sensitive scenarios</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#disaggregated-serving-splitwise","title":"Disaggregated Serving (Splitwise)","text":"<ul> <li>Separate prefill and decode clusters</li> <li>Prefill: GPU compute optimized (A100)</li> <li>Decode: Memory bandwidth optimized (H100)</li> <li>Transfer KV cache between clusters</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#9-key-metrics-to-monitor","title":"9. Key Metrics to Monitor","text":"<pre><code>P50, P95, P99 Latency - Distribution matters\nThroughput (tokens/sec) - Absolute capacity\nQueue Depth - Leading indicator of overload\nGPU Utilization - Efficiency metric\nCost per 1M tokens - Business metric\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#10-benchmarking-tips","title":"10. Benchmarking Tips","text":"<ul> <li>Measure real production traffic patterns</li> <li>Include cold start times if relevant</li> <li>Test at different concurrency levels</li> <li>Monitor long-tail latency (p99, p99.9)</li> <li>Account for sequence length variance</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#11-key-takeaways","title":"11. Key Takeaways","text":"<ol> <li>Latency and throughput are inversely related via batching</li> <li>Continuous batching is standard for production (vLLM, TRT-LLM)</li> <li>Different use cases need different optimization targets</li> <li>Hardware choice matters: H100 better for both metrics vs A100</li> <li>Monitor distributions (p99), not just averages</li> </ol>"},{"location":"fundamentals/memory_compute_tradeoffs/","title":"Memory Compute Tradeoffs","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#1-the-core-tradeoff","title":"1. The Core Tradeoff","text":"<p>Memory Savings \u2192 Compute Overhead (Usually)</p> <p>Techniques that reduce memory often require:</p> <ul> <li>Additional computation (quantization/dequantization)</li> <li>Recomputation instead of caching</li> <li>More complex kernels</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#2-memory-bottlenecks-in-llm-inference","title":"2. Memory Bottlenecks in LLM Inference","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#1-model-weights-static","title":"1. Model Weights (Static)","text":"<ul> <li>70B model in FP16: 140 GB</li> <li>Must fit in GPU memory</li> <li>Loaded repeatedly during decode (memory bandwidth bound)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#2-kv-cache-dynamic","title":"2. KV Cache (Dynamic)","text":"<ul> <li>Grows with sequence length and batch size</li> <li>Often largest memory consumer in production</li> <li>Formula: <code>2 \u00d7 B \u00d7 S \u00d7 L \u00d7 H \u00d7 D \u00d7 bytes</code></li> <li>B=batch, S=seq_len, L=layers, H=heads, D=head_dim</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#3-activations-temporary","title":"3. Activations (Temporary)","text":"<ul> <li>Intermediate tensors during forward pass</li> <li>Recomputed in inference (no backprop needed)</li> <li>~5-10% of total memory</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#3-quantization-trading-precision-for-memory","title":"3. Quantization: Trading Precision for Memory","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#weight-quantization","title":"Weight Quantization","text":"<p>FP16 \u2192 INT8 (8-bit)</p> <ul> <li>2x memory reduction (2 bytes \u2192 1 byte)</li> <li>Minimal accuracy loss (&lt;1% typically)</li> <li>Faster on hardware with INT8 support (Tensor Cores)</li> <li>Compute: Dequantize to FP16 for matmul (overhead ~10%)</li> </ul> <p>FP16 \u2192 INT4 (4-bit)</p> <ul> <li>4x memory reduction</li> <li>Quality degradation possible (1-3% on benchmarks)</li> <li>Requires calibration data</li> <li>Compute: More dequant overhead (~20-30%)</li> </ul> <p>Techniques:</p> <pre><code>Per-Tensor: Single scale for entire tensor\nPer-Channel: Scale per output channel (better quality)\nGroup Quantization: Scale per 128 elements (GPTQ, AWQ)\n\nGPTQ: Layer-wise quantization, minimizes error\nAWQ: Activation-aware, protects important weights\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#kv-cache-quantization","title":"KV Cache Quantization","text":"<ul> <li>KV cache in INT8 instead of FP16</li> <li>2x memory savings \u2192 2x larger batch or sequence length</li> <li>Quality loss typically &lt;0.5%</li> <li>Growing adoption in production (2024+)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#mixed-precision","title":"Mixed Precision","text":"<ul> <li>Keep critical layers in FP16 (first/last, attention)</li> <li>Quantize FFN layers to INT4</li> <li>Balance quality and memory</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#4-kv-cache-optimization","title":"4. KV Cache Optimization","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#multi-query-attention-mqa","title":"Multi-Query Attention (MQA)","text":"<pre><code>Standard: num_kv_heads = num_query_heads (e.g., 32)\nMQA: num_kv_heads = 1\n\nMemory reduction: 32x fewer KV parameters\nTradeoff: Slight quality degradation\nUsed in: Falcon, StarCoder\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#grouped-query-attention-gqa","title":"Grouped Query Attention (GQA)","text":"<pre><code>num_kv_heads &lt; num_query_heads\nExample: 8 KV heads, 32 query heads (4 queries per KV)\n\nMemory reduction: 4x fewer KV parameters\nTradeoff: Minimal quality loss\nUsed in: LLaMA-2, Mistral, GPT-4 (rumored)\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#paged-attention-vllm","title":"Paged Attention (vLLM)","text":"<ul> <li>KV cache in non-contiguous \"pages\" (like OS virtual memory)</li> <li>Eliminates fragmentation</li> <li>Enables ~2x higher batch size for same memory</li> <li>Compute: Slight overhead for page lookup</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#multi-token-prediction","title":"Multi-Token Prediction","text":"<ul> <li>Cache prefixes for common prompts</li> <li>Reduces redundant computation</li> <li>Memory: Store prompt KV cache (shared across requests)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#4-recomputation-vs-caching","title":"4. Recomputation vs Caching","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#activation-checkpointing-training","title":"Activation Checkpointing (Training)","text":"<ul> <li>Not used in inference (no backprop)</li> <li>Mentioned for completeness</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#selective-recomputation","title":"Selective Recomputation","text":"<ul> <li>Recompute cheap operations instead of storing</li> <li>Example: Recompute layer norm instead of caching</li> <li>Memory savings: ~10-20%</li> <li>Compute overhead: ~5-10%</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#5-model-architecture-choices","title":"5. Model Architecture Choices","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#width-vs-depth","title":"Width vs Depth","text":"<pre><code>Wide: More hidden dimensions, fewer layers\n- More memory for weights\n- Less memory for KV cache (fewer layers)\n\nDeep: More layers, smaller hidden dimensions\n- Less memory for weights  \n- More memory for KV cache (more layers)\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#ffn-expansion-ratio","title":"FFN Expansion Ratio","text":"<ul> <li>Standard: <code>d_ff = 4 \u00d7 d_model</code></li> <li>Smaller ratio (2x or 3x): Less memory, potential quality loss</li> <li>MoE: Sparse activation, more parameters but same compute</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#6-hardware-specific-tradeoffs","title":"6. Hardware-Specific Tradeoffs","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#memory-bandwidth-vs-compute","title":"Memory Bandwidth vs Compute","text":"<pre><code>A100: 1,935 GB/s bandwidth, 312 TFLOPS (FP16)\nH100: 3,350 GB/s bandwidth, 989 TFLOPS (FP16)\n\nBandwidth-to-Compute Ratio:\nA100: 6.2 GB/s per TFLOP\nH100: 3.4 GB/s per TFLOP\n</code></pre> <p>Implication: H100 relatively more compute-bound, benefits more from quantization compute overhead</p>"},{"location":"fundamentals/memory_compute_tradeoffs/#tensor-core-utilization","title":"Tensor Core Utilization","text":"<ul> <li>FP16: Full tensor core speed</li> <li>INT8: 2x faster on Ampere/Hopper with DP4A</li> <li>INT4: 4x faster (requires specialized kernels)</li> </ul> <p>Tradeoff: Quantization compute overhead offset by faster matmul</p>"},{"location":"fundamentals/memory_compute_tradeoffs/#7-memory-compute-decision-matrix","title":"7. Memory-Compute Decision Matrix","text":"Technique Memory Saved Compute Overhead Quality Impact INT8 Quantization 2x +10% &lt;1% INT4 Quantization 4x +30% 1-3% GQA (4:1) 4x KV cache Minimal &lt;0.5% MQA 32x KV cache Minimal 1-2% KV Cache INT8 2x KV cache +5% &lt;0.5% FlashAttention Minimal -30% latency None"},{"location":"fundamentals/memory_compute_tradeoffs/#8-common-interview-questions","title":"8. Common Interview Questions","text":"<p>Q: You have a 70B model but only 40GB GPU memory. What do you do?</p> <pre><code>Options:\n1. INT4 quantization: 140GB \u2192 35GB \u2713\n2. INT8 + model parallelism across 2 GPUs\n3. Offload layers to CPU (slow, not recommended)\n4. Use smaller model variant (13B/7B)\n</code></pre> <p>Q: Explain the tradeoff in GQA (Grouped Query Attention)</p> <ul> <li>Save memory: Fewer KV heads \u2192 smaller KV cache</li> <li>Minimal compute overhead: Attention computation slightly changes</li> <li>Quality: Negligible impact (&lt;0.5% on benchmarks)</li> <li>Production: Widely adopted (Mistral, LLaMA-2)</li> </ul> <p>Q: Why is decode phase memory-bound?</p> <ul> <li>Single token generation: Low arithmetic intensity</li> <li>Must fetch entire weight matrix from memory</li> <li>Memory bandwidth saturated, compute underutilized</li> <li>Arithmetic Intensity: FLOPs / Bytes Loaded \u2248 1-2 (very low)</li> </ul> <p>Q: When does quantization hurt performance?</p> <ul> <li>Small batch size: Dequant overhead dominates</li> <li>Compute-bound workloads: Adding compute makes it worse</li> <li>Old hardware: No INT8 tensor core support</li> <li>Generally: Decode phase on modern GPUs (H100) benefits from quantization</li> </ul> <p>Q: Calculate KV cache size: LLaMA-2-70B, batch=16, seq=4096, FP16</p> <pre><code>GQA with 8 KV heads (70B uses this)\n2 \u00d7 16 \u00d7 4096 \u00d7 80_layers \u00d7 8_heads \u00d7 128_dim \u00d7 2_bytes\n= 2 \u00d7 16 \u00d7 4096 \u00d7 80 \u00d7 8 \u00d7 128 \u00d7 2\n= 1,073,741,824 bytes \u2248 1 GB per sample \u00d7 16 = 16 GB total\n\n(If standard MHA with 64 heads: 128 GB - impractical!)\n</code></pre> <p>Q: How does FlashAttention affect memory-compute tradeoff?</p> <ul> <li>Reduces memory: Avoids materializing full attention matrix</li> <li>Reduces compute time: Fused kernel, better cache locality</li> <li>Win-win: Memory AND compute improvement</li> <li>No quality impact (mathematically equivalent)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#9-modern-techniques-2024-2025","title":"9. Modern Techniques (2024-2025)","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#awq-activation-aware-weight-quantization","title":"AWQ (Activation-Aware Weight Quantization)","text":"<ul> <li>Protect weights with high activation magnitude</li> <li>Better quality than naive INT4</li> <li>Used in production (Hugging Face TGI)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#smoothquant","title":"SmoothQuant","text":"<ul> <li>Migrate difficulty from weights to activations</li> <li>Enables better INT8 quantization</li> <li>Particularly for older models not trained for quantization</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#fp8-h100","title":"FP8 (H100)","text":"<ul> <li>Native FP8 support on Hopper</li> <li>2x memory saving vs FP16</li> <li>Minimal quality loss</li> <li>Compute: Faster than FP16 (2x with tensor cores)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#quip-aqlm","title":"QuIP# / AQLM","text":"<ul> <li>Extreme quantization (2-3 bits)</li> <li>Lattice-based, better than naive 2-bit</li> <li>Research stage, not production yet</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#10-practical-guidelines","title":"10. Practical Guidelines","text":"<ol> <li>Start with INT8: Minimal quality loss, 2x memory saving</li> <li>Use GQA architecture: If designing new models</li> <li>Enable KV cache quantization: Production-ready in vLLM</li> <li>FlashAttention is mandatory: No downside</li> <li>INT4 for large models: When GPU memory is the constraint</li> <li>Monitor quality: Always benchmark on your task</li> </ol>"},{"location":"fundamentals/memory_compute_tradeoffs/#11-key-takeaways","title":"11. Key Takeaways","text":"<ol> <li>Most memory optimizations have negligible compute cost (GQA, FlashAttention)</li> <li>Quantization is a clear win on modern hardware (INT8 tensor cores)</li> <li>KV cache often dominates memory in long-context scenarios</li> <li>Decode phase is memory-bound: Reducing memory access helps latency</li> <li>Hardware matters: H100 handles quantization overhead better than A100</li> </ol>"},{"location":"quantization/awq/","title":"AWQ","text":""},{"location":"quantization/awq/#1-paper","title":"1. Paper","text":"<p>Lin et al., 2023 - \"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\"</p>"},{"location":"quantization/awq/#2-core-insight","title":"2. Core Insight","text":"<p>Not all weights are equal - 1% of salient weights (channels) matter disproportionately for model quality.</p>"},{"location":"quantization/awq/#3-key-observation","title":"3. Key Observation","text":"<p>Salient weight channels correlate with large activation magnitudes. Protect these during quantization.</p>"},{"location":"quantization/awq/#4-algorithm","title":"4. Algorithm","text":""},{"location":"quantization/awq/#1-identify-salient-channels","title":"1. Identify Salient Channels","text":"<pre><code># Collect activation statistics\nsalient_scores = activation_magnitude.mean(dim=samples)\n# Top 1% channels by activation magnitude\n</code></pre>"},{"location":"quantization/awq/#2-per-channel-scaling","title":"2. Per-Channel Scaling","text":"<pre><code># Scale up salient weights BEFORE quantization\ns = compute_optimal_scale(W, X)  # Based on activation distribution\nW_scaled = W * s  # Per-channel scale\nW_quantized = quantize(W_scaled)\n\n# At inference: \n# output = (W_quantized / s) @ X = W_quantized @ (X * s)\n# Move scaling to activations (cheap)\n</code></pre>"},{"location":"quantization/awq/#3-search-optimal-scales","title":"3. Search Optimal Scales","text":"<p>Minimize: <code>||W\u00b7X - Q(W\u00b7s)\u00b7(X/s)||</code></p> <p>Grid search over s \u2208 [0.5, 1.5] per channel</p>"},{"location":"quantization/awq/#5-why-it-works","title":"5. Why It Works","text":"<ul> <li>Increases effective resolution for important weights</li> <li>Shifts dynamic range to match activation distribution</li> <li>Quantization error on salient weights reduced by 2-4\u00d7</li> </ul>"},{"location":"quantization/awq/#6-specifications","title":"6. Specifications","text":"<p>Calibration: 128 samples Quantization Time: ~10 minutes for 7B model (much faster than GPTQ) Group Size: 128 typical Bits: Optimized for 4-bit, works for 3-bit</p>"},{"location":"quantization/awq/#7-performance-comparison","title":"7. Performance Comparison","text":"Method LLaMA-7B (4-bit) Quantization Time RTN 73.2 PPL seconds GPTQ 68.4 PPL 4 hours AWQ 68.1 PPL 10 min"},{"location":"quantization/awq/#8-advantages-over-gptq","title":"8. Advantages over GPTQ","text":"<ol> <li>Speed: 20-30\u00d7 faster quantization</li> <li>Simplicity: No Hessian computation</li> <li>Hardware-friendly: Simple per-channel scales</li> </ol>"},{"location":"quantization/awq/#9-tinychat-integration","title":"9. TinyChat Integration","text":"<p>AWQ includes custom CUDA kernels for efficient INT4 inference: - Fused dequantization + GEMM - 3-4\u00d7 speedup over FP16 on consumer GPUs</p>"},{"location":"quantization/awq/#10-implementation","title":"10. Implementation","text":"<pre><code>from awq import AutoAWQForCausalLM\n\nmodel = AutoAWQForCausalLM.from_pretrained(\"model_name\")\nmodel.quantize(\n    calib_data=\"wikitext\",\n    w_bit=4,\n    q_group_size=128,\n    version=\"GEMM\"  # Inference kernel\n)\n</code></pre>"},{"location":"quantization/awq/#11-common-interview-questions","title":"11. Common Interview Questions","text":"<p>Q1: How does AWQ differ from GPTQ philosophically?  A: GPTQ compensates errors across all weights. AWQ protects important weights from error in the first place. Prevention vs. compensation.</p> <p>Q2: Why is AWQ faster to quantize?  A: No iterative weight updates or Hessian computation. Just statistics collection + grid search for scales. Embarrassingly parallel.</p> <p>Q3: What's the \"1% salient weights\" finding?  A: 1% of weight channels (those with highest activation magnitude) contribute disproportionately. Protecting them preserves 90%+ of model quality.</p> <p>Q4: How are scales applied at inference?  A: Mathematically equivalent to scale weights (expensive) or scale activations (cheap). AWQ scales activations: <code>(W/s) @ X = (W) @ (X/s)</code>.</p> <p>Q5: Can AWQ work for INT8?  A: Yes, but less beneficial. INT8 already preserves most weights well. AWQ's advantage is strongest at 3-4 bits where bit budget is tight.</p> <p>Q6: What's the memory overhead of scales?  A: Per-channel FP16 scales: 0.1-0.2% overhead. Negligible compared to 8\u00d7 weight reduction.</p> <p>Q7: AWQ vs. SmoothQuant?  A: SmoothQuant smooths activations for easier quantization. AWQ protects important weights. Can be combined: SmoothQuant for activation quantization, AWQ for weights.</p> <p>Q8: Why grid search for scales?  A: Closed-form solution doesn't exist for optimal scales. Grid search over reasonable range [0.5, 1.5] is fast and effective. Can use gradient-based for better results but slower.</p>"},{"location":"quantization/gguf_ggml/","title":"GGUF GGML","text":""},{"location":"quantization/gguf_ggml/#1-ggml-georgi-gerganov-machine-learning","title":"1. GGML (Georgi Gerganov Machine Learning)","text":"<p>C library for machine learning inference, optimized for CPU execution of LLMs.</p> <p>Key Features:  - Pure C/C++ (no Python runtime) - CPU-optimized kernels (AVX2, AVX512, NEON) - 4-bit to 16-bit quantization - Memory-mapped model loading - Apple Metal, CUDA, OpenCL backends</p>"},{"location":"quantization/gguf_ggml/#gguf-ggml-universal-format","title":"GGUF (GGML Universal Format)","text":"<p>Successor to GGML format (deprecated). Single-file model container.</p>"},{"location":"quantization/gguf_ggml/#file-structure","title":"File Structure","text":"<pre><code>[Header]\n- Magic number: \"GGUF\"\n- Version\n- Tensor count, KV metadata count\n\n[Metadata]\n- Model hyperparameters\n- Tokenizer config\n- Quantization scheme\n- Author, license, etc.\n\n[Tensor Info]\n- Name, dimensions, type, offset\n\n[Tensor Data]\n- Actual quantized weights (memory-mapped)\n</code></pre>"},{"location":"quantization/gguf_ggml/#advantages","title":"Advantages","text":"<ul> <li>Single file: All model data + config in one .gguf</li> <li>Memory mapping: Load multi-GB models instantly, use minimal RAM</li> <li>Extensible: KV metadata for any additional info</li> <li>Backward compatible: Old GGML loaders fail safely</li> </ul>"},{"location":"quantization/gguf_ggml/#quantization-types","title":"Quantization Types","text":""},{"location":"quantization/gguf_ggml/#k-quantization-k-quants","title":"K-Quantization (K-quants)","text":"<p>Optimized 2-6 bit quantization schemes:</p> Type Bits Description Use Case Q2_K ~2.5 Extreme compression Large models on limited RAM Q3_K_S ~3.4 Small, less accurate Acceptable quality loss Q3_K_M ~3.7 Medium quality Balanced Q4_K_S ~4.0 Small, good quality Recommended default Q4_K_M ~4.5 Medium, best quality Best 4-bit option Q5_K_S ~5.0 Small, very good Low loss Q6_K ~6.0 High quality Near-FP16 quality"},{"location":"quantization/gguf_ggml/#legacy-quantization","title":"Legacy Quantization","text":"<ul> <li>Q4_0: Original 4-bit (group size 32)</li> <li>Q4_1: 4-bit with per-group min (better than Q4_0)</li> <li>Q5_0, Q5_1: 5-bit variants</li> <li>Q8_0: INT8 quantization</li> </ul>"},{"location":"quantization/gguf_ggml/#importance-matrix-i-quants","title":"Importance Matrix (I-quants)","text":"<p>Uses importance scores to allocate more bits to salient weights: - <code>IQ3_XXS</code>: 3-bit with importance weighting - <code>IQ4_XS</code>: 4-bit with importance weighting</p>"},{"location":"quantization/gguf_ggml/#llamacpp","title":"llama.cpp","text":"<p>Reference implementation for GGUF inference.</p> <pre><code># Convert HF model to GGUF\npython convert-hf-to-gguf.py model_path --outtype q4_K_M\n\n# Inference\n./main -m model.gguf -p \"Prompt\" \\\n  -n 512 \\              # Max tokens\n  -c 4096 \\             # Context length\n  -t 8 \\                # Threads\n  --mlock               # Lock in RAM (prevent swapping)\n</code></pre>"},{"location":"quantization/gguf_ggml/#cpu-optimizations","title":"CPU Optimizations","text":""},{"location":"quantization/gguf_ggml/#kernel-fusion","title":"Kernel Fusion","text":"<p><pre><code>attention = softmax(QK^T/\u221ad) @ V\n</code></pre> Fused into single kernel instead of 3 separate ops.</p>"},{"location":"quantization/gguf_ggml/#cache-friendly-layout","title":"Cache-Friendly Layout","text":"<p>Reorder tensors for sequential memory access. Dramatic speedup on CPU.</p>"},{"location":"quantization/gguf_ggml/#quantized-matrix-multiply","title":"Quantized Matrix Multiply","text":"<p>Custom AVX2/AVX512 kernels for INT4/INT8 GEMM. 4-8\u00d7 faster than naive C.</p>"},{"location":"quantization/gguf_ggml/#performance","title":"Performance","text":"<p>M2 Max (Metal):  - 7B Q4_K_M: ~40 tokens/sec - 13B Q4_K_M: ~25 tokens/sec</p> <p>AMD 5950X (16-core):  - 7B Q4_K_M: ~30 tokens/sec - 13B Q4_K_M: ~15 tokens/sec</p>"},{"location":"quantization/gguf_ggml/#common-interview-questions","title":"Common Interview Questions","text":"<p>Q1: Why GGUF instead of safetensors or PyTorch?  A: GGUF designed for inference, not training. Memory-mapped loading, quantization metadata embedded, optimized for llama.cpp ecosystem.</p> <p>Q2: What's memory mapping and why does it matter?  A: OS maps file directly to virtual memory. Model loads \"instantly\" because data stays on disk until accessed. Enables running larger models than RAM.</p> <p>Q3: Why CPU inference for LLMs?  A: Consumer hardware accessibility. Most users don't have high-end GPUs. CPUs have large memory, enabling bigger models with quantization.</p> <p>Q4: What's the quality difference between Q4_K_M and Q8_0?  A: Q8_0: ~99% of FP16 quality. Q4_K_M: ~95-97%. For chat, Q4_K_M usually indistinguishable. For precise tasks, Q8_0 safer.</p> <p>Q5: How do K-quants improve on original Q4_0?  A: Better block structure, per-block scales and mins, optimized for specific bit rates. Q4_K_M beats Q4_0 quality while being similar size.</p> <p>Q6: Can GGUF models be used outside llama.cpp?  A: Yes. Libraries: llama-cpp-python (Python), whisper.cpp (audio), GPT4All, Ollama, LM Studio all support GGUF.</p> <p>Q7: What's the tradeoff between Q4_K_S and Q4_K_M?  A: Q4_K_S: Smaller (~4.0 bpw), faster. Q4_K_M: Slightly larger (~4.5 bpw), better quality. Difference: ~0.3 PPL for 7B models.</p> <p>Q8: Why multiple quantization types instead of just one?  A: Different hardware, use cases, quality requirements. Q2_K for extreme memory constraints, Q6_K for quality-critical applications, Q4_K_M for general use.</p> <p>Q9: What's \"bpw\" (bits per weight)?  A: Effective bits including quantization metadata overhead. Q4_K_M is labeled \"4-bit\" but actually ~4.5 bpw due to scales/mins storage.</p>"},{"location":"quantization/gptq/","title":"GPTQ","text":""},{"location":"quantization/gptq/#1-paper","title":"1. Paper","text":"<p>Frantar et al., 2022 - \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\"</p>"},{"location":"quantization/gptq/#2-core-idea","title":"2. Core Idea","text":"<p>Quantize weights one-by-one while compensating errors in remaining weights using second-order information (Hessian).</p>"},{"location":"quantization/gptq/#3-algorithm-simplified","title":"3. Algorithm (Simplified)","text":"<pre><code># For each layer's weight matrix W\nH = 2 * X^T @ X / n_samples  # Hessian (input correlations)\n\nfor i in range(n_columns):\n    # Quantize column i\n    w_q = quantize(W[:, i])\n    error = W[:, i] - w_q\n\n    # Distribute error to remaining columns using Hessian\n    W[:, i+1:] -= error * H[i, i+1:] / H[i, i]\n\n    W[:, i] = w_q\n</code></pre>"},{"location":"quantization/gptq/#4-key-innovation-optimal-brain-quantization-obq","title":"4. Key Innovation: Optimal Brain Quantization (OBQ)","text":"<ul> <li>Uses second-order Taylor expansion to minimize quantization loss</li> <li>Iteratively quantizes weights in order that minimizes Hessian-weighted error</li> <li>Compensates each quantization error before next step</li> </ul>"},{"location":"quantization/gptq/#lazy-batch-updates-efficiency","title":"Lazy Batch Updates (Efficiency)","text":"<ul> <li>Don't update all weights individually</li> <li>Process in blocks of 128 columns</li> <li>Dramatic speedup with minimal quality loss</li> </ul>"},{"location":"quantization/gptq/#specifications","title":"Specifications","text":"<ul> <li>Calibration: 128 samples typical</li> <li>Time: 3-4 hours for 175B model on single GPU</li> <li>Group Size: 128 (default), lower for better quality</li> <li>Bits: Designed for 3-4 bit, works for INT8 too</li> </ul>"},{"location":"quantization/gptq/#accuracy","title":"Accuracy","text":"Model Bits Group Size Perplexity \u0394 LLaMA-7B 4 128 +0.2 LLaMA-13B 3 128 +0.9 OPT-175B 4 128 +0.1"},{"location":"quantization/gptq/#vs-round-to-nearest-rtn","title":"vs. Round-To-Nearest (RTN)","text":"<ul> <li>RTN: Fast, 5-10% degradation at 4-bit</li> <li>GPTQ: Slow quantization, &lt;2% degradation at 4-bit</li> </ul>"},{"location":"quantization/gptq/#implementation","title":"Implementation","text":"<pre><code># AutoGPTQ library\nfrom auto_gptq import AutoGPTQForCausalLM\n\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    \"model_name\",\n    quantize_config={\n        \"bits\": 4,\n        \"group_size\": 128,\n        \"desc_act\": False  # Activation ordering\n    }\n)\n</code></pre>"},{"location":"quantization/gptq/#common-interview-questions","title":"Common Interview Questions","text":"<p>Q1: Why does GPTQ outperform naive quantization?  A: It compensates each quantization error by adjusting remaining weights based on input correlations (Hessian), preventing error accumulation.</p> <p>Q2: What's the computational complexity?  A: O(n\u00b2) in weight dimensions due to Hessian computation and updates. Lazy batching reduces this to O(n\u00b2/b) where b = block size.</p> <p>Q3: Why use Hessian instead of just gradient?  A: Second-order information captures weight interactions. First-order (gradient) only gives local slope, not curvature of loss landscape.</p> <p>Q4: What's \"desc_act\" in GPTQ?  A: Reorders activation channels by importance before quantization. Helps but adds complexity; often disabled for speed.</p> <p>Q5: Can GPTQ quantize activations?  A: No, GPTQ is weight-only. Activations typically stay FP16 or use runtime INT8 quantization.</p> <p>Q6: GPTQ vs. AWQ - when to use which?  A: GPTQ: Better for extreme compression (3-bit). AWQ: Faster quantization, better at 4-bit, protects important weights rather than compensating errors.</p> <p>Q7: Why is calibration data needed?  A: To compute Hessian (H = X^T X), which captures input statistics. Need representative samples to estimate weight importance accurately.</p>"},{"location":"quantization/int4_quantization/","title":"INT4 Quantization","text":""},{"location":"quantization/int4_quantization/#1-overview","title":"1. Overview","text":"<p>4-bit quantization (16 discrete values) achieves 8\u00d7 memory reduction from FP32. Requires careful techniques to maintain quality.</p>"},{"location":"quantization/int4_quantization/#2-key-challenge","title":"2. Key Challenge","text":"<p>Limited range (16 values) makes naive quantization lossy. Need sophisticated methods: GPTQ, AWQ, or group quantization.</p>"},{"location":"quantization/int4_quantization/#3-group-quantization","title":"3. Group Quantization","text":"<p>Concept: Different scales for weight groups instead of entire layer</p> <pre><code># Group size typically 32-128\nfor group in split_weights(W, group_size=128):\n    scale = max(abs(group)) / 7  # 4-bit signed: -8 to 7\n    group_int4 = round(group / scale).clip(-8, 7)\n</code></pre> <p>Tradeoff: Better accuracy vs. more scales to store (usually 1-2% overhead)</p>"},{"location":"quantization/int4_quantization/#normalfloat-nf4-qlora","title":"NormalFloat (NF4) - QLoRA","text":"<p>Innovation: Non-uniform quantization matching normal distribution</p> <pre><code>Standard INT4: [-8, -7, ..., 0, ..., 7]\nNF4: [-1.0, -0.6962, -0.5251, -0.3949, ...]\n</code></pre> <p>Why it works: Pre-trained weights follow ~N(0, \u03c3), NF4 bins optimally quantize normal distribution</p> <p>Usage: QLoRA for parameter-efficient fine-tuning</p>"},{"location":"quantization/int4_quantization/#double-quantization","title":"Double Quantization","text":"<p>Quantize the quantization scales themselves (QLoRA technique)</p> <ul> <li>FP16 scales \u2192 INT8 scales</li> <li>Saves additional 0.4 bits per parameter</li> <li>Minimal accuracy impact</li> </ul>"},{"location":"quantization/int4_quantization/#inference-kernels","title":"Inference Kernels","text":"<p>Challenge: No native INT4 arithmetic on most hardware</p> <p>Solution: Pack two INT4 values per byte, unpack during compute <pre><code>byte = (val1 &lt;&lt; 4) | val2  # Pack\nval1 = (byte &gt;&gt; 4) &amp; 0xF   # Unpack\n</code></pre></p>"},{"location":"quantization/int4_quantization/#performance","title":"Performance","text":"<ul> <li>Memory: 8\u00d7 reduction (2GB for 7B model)</li> <li>Speed: 1.5-2\u00d7 faster than INT8 (memory-bound scenarios)</li> <li>Accuracy drop: 3-7% with naive methods, &lt;2% with GPTQ/AWQ</li> </ul>"},{"location":"quantization/int4_quantization/#common-interview-questions","title":"Common Interview Questions","text":"<p>Q1: Why not INT4 everywhere if it's 8\u00d7 smaller?  A: Quality degradation becomes significant. Activations especially need higher precision. Still typically use FP16/INT8 for activations.</p> <p>Q2: What's the typical group size for INT4?  A: 32-128. Smaller = better accuracy but more overhead. 128 is common sweet spot.</p> <p>Q3: How does NF4 differ from uniform INT4?  A: NF4 uses quantiles of normal distribution as bins instead of uniform spacing. Since weights are normally distributed, this minimizes quantization error.</p> <p>Q4: Can you do INT4 quantization without GPTQ/AWQ?  A: Yes, but expect 5-10% accuracy drop. Round-to-nearest with group quantization gets you ~3-5% drop. GPTQ/AWQ optimize to &lt;2%.</p> <p>Q5: What's the memory breakdown for INT4 model?  A: Weights: 4 bits, Scales: ~0.1 bits (with double quantization), KV cache: still FP16/INT8 (separate issue).</p> <p>Q6: Why is INT4 harder than INT8 for activations?  A: Activations have wider dynamic range and outliers. INT4's 16 values can't capture this without severe clipping or poor resolution.</p>"},{"location":"quantization/int8_quantization/","title":"INT8 Quantization","text":""},{"location":"quantization/int8_quantization/#1-overview","title":"1. Overview","text":"<p>Maps FP16/FP32 values to 8-bit integers (256 discrete values). Standard for production LLM deployment.</p>"},{"location":"quantization/int8_quantization/#2-quantization-process","title":"2. Quantization Process","text":""},{"location":"quantization/int8_quantization/#weight-quantization","title":"Weight Quantization","text":"<pre><code># Per-channel quantization\nscale = max(abs(W)) / 127\nW_int8 = round(W / scale).clip(-128, 127)\n</code></pre>"},{"location":"quantization/int8_quantization/#activation-quantization","title":"Activation Quantization","text":"<pre><code># Calibration phase (100-1000 samples)\nmin_val, max_val = collect_statistics(calibration_data)\nscale = (max_val - min_val) / 255\nzero_point = -round(min_val / scale)\n</code></pre>"},{"location":"quantization/int8_quantization/#3-llmint8-dettmers-et-al-2022","title":"3. LLM.int8() (Dettmers et al., 2022)","text":"<p>Key Innovation: Mixed-precision decomposition for outliers</p> <p>Process:</p> <ol> <li>Detect outlier features (&gt;6\u03c3 threshold)</li> <li>Separate matrix multiplication: FP16 for outliers, INT8 for rest</li> <li>Typically, &lt;0.1% outlier features, but they're critical</li> </ol> <p>Memory: 2\u00d7 reduction with minimal accuracy loss</p>"},{"location":"quantization/int8_quantization/#smoothquant-bridge","title":"SmoothQuant Bridge","text":"<p>Often combined with SmoothQuant for activation smoothing before INT8 conversion.</p>"},{"location":"quantization/int8_quantization/#hardware-support","title":"Hardware Support","text":"<ul> <li>NVIDIA Tensor Cores: INT8 GEMM operations</li> <li>Intel VNNI: Vector Neural Network Instructions</li> <li>ARM: INT8 GEMM on modern CPUs</li> </ul> <p>Speedup: 2-4\u00d7 on modern hardware</p>"},{"location":"quantization/int8_quantization/#common-interview-questions","title":"Common Interview Questions","text":"<p>Q1: Why is INT8 considered the \"sweet spot\"? A: Best balance of compression (4\u00d7 from FP32), hardware support, and accuracy preservation. INT4 needs more careful handling.</p> <p>Q2: What's the bottleneck in INT8 inference? A: Dequantization overhead and memory bandwidth. For small batches, compute isn't fully saturated.</p> <p>Q3: How does LLM.int8() handle outliers? A: Uses vector-wise quantization to detect outliers (values &gt;6\u03c3), processes them in FP16 while using INT8 for the remaining 99.9% of values.</p> <p>Q4: Can you quantize all layers to INT8? A: No. Embedding layers, layer norms, and sometimes first/last layers stay in FP16 for stability.</p> <p>Q5: What's absmax quantization? A: Symmetric quantization using absolute maximum: <code>scale = max(|W|) / 127</code>. Simple but can waste range if distribution is skewed.</p> <p>Q6: Calibration dataset size? A: 100-1000 samples from training distribution. More doesn't always help; diversity matters more than quantity.</p>"},{"location":"quantization/quantization_basics/","title":"Quantization Basics","text":""},{"location":"quantization/quantization_basics/#1-core-concept","title":"1. Core Concept","text":"<p>Quantization reduces model precision from FP32/FP16 to lower bit representations (INT8, INT4) to decrease memory and increase inference speed.</p> <p>Key Formula: <code>Q(x) = round(x/S) - Z</code> where S = scale, Z = zero-point</p>"},{"location":"quantization/quantization_basics/#2-types","title":"2. Types","text":""},{"location":"quantization/quantization_basics/#post-training-quantization-ptq","title":"Post-Training Quantization (PTQ)","text":"<ul> <li>Applied after training</li> <li>No retraining needed</li> <li>Calibration dataset required</li> <li>Common methods: MinMax, Percentile, MSE</li> </ul>"},{"location":"quantization/quantization_basics/#quantization-aware-training-qat","title":"Quantization-Aware Training (QAT)","text":"<ul> <li>Simulates quantization during training</li> <li>Better accuracy but requires full training</li> <li>Fake quantization nodes in forward pass</li> </ul>"},{"location":"quantization/quantization_basics/#3-quantization-schemes","title":"3. Quantization Schemes","text":"<p>Symmetric: Zero-point = 0, range = [-127, 127] for INT8  Asymmetric: Zero-point \u2260 0, range = [0, 255] for UINT8</p> <p>Per-Tensor: Single scale for entire tensor  Per-Channel: Different scale per output channel (better accuracy)</p>"},{"location":"quantization/quantization_basics/#4-memory-savings","title":"4. Memory Savings","text":"<ul> <li>FP32 \u2192 INT8: 4\u00d7 reduction</li> <li>FP32 \u2192 INT4: 8\u00d7 reduction</li> <li>Attention and FFN layers: Primary targets</li> </ul>"},{"location":"quantization/quantization_basics/#5-common-interview-questions","title":"5. Common Interview Questions","text":"<p>Q1: Why does quantization work for LLMs? A: LLMs have activation/weight distributions that cluster around certain values. Most information is captured in relative magnitudes rather than absolute precision.</p> <p>Q2: What's the difference between static and dynamic quantization? A: Static uses calibration data to determine scales offline. Dynamic computes scales at runtime (slower but more accurate for varied inputs).</p> <p>Q3: Which layers are hardest to quantize? A: Layer normalization and first/last layers are most sensitive. Activations often need higher precision than weights.</p> <p>Q4: How do you measure quantization quality? A: Perplexity on validation set, task-specific metrics (accuracy, F1), and activation distribution analysis (KL divergence).</p> <p>Q5: What's the typical accuracy drop for INT8 quantization? A: Well-executed INT8 PTQ: &lt;1% degradation. INT4: 2-5% depending on model size and method.</p>"},{"location":"quantization/quantization_tradeoffs/","title":"Quantization Tradeoffs","text":""},{"location":"quantization/quantization_tradeoffs/#1-memory-vs-quality-spectrum","title":"1. Memory vs. Quality Spectrum","text":"Precision Memory (7B) Typical PPL \u0394 Use Case FP16 14 GB 0.0 (baseline) Training, high-quality inference INT8 7 GB +0.1-0.5 Production standard INT4 (GPTQ/AWQ) 3.5 GB +0.5-1.5 Commodity GPU inference 3-bit 2.6 GB +1.5-3.0 Extreme compression Q2_K 2 GB +3.0-5.0 Last resort"},{"location":"quantization/quantization_tradeoffs/#speed-vs-quality","title":"Speed vs. Quality","text":""},{"location":"quantization/quantization_tradeoffs/#inference-latency-7b-model-batch1","title":"Inference Latency (7B model, batch=1)","text":"Method GPU (A100) CPU (32-core) FP16 20 ms/token N/A (OOM) INT8 10 ms/token N/A (OOM) INT4 (AWQ) 7 ms/token 80 ms/token GGUF Q4_K_M 8 ms/token 35 ms/token <p>Key insight: CPU competitive for quantized models, especially with optimized kernels.</p>"},{"location":"quantization/quantization_tradeoffs/#quantization-method-selection","title":"Quantization Method Selection","text":""},{"location":"quantization/quantization_tradeoffs/#decision-tree","title":"Decision Tree","text":"<p>Need extreme compression (2-3 bit)? \u2192 GPTQ (best quality at extreme compression)</p> <p>Standard 4-bit, fast quantization needed? \u2192 AWQ (10 min vs 4 hours for GPTQ, similar quality)</p> <p>CPU deployment? \u2192 GGUF with llama.cpp (optimized CPU kernels)</p> <p>GPU deployment, production quality? \u2192 INT8 with SmoothQuant (robust, well-supported)</p> <p>Fine-tuning on limited memory? \u2192 QLoRA with NF4 (efficient training)</p>"},{"location":"quantization/quantization_tradeoffs/#layer-wise-quantization-strategy","title":"Layer-wise Quantization Strategy","text":""},{"location":"quantization/quantization_tradeoffs/#typical-configuration","title":"Typical Configuration","text":"<pre><code>Embeddings: FP16 (critical for semantic space)\nAttention Weights (Q, K, V): INT4/INT8\nAttention Output: INT8\nFFN Weights: INT4 (largest, most compressible)\nFFN Activations: INT8\nLayer Norm: FP16 (small, sensitive)\nFinal Layer: FP16 or INT8\n</code></pre>"},{"location":"quantization/quantization_tradeoffs/#rationale","title":"Rationale","text":"<ul> <li>FFN: 66% of parameters, less sensitive \u2192 aggressive INT4</li> <li>Attention: 33% of parameters, more sensitive \u2192 INT8 or careful INT4</li> <li>Norms/Embeddings: &lt;1% of parameters \u2192 keep FP16</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#mixed-precision-strategies","title":"Mixed Precision Strategies","text":""},{"location":"quantization/quantization_tradeoffs/#w4a8-weight-4-bit-activation-8-bit","title":"W4A8 (Weight 4-bit, Activation 8-bit)","text":"<ul> <li>Best of both worlds for many use cases</li> <li>Weights: AWQ/GPTQ 4-bit</li> <li>Activations: SmoothQuant INT8</li> <li>6-8\u00d7 memory reduction, &lt;1% quality loss</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#w8a8-both-8-bit","title":"W8A8 (Both 8-bit)","text":"<ul> <li>Production standard for quality-critical apps</li> <li>4\u00d7 memory reduction</li> <li>Hardware-accelerated on all modern platforms</li> <li>&lt;0.5% quality loss with SmoothQuant</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#hardware-considerations","title":"Hardware Considerations","text":""},{"location":"quantization/quantization_tradeoffs/#nvidia-gpus","title":"NVIDIA GPUs","text":"<ul> <li>Tensor Cores: INT8 (Turing+), INT4 (Hopper)</li> <li>Recommendation: INT8 for A100, INT4 for H100</li> <li>Custom kernels: AWQ's TinyChat, ExLlamaV2 for GPTQ</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#amd-gpus","title":"AMD GPUs","text":"<ul> <li>ROCm: INT8 support</li> <li>Recommendation: INT8, limited INT4 optimization</li> <li>Ecosystem: Less mature than NVIDIA</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#apple-silicon","title":"Apple Silicon","text":"<ul> <li>Metal: INT8, INT4 via llama.cpp</li> <li>Recommendation: GGUF Q4_K_M or Q6_K</li> <li>Strength: Unified memory architecture</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#cpu-x86","title":"CPU (x86)","text":"<ul> <li>VNNI (Cascade Lake+): INT8 acceleration</li> <li>AVX512: INT8/INT4 kernels</li> <li>Recommendation: GGUF with llama.cpp, Q4_K_M sweet spot</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#calibration-data-tradeoffs","title":"Calibration Data Tradeoffs","text":""},{"location":"quantization/quantization_tradeoffs/#size","title":"Size","text":"<ul> <li>100 samples: Usually sufficient, fast</li> <li>1000 samples: Marginal quality improvement</li> <li>10000 samples: No additional benefit, waste of time</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#diversity-vs-representativeness","title":"Diversity vs. Representativeness","text":"<ul> <li>In-domain: Better for specialized models</li> <li>General (WikiText): Better for general models</li> <li>Mixed: Best for production</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#dynamic-vs-static-quantization","title":"Dynamic vs. Static Quantization","text":""},{"location":"quantization/quantization_tradeoffs/#static-ptq","title":"Static (PTQ)","text":"<p>Pros: Faster inference, lower memory Cons: Fixed scales, may underfit outliers Best for: Stable input distributions</p>"},{"location":"quantization/quantization_tradeoffs/#dynamic","title":"Dynamic","text":"<p>Pros: Adapts to inputs, better quality Cons: Runtime overhead (scale computation) Best for: Varied input distributions, activation quantization</p>"},{"location":"quantization/quantization_tradeoffs/#common-interview-questions","title":"Common Interview Questions","text":"<p>Q1: When would you use INT8 over INT4? A: (1) Quality-critical applications where 0.5% matters, (2) Hardware with INT8 acceleration but no INT4, (3) Activations (INT4 activations too lossy).</p> <p>Q2: What's the minimum model size for effective quantization? A: ~1B parameters. Smaller models have less redundancy, quantization hurts more. &lt;500M models: stick to FP16.</p> <p>Q3: How do you decide between GPTQ and AWQ? A: GPTQ for 3-bit or when quality is paramount. AWQ for 4-bit, faster iteration, production deadlines. Quality difference minimal at 4-bit.</p> <p>Q4: What's the biggest failure mode of quantization? A: Outlier channels not handled properly. SmoothQuant, AWQ, or mixed-precision decomposition (LLM.int8()) essential for robust quantization.</p> <p>Q5: Can you quantize a fine-tuned model? A: Yes, but better to fine-tune with QLoRA (quantize-aware fine-tuning). Post-hoc quantization of fine-tuned models can be more sensitive than base models.</p> <p>Q6: What's the practical lower bound for useful quantization? A: 2-bit with current methods. Below that, quality degrades unacceptably even for large models (70B+). Active research on sub-2-bit.</p> <p>Q7: How much quality loss is acceptable? A: Domain-dependent. Chatbots: 2-3% acceptable. Code generation: &lt;1%. Reasoning tasks: &lt;0.5%. Benchmark on your specific use case.</p> <p>Q8: Should you quantize KV cache? A: Yes for long context (4K+). INT8 KV cache with SmoothQuant: 2\u00d7 memory savings, &lt;0.5% quality loss. Critical for 32K+ context.</p> <p>Q9: What's the ROI of quantization engineering time? A: High. 4 hours GPTQ quantization \u2192 8\u00d7 memory reduction \u2192 8\u00d7 more users per GPU \u2192 8\u00d7 cost reduction. One-time cost, continuous savings.</p> <p>Q10: Biggest misconception about quantization? A: \"Lower bits always means faster.\" Reality: memory-bound scenarios see speedup, compute-bound scenarios don't. Batch size, context length matter more than bit width for speed.</p>"},{"location":"quantization/smoothquant/","title":"SmoothQuant","text":""},{"location":"quantization/smoothquant/#1-paper","title":"1. Paper","text":"<p>Xiao et al., 2022 - \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\"</p>"},{"location":"quantization/smoothquant/#2-problem-statement","title":"2. Problem Statement","text":"<p>Activation outliers make INT8 quantization difficult. Weights quantize well, activations don't.</p>"},{"location":"quantization/smoothquant/#observation","title":"Observation","text":"<ul> <li>Weight range: typically within [-3\u03c3, 3\u03c3]</li> <li>Activation range: 10-100\u00d7 larger due to systematic outliers in specific channels</li> </ul>"},{"location":"quantization/smoothquant/#3-core-idea-smoothing","title":"3. Core Idea: Smoothing","text":"<p>Migrate difficulty from activations to weights via mathematically equivalent transformation.</p>"},{"location":"quantization/smoothquant/#key-transformation","title":"Key Transformation","text":"<pre><code>Y = XW = (X / s) \u00b7 (W \u00b7 s)\n</code></pre> <p>Where <code>s</code> is per-channel smoothing factor.</p> <ul> <li>Divide activations by s \u2192 reduces outliers</li> <li>Multiply weights by s \u2192 increases weight range (but weights easier to quantize)</li> </ul>"},{"location":"quantization/smoothquant/#4-algorithm","title":"4. Algorithm","text":""},{"location":"quantization/smoothquant/#1-identify-outlier-channels","title":"1. Identify Outlier Channels","text":"<pre><code># Collect calibration statistics\nalpha_x = max(|X|, dim=tokens)  # Per-channel activation range\nalpha_w = max(|W|, dim=input_dim)  # Per-channel weight range\n</code></pre>"},{"location":"quantization/smoothquant/#2-compute-smoothing-scales","title":"2. Compute Smoothing Scales","text":"<pre><code># Migration strength \u03b1 \u2208 [0, 1]\n# \u03b1=0: no smoothing, \u03b1=1: full migration\ns = alpha_x^\u03b1 / alpha_w^(1-\u03b1)\n</code></pre>"},{"location":"quantization/smoothquant/#3-apply-smoothing","title":"3. Apply Smoothing","text":"<pre><code># Offline transformation\nW_smooth = W * s  # Fold into weights\n# At runtime: X_smooth = X / s\n</code></pre>"},{"location":"quantization/smoothquant/#5-migration-strength","title":"5. Migration Strength \u03b1","text":"<p>\u03b1 = 0.5: Balanced migration (default)  - Geometric mean of activation and weight ranges - Empirically optimal for most models</p> <p>\u03b1 = 0.75: More aggressive activation smoothing  - Better for models with severe outliers (OPT)</p>"},{"location":"quantization/smoothquant/#6-per-token-vs-per-tensor","title":"6. Per-Token vs. Per-Tensor","text":"<p>Per-Tensor Dynamic: Single scale per activation tensor (fast, less accurate) Per-Token Dynamic: Scale per token in sequence (better accuracy, slower)</p> <p>SmoothQuant enables per-tensor quantization by smoothing outliers beforehand.</p>"},{"location":"quantization/smoothquant/#7-performance","title":"7. Performance","text":"Model W8A8 Accuracy vs FP16 OPT-175B 66.7% -0.1% BLOOM-176B 68.4% -0.3% LLaMA-65B 69.2% -0.2% <p>Speedup: 1.5-2\u00d7 on A100 (INT8 Tensor Cores)</p>"},{"location":"quantization/smoothquant/#8-integration-with-other-methods","title":"8. Integration with Other Methods","text":"<p>SmoothQuant + AWQ:  - SmoothQuant for activation INT8 - AWQ for weight INT4 - Hybrid W4A8 quantization</p> <p>SmoothQuant + LLM.int8():  - SmoothQuant pre-processing - LLM.int8() for outlier handling - Complementary techniques</p>"},{"location":"quantization/smoothquant/#9-implementation","title":"9. Implementation","text":"<pre><code>from smoothquant.smooth import smooth_lm\n\n# Apply smoothing transformation\nmodel = smooth_lm(\n    model,\n    calibration_data,\n    alpha=0.5  # Migration strength\n)\n\n# Then quantize with standard tools\nquantized = quantize_model(model, w_bit=8, a_bit=8)\n</code></pre>"},{"location":"quantization/smoothquant/#10-common-interview-questions","title":"10. Common Interview Questions","text":"<p>Q1: Why do LLM activations have outliers?  A: Systematic outliers in specific channels across all tokens, likely due to attention patterns and positional encodings. Some channels accumulate large values.</p> <p>Q2: How does smoothing preserve mathematical equivalence?  A: Matrix multiplication property: <code>(X/s) @ (W*s) = X @ W</code>. Division and multiplication by same per-channel scales cancel out.</p> <p>Q3: Why can't we just clip outliers?  A: Clipping loses information and degrades quality significantly (5-10%). Smoothing redistributes dynamic range without information loss.</p> <p>Q4: What's the overhead of smoothing at runtime?  A: Negligible. Smoothing scales folded into weights offline. Only <code>X/s</code> at runtime (cheap element-wise division before matmul).</p> <p>Q5: Does SmoothQuant help with KV cache quantization?  A: Yes! KV cache contains activations. Smoothing reduces outliers, enabling INT8 KV cache with minimal quality loss.</p> <p>Q6: Why is \u03b1=0.5 optimal?  A: Balances difficulty migration. Too low \u2192 activations still hard. Too high \u2192 weights become hard. Geometric mean (0.5) is empirical sweet spot.</p> <p>Q7: Can SmoothQuant be applied per-layer?  A: Yes, \u03b1 can be tuned per-layer. Some layers benefit from more aggressive smoothing (\u03b1=0.7), others from less (\u03b1=0.3).</p> <p>Q8: SmoothQuant vs. absmax/percentile scaling?  A: Those methods scale after observing outliers. SmoothQuant prevents outliers from forming. More fundamental solution.</p>"},{"location":"serving_frameworks/deepspeed_inference/","title":"DeepSpeed Inference","text":""},{"location":"serving_frameworks/deepspeed_inference/#1-overview","title":"1. Overview","text":"<p>Microsoft's inference optimization library  - Part of the larger DeepSpeed training ecosystem - Focus: Multi-GPU inference, kernel optimizations, quantization - Integrated with DeepSpeed-MII (Model Implementations for Inference)</p>"},{"location":"serving_frameworks/deepspeed_inference/#2-core-innovations","title":"2. Core Innovations","text":""},{"location":"serving_frameworks/deepspeed_inference/#deepspeed-mii","title":"DeepSpeed-MII","text":"<p>High-level serving framework built on DeepSpeed-Inference - REST API server - Dynamic batching - Multi-GPU tensor parallelism - Lower-level alternative to vLLM/TGI</p>"},{"location":"serving_frameworks/deepspeed_inference/#zero-inference","title":"ZeRO-Inference","text":"<ul> <li>Adapts ZeRO training optimizations for inference</li> <li>Offloading strategies for large models</li> <li>CPU/NVMe offloading when GPU memory insufficient</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#3-kernel-optimizations","title":"3. Kernel Optimizations","text":""},{"location":"serving_frameworks/deepspeed_inference/#custom-cuda-kernels","title":"Custom CUDA Kernels","text":"<ul> <li>Optimized Transformer layers</li> <li>Attention mechanisms (pre-FlashAttention era)</li> <li>Fused operations (LayerNorm+Residual, etc.)</li> </ul> <p>Note: Some kernels now superseded by FlashAttention and newer libraries</p>"},{"location":"serving_frameworks/deepspeed_inference/#inference-specialized-ops","title":"Inference-Specialized Ops","text":"<ul> <li>KV cache management (simpler than vLLM's paging)</li> <li>Optimized softmax for long sequences</li> <li>Custom GEMM operations</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#4-quantization-support","title":"4. Quantization Support","text":""},{"location":"serving_frameworks/deepspeed_inference/#int8-quantization","title":"INT8 Quantization","text":"<ul> <li>Symmetric/asymmetric quantization</li> <li>Per-channel or per-tensor</li> <li>ZeroQuant for activation quantization</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#mixed-precision","title":"Mixed Precision","text":"<ul> <li>FP16/BF16 computation</li> <li>INT8 weights with FP16 activations</li> <li>Automatic mixed precision selection</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#5-model-parallelism","title":"5. Model Parallelism","text":""},{"location":"serving_frameworks/deepspeed_inference/#tensor-parallelism","title":"Tensor Parallelism","text":"<ul> <li>Column/row parallelism for linear layers</li> <li>Optimized communication patterns</li> <li>Supports pipeline parallelism combination</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<ul> <li>Micro-batching for throughput</li> <li>1F1B (one-forward-one-backward) scheduling adapted for inference</li> <li>Good for extremely large models (&gt;100B parameters)</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#6-deepspeed-fastgen-2024","title":"6. DeepSpeed-FastGen (2024+)","text":"<p>Latest addition: Dynamic SplitFuse scheduling   - Combines prefill and decode in single batch - Similar to vLLM's chunked prefill concept - Claimed improvements over naive continuous batching</p>"},{"location":"serving_frameworks/deepspeed_inference/#splitfuse-algorithm","title":"SplitFuse Algorithm","text":"<ol> <li>Split long prefills into chunks</li> <li>Fuse with decode operations</li> <li>Balance compute resources dynamically</li> </ol> <p>Benefit: Reduces tail latency for long prompts</p>"},{"location":"serving_frameworks/deepspeed_inference/#7-inference-engine-initialization","title":"7. Inference Engine Initialization","text":"<p>Simplified API: <pre><code>import deepspeed\nengine = deepspeed.init_inference(\n    model,\n    tensor_parallel={\"tp_size\": 4},\n    dtype=torch.float16,\n    replace_with_kernel_inject=True\n)\n</code></pre></p> <p>replace_with_kernel_inject: Swaps model ops with DeepSpeed optimized kernels</p>"},{"location":"serving_frameworks/deepspeed_inference/#8-performance-characteristics","title":"8. Performance Characteristics","text":"<p>Strengths:  - Good for research/prototyping - Integrated training-to-inference workflow - Strong multi-GPU support</p> <p>Limitations:  - Less production-hardened than TGI/vLLM - Smaller community/ecosystem - Kernel optimizations lag behind latest research</p>"},{"location":"serving_frameworks/deepspeed_inference/#9-interview-qa","title":"9. Interview Q&amp;A","text":"<p>Q: When to use DeepSpeed-Inference vs vLLM?  A: DeepSpeed-Inference for research environments with existing DeepSpeed training pipelines. vLLM for production serving with better memory efficiency and throughput.</p> <p>Q: What is ZeRO-Inference's offloading strategy?  A: Hierarchical offloading: GPU \u2192 CPU RAM \u2192 NVMe SSD. Brings parameters into GPU on-demand. Enables inference of models larger than GPU memory but with latency penalty.</p> <p>Q: How does DeepSpeed-FastGen compare to vLLM's continuous batching? A: Both use iteration-level scheduling. FastGen adds SplitFuse for better prefill/decode balance. vLLM has more mature PagedAttention for memory efficiency. Performance similar in practice.</p> <p>Q: Why isn't DeepSpeed-Inference as popular as vLLM for serving?  A: Later entry to production serving space, less focus on ease-of-use, smaller ecosystem. Primarily adopted by users already in DeepSpeed training ecosystem.</p> <p>Q: What's the role of kernel injection?  A: Automatically replaces PyTorch operations with optimized DeepSpeed kernels at runtime. Transparent acceleration without model code changes. Trade-off: may have compatibility issues with custom model architectures.</p>"},{"location":"serving_frameworks/framework_comparison/","title":"Framework Comparison","text":""},{"location":"serving_frameworks/framework_comparison/#1-quick-selection-guide","title":"1. Quick Selection Guide","text":"Use Case Recommended Framework Rationale Maximum throughput, multi-tenancy vLLM PagedAttention, multi-LoRA, continuous batching Peak NVIDIA GPU performance TensorRT-LLM Hardware-specific optimization, FP8 support Production stability, HF ecosystem TGI Rust reliability, grammar constraints, fast deploys Research + training integration DeepSpeed-Inference Unified training/inference, ZeRO-Inference Multi-model pipelines, enterprise Triton Framework-agnostic, model versioning, ensembles"},{"location":"serving_frameworks/framework_comparison/#2-feature-comparison-matrix","title":"2. Feature Comparison Matrix","text":"Feature vLLM TensorRT-LLM TGI DeepSpeed Triton Memory Efficiency \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 (via vLLM) Ease of Setup \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 Peak Throughput \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 (via backends) Multi-LoRA \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2606\u2606\u2606 \u2606\u2606\u2606\u2606\u2606 \u2606\u2606\u2606\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 (via vLLM) Model Support \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 Production Maturity \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605"},{"location":"serving_frameworks/framework_comparison/#3-technical-deep-dive","title":"3. Technical Deep Dive","text":""},{"location":"serving_frameworks/framework_comparison/#memory-management-approaches","title":"Memory Management Approaches","text":"<p>vLLM (PagedAttention):  - Paged KV cache with block tables - &lt;4% memory waste - Best for variable-length sequences</p> <p>TensorRT-LLM:  - Paged KV cache inspired by vLLM - NVIDIA-optimized CUDA kernels - Tightly coupled with GPU architecture</p> <p>TGI:  - FlashAttention for memory efficiency - No paging, simpler approach - Good for single-tenant scenarios</p> <p>DeepSpeed:  - Basic KV cache management - ZeRO-Inference for CPU/NVMe offloading - Suited for extreme model sizes</p>"},{"location":"serving_frameworks/framework_comparison/#batching-strategies","title":"Batching Strategies","text":"<p>Continuous Batching (vLLM, TGI, DeepSpeed-FastGen):  - Iteration-level scheduling - Immediate slot filling - 20-30% throughput improvement</p> <p>Static Batching (Traditional):  - Wait for full batch completion - Simpler implementation - GPU idle time</p> <p>Dynamic Batching (Triton):  - Time-window accumulation - Less sophisticated than continuous - Still effective for many workloads</p>"},{"location":"serving_frameworks/framework_comparison/#quantization-comparison","title":"Quantization Comparison","text":"Framework INT8 INT4 FP8 Methods vLLM \u2713 \u2713 \u2713 AWQ, GPTQ, SmoothQuant TensorRT-LLM \u2713 \u2713 \u2713 Native + AWQ, GPTQ TGI \u2713 \u2713 \u2713 bitsandbytes, AWQ, GPTQ, EETQ DeepSpeed \u2713 \u2713 \u2717 ZeroQuant Triton Depends on backend <p>FP8 Note: Only on NVIDIA Hopper (H100+), 2x throughput vs FP16</p>"},{"location":"serving_frameworks/framework_comparison/#4-latency-characteristics","title":"4. Latency Characteristics","text":""},{"location":"serving_frameworks/framework_comparison/#first-token-time-to-time-ttft","title":"First Token Time to Time (TTFT)","text":"<p>Best to Worst:  1. TGI (Rust + safetensors, optimized cold start) 2. vLLM (Python overhead but chunked prefill) 3. TensorRT-LLM (engine loading overhead) 4. DeepSpeed-Inference 5. Triton (abstraction layer overhead)</p>"},{"location":"serving_frameworks/framework_comparison/#inter-token-latency-itl","title":"Inter-Token Latency (ITL)","text":"<p>Best to Worst:  1. TensorRT-LLM (maximum kernel optimization) 2. vLLM (PagedAttention efficiency) 3. TGI (FlashAttention + Rust) 4. Triton (depends on backend) 5. DeepSpeed-Inference</p>"},{"location":"serving_frameworks/framework_comparison/#throughput-tokenssecond","title":"Throughput (tokens/second)","text":"<p>Best to Worst:  1. vLLM (PagedAttention + continuous batching) 2. TensorRT-LLM (hardware optimization) 3. TGI (solid continuous batching) 4. Triton + vLLM backend 5. DeepSpeed-Inference</p>"},{"location":"serving_frameworks/framework_comparison/#5-multi-gpu-considerations","title":"5. Multi-GPU Considerations","text":""},{"location":"serving_frameworks/framework_comparison/#tensor-parallelism-performance","title":"Tensor Parallelism Performance","text":"<p>TensorRT-LLM:  - Custom NCCL optimizations - Lowest latency for TP</p> <p>vLLM:  - Ray-based distribution - Good performance, more overhead</p> <p>TGI:  - Rust-based TP implementation - Efficient but less optimized than TensorRT</p>"},{"location":"serving_frameworks/framework_comparison/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<ul> <li>Best support: DeepSpeed-Inference, TensorRT-LLM</li> <li>Limited: vLLM (experimental)</li> <li>Not primary focus: TGI</li> </ul>"},{"location":"serving_frameworks/framework_comparison/#6-production-deployment-factors","title":"6. Production Deployment Factors","text":""},{"location":"serving_frameworks/framework_comparison/#containerization","title":"Containerization","text":"<p>Easiest: TGI, vLLM (official Docker images, simple configs) Medium: Triton (more complex configs) Complex: TensorRT-LLM (build dependencies), DeepSpeed</p>"},{"location":"serving_frameworks/framework_comparison/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>Most Comprehensive: Triton &gt; TGI &gt; vLLM &gt; DeepSpeed Key Metrics: Queue depth, batch size, KV cache utilization, token throughput</p>"},{"location":"serving_frameworks/framework_comparison/#scaling-patterns","title":"Scaling Patterns","text":"<p>Horizontal (Multiple Instances): All support, TGI/Triton easiest Vertical (Bigger GPUs): TensorRT-LLM extracts most value Multi-Model: Triton's core strength</p>"},{"location":"serving_frameworks/framework_comparison/#7-interview-qa","title":"7. Interview Q&amp;A","text":"<p>Q: vLLM vs TensorRT-LLM for production?  A: vLLM for faster iteration, multi-LoRA, easier ops. TensorRT-LLM when you need absolute maximum throughput and have dedicated ML Eng team for maintenance.</p> <p>Q: Why doesn't everyone use TensorRT-LLM if it's fastest?  A: Setup complexity, need to rebuild engines for changes, GPU-specific builds, harder debugging. Speed gain (10-20%) often not worth operational overhead.</p> <p>Q: When is DeepSpeed-Inference the right choice?  A: When you're already using DeepSpeed for training and want unified tooling. Or when you need ZeRO-Inference for models larger than GPU memory. Not for general production serving.</p> <p>Q: Can you mix frameworks?  A: Yes via Triton backends. Run vLLM for LLM, TensorRT for embeddings, Python backend for custom logic. Single server, unified API.</p> <p>Q: How to choose between vLLM and TGI?  A: Similar performance. Choose TGI for HuggingFace integration, grammar constraints, Rust reliability. Choose vLLM for multi-LoRA, latest features, slightly higher throughput.</p> <p>Q: What's the main bottleneck each framework optimizes?  A: vLLM \u2192 memory fragmentation. TensorRT-LLM \u2192 compute efficiency. TGI \u2192 deployment stability. DeepSpeed \u2192 model size limits. Triton \u2192 pipeline complexity.</p> <p>Q: Impact of continuous batching on latency?  A: Slightly increases average latency per request (5-10%) but dramatically increases throughput (20-30%). Worth it for high-traffic scenarios, not for latency-critical single-user apps.</p>"},{"location":"serving_frameworks/tensorrt_llm/","title":"TensorRT LLM","text":""},{"location":"serving_frameworks/tensorrt_llm/#1-core-architecture","title":"1. Core Architecture","text":"<p>NVIDIA's optimization stack for LLM inference on their GPUs  - Built on TensorRT for kernel-level optimization - Focuses on extracting maximum performance from NVIDIA hardware - Trade-off: Complex setup vs peak performance</p>"},{"location":"serving_frameworks/tensorrt_llm/#2-key-technologies","title":"2. Key Technologies","text":""},{"location":"serving_frameworks/tensorrt_llm/#1-in-flight-batching-continuous-batching","title":"1. In-flight Batching (Continuous Batching)","text":"<ul> <li>Similar to vLLM's approach</li> <li>Dynamically adds/removes requests during execution</li> <li>Optimized specifically for NVIDIA GPU scheduling</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#2-paged-kv-cache","title":"2. Paged KV Cache","text":"<ul> <li>Inspired by vLLM's PagedAttention</li> <li>NVIDIA-optimized memory management</li> <li>Custom CUDA kernels for memory operations</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#3-kernel-fusion","title":"3. Kernel Fusion","text":"<ul> <li>Combines multiple operations into single kernels</li> <li>Reduces memory transfers between GPU operations</li> <li>Examples: LayerNorm+Residual, QKV projection fusion</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#4-flashattention-fp8-support","title":"4. FlashAttention &amp; FP8 Support","text":"<ul> <li>Integrated FlashAttention-2 for memory-efficient attention</li> <li>Native FP8 quantization on Hopper GPUs (H100)</li> <li>2x throughput vs FP16 with minimal accuracy loss</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#3-quantization-support","title":"3. Quantization Support","text":"<p>Weight-Only Quantization:  - INT8/INT4 weights, FP16 activations - 2-4x memory reduction - GPTQ, AWQ methods supported</p> <p>Activation Quantization:  - FP8 (Hopper GPUs only) - SmoothQuant for INT8 activations</p>"},{"location":"serving_frameworks/tensorrt_llm/#4-model-parallelism","title":"4. Model Parallelism","text":""},{"location":"serving_frameworks/tensorrt_llm/#tensor-parallelism","title":"Tensor Parallelism","text":"<ul> <li>Splits model layers across GPUs</li> <li>Low-latency (intra-node communication)</li> <li>Best for latency-sensitive serving</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<ul> <li>Splits model vertically into stages</li> <li>Higher throughput for large batches</li> <li>Micro-batching to reduce bubbles</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#combined-tppp","title":"Combined TP+PP","text":"<ul> <li>Multi-dimensional parallelism</li> <li>Example: 8-way TP \u00d7 4-way PP for 32 GPUs</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#5-engine-building-process","title":"5. Engine Building Process","text":"<p>Two-Step Workflow:  1. Build: Model \u2192 Optimized TensorRT engine (slow, one-time) 2. Runtime: Load engine \u2192 Inference (fast)</p> <p>Key considerations:  - Engines are GPU-specific (H100 engine \u2260 A100 engine) - Rebuild required for different batch sizes or sequence lengths - Trade flexibility for maximum performance</p>"},{"location":"serving_frameworks/tensorrt_llm/#6-multi-gpu-inference-modes","title":"6. Multi-GPU Inference Modes","text":"<p>KV Cache Transfer Optimization: - Custom NCCL/NVLink operations for KV cache - Overlaps communication with computation - Critical for tensor parallel setups</p>"},{"location":"serving_frameworks/tensorrt_llm/#7-interview-qa","title":"7. Interview Q&amp;A","text":"<p>Q: When to choose TensorRT-LLM over vLLM?  A: When you need absolute maximum throughput on NVIDIA GPUs and can handle complex setup. vLLM for ease of use and flexibility; TensorRT-LLM for peak performance.</p> <p>Q: Why is engine building necessary?  A: TensorRT optimizes compute graphs at compile time (kernel selection, fusion, memory layout). This specialization achieves maximum performance but loses runtime flexibility.</p> <p>Q: How does TensorRT-LLM handle dynamic shapes?  A: Uses optimization profiles with min/max ranges during build. Runtime performance varies by how well actual inputs match the profile. Too wide a range reduces optimization effectiveness.</p> <p>Q: What's the FP8 accuracy impact?  A: On Hopper GPUs with proper calibration, &lt;1% accuracy degradation for most models. Requires per-tensor scaling and careful quantization of outlier features.</p> <p>Q: Why does TensorRT-LLM require specific CUDA versions?  A: Tightly integrated with CUDA toolkit for custom kernel launches, memory management, and GPU-specific optimizations. Newer releases exploit latest CUDA features (e.g., Hopper's Tensor Memory Accelerator).</p>"},{"location":"serving_frameworks/text_generation_inference/","title":"Text Generation Inference","text":""},{"location":"serving_frameworks/text_generation_inference/#1-overview","title":"1. Overview","text":"<p>Hugging Face's production serving solution  - Written in Rust for performance and safety - Python bindings for model loading - Focus: Stability, HuggingFace ecosystem integration, ease of deployment</p>"},{"location":"serving_frameworks/text_generation_inference/#2-core-architecture","title":"2. Core Architecture","text":""},{"location":"serving_frameworks/text_generation_inference/#token-streaming","title":"Token Streaming","text":"<ul> <li>Server-Sent Events (SSE) for real-time streaming</li> <li>Low-latency first-token time</li> <li>Optimized for chat applications</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#continuous-batching","title":"Continuous Batching","text":"<ul> <li>Dynamic batching like vLLM</li> <li>Request prioritization support</li> <li>Smart scheduling for mixed workloads</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#flashattention-integration","title":"FlashAttention Integration","text":"<ul> <li>Uses FlashAttention for memory-efficient attention</li> <li>Custom kernels for specific model architectures</li> <li>Optimized for both prefill and decode</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#3-quantization-features","title":"3. Quantization Features","text":"<p>Built-in Quantization:  - bitsandbytes (INT8, NF4) - GPTQ (INT4, INT8) - AWQ (INT4) - EETQ (INT8, FP8-like)</p> <p>No separate build step - quantization at runtime</p>"},{"location":"serving_frameworks/text_generation_inference/#4-model-support","title":"4. Model Support","text":"<p>Broad Architecture Coverage:  - All major HuggingFace models out-of-box - Automatic architecture detection - Custom model support via transformers library</p> <p>Specializations:  - Mistral/Mixtral with custom kernels - Llama (1, 2, 3) optimizations - Falcon, Starcoder optimizations</p>"},{"location":"serving_frameworks/text_generation_inference/#5-distributed-serving","title":"5. Distributed Serving","text":""},{"location":"serving_frameworks/text_generation_inference/#tensor-parallelism","title":"Tensor Parallelism","text":"<ul> <li>Multi-GPU inference with automatic sharding</li> <li>Based on custom Rust implementation</li> <li>Lower overhead than Python-based solutions</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#safetensors-format","title":"Safetensors Format","text":"<ul> <li>Lazy loading with mmap</li> <li>Fast cold starts</li> <li>Memory-efficient weight loading</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#6-production-features","title":"6. Production Features","text":""},{"location":"serving_frameworks/text_generation_inference/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>Prometheus metrics endpoint</li> <li>Request/token-level tracing</li> <li>Queue depth, batch size, latency metrics</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#safety-features","title":"Safety Features","text":"<ul> <li>Request validation and sanitization</li> <li>Token limit enforcement</li> <li>Grammar/JSON schema validation</li> <li>Repetition penalty controls</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#docker-kubernetes","title":"Docker &amp; Kubernetes","text":"<ul> <li>Official Docker images</li> <li>Helm charts for K8s deployment</li> <li>Auto-scaling support with metrics</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#7-grammar-constrained-generation","title":"7. Grammar-Constrained Generation","text":"<p>Unique Feature vs Competitors:  - Force model to follow regex patterns - JSON schema validation during generation - Prevents malformed outputs</p> <p>Example: Generate only valid JSON with specific schema</p>"},{"location":"serving_frameworks/text_generation_inference/#8-performance-characteristics","title":"8. Performance Characteristics","text":"<p>Strengths:  - Fast cold start (Rust + safetensors) - Stable long-running deployments - Lower memory overhead than Python frameworks</p> <p>Trade-offs:  - Slightly lower peak throughput vs TensorRT-LLM - Less aggressive optimizations vs vLLM's latest features</p>"},{"location":"serving_frameworks/text_generation_inference/#9-interview-qa","title":"9. Interview Q&amp;A","text":"<p>Q: Why choose TGI over vLLM?  A: TGI for production stability, HuggingFace integration, and grammar constraints. vLLM for maximum throughput and cutting-edge features like multi-LoRA.</p> <p>Q: How does TGI handle model updates?  A: Hot-swapping not supported. Deploy new instances and gradually shift traffic. Safetensors format enables fast restarts (&lt;30s for most models).</p> <p>Q: What's TGI's approach to KV cache management?  A: Uses FlashAttention's memory-efficient approach rather than paging. Simpler but less flexible than vLLM's PagedAttention for extreme multi-tenancy.</p> <p>Q: How does grammar-constrained generation work?  A: Token sampling filtered by regex/grammar rules. If next token violates constraint, it's masked and next-best token chosen. Slight performance overhead but guarantees format compliance.</p> <p>Q: Why Rust for inference serving?  A: Memory safety without garbage collection pauses, zero-cost abstractions, excellent async performance. Critical for long-running production services with 99.9% uptime requirements.</p> <p>Q: How does TGI handle request timeouts?  A: Cancellation tokens propagate through async runtime. Partial generation discarded immediately, freeing batch slot for new requests. No \"zombie\" requests blocking GPU.</p>"},{"location":"serving_frameworks/triton_inference_server/","title":"Triton Inference Server","text":""},{"location":"serving_frameworks/triton_inference_server/#1-overview","title":"1. Overview","text":"<p>NVIDIA's general-purpose inference server  - Framework-agnostic (PyTorch, TensorFlow, ONNX, TensorRT) - Not LLM-specific, but increasingly optimized for them - Focus: Enterprise deployment, multi-model serving, complex pipelines</p>"},{"location":"serving_frameworks/triton_inference_server/#2-core-architecture","title":"2. Core Architecture","text":""},{"location":"serving_frameworks/triton_inference_server/#backend-system","title":"Backend System","text":"<p>Pluggable backends for different frameworks: - Python backend (custom inference logic) - PyTorch backend (TorchScript models) - TensorRT backend (TensorRT engines) - vLLM backend (integration added 2024)</p> <p>Benefit: Mix different model types in same server</p>"},{"location":"serving_frameworks/triton_inference_server/#model-repository","title":"Model Repository","text":"<ul> <li>Centralized model storage (local/S3/GCS/Azure)</li> <li>Version management</li> <li>Hot-reloading of model versions</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#3-llm-specific-features-2024-2025","title":"3. LLM-Specific Features (2024-2025)","text":""},{"location":"serving_frameworks/triton_inference_server/#vllm-backend-integration","title":"vLLM Backend Integration","text":"<ul> <li>Uses vLLM engine under the hood</li> <li>Triton API layer on top</li> <li>Get vLLM's PagedAttention + Triton's enterprise features</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#tensorrt-llm-backend","title":"TensorRT-LLM Backend","text":"<ul> <li>Native integration with TensorRT-LLM engines</li> <li>Maximum performance for NVIDIA GPUs</li> <li>Requires pre-built TensorRT-LLM engines</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#4-advanced-serving-capabilities","title":"4. Advanced Serving Capabilities","text":""},{"location":"serving_frameworks/triton_inference_server/#model-ensembles","title":"Model Ensembles","text":"<p>Multi-stage pipelines as single endpoint: - Preprocessing \u2192 Embedding \u2192 LLM \u2192 Postprocessing - Automatic scheduling between stages - Example: RAG pipeline with retrieval + generation</p>"},{"location":"serving_frameworks/triton_inference_server/#dynamic-batching","title":"Dynamic Batching","text":"<ul> <li>Accumulates requests up to max batch size</li> <li>Timeout-based flushing</li> <li>More basic than vLLM/TGI continuous batching</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#sequence-batching","title":"Sequence Batching","text":"<ul> <li>For stateful models (e.g., streaming LLMs)</li> <li>Maintains state across multiple requests</li> <li>Useful for chat applications</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#5-model-configuration","title":"5. Model Configuration","text":"<p>Model config.pbtxt example: <pre><code>backend: \"vllm\"\nmax_batch_size: 32\n\ninstance_group [{ \n  count: 1\n  kind: KIND_GPU\n}]\n\nparameters: {\n  key: \"max_tokens\"\n  value: { string_value: \"2048\" }\n}\n</code></pre></p>"},{"location":"serving_frameworks/triton_inference_server/#6-scaling-deployment","title":"6. Scaling &amp; Deployment","text":""},{"location":"serving_frameworks/triton_inference_server/#kubernetes-native","title":"Kubernetes Native","text":"<ul> <li>Official Helm charts</li> <li>Horizontal Pod Autoscaler support</li> <li>Integration with Istio/Envoy for traffic management</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#multi-instance-serving","title":"Multi-Instance Serving","text":"<ul> <li>Multiple model instances per GPU</li> <li>Rate limiting and priority queues</li> <li>Request routing based on model version</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#7-metrics-observability","title":"7. Metrics &amp; Observability","text":"<p>Comprehensive Monitoring:  - Prometheus metrics (latency, throughput, queue depth) - Per-model and per-version metrics - GPU utilization tracking - Inference count, batch statistics</p> <p>Tracing:  - OpenTelemetry support - Request-level tracing through pipeline stages</p>"},{"location":"serving_frameworks/triton_inference_server/#8-performance-optimization","title":"8. Performance Optimization","text":""},{"location":"serving_frameworks/triton_inference_server/#concurrent-model-execution","title":"Concurrent Model Execution","text":"<ul> <li>Multiple models on same GPU (if memory allows)</li> <li>Scheduler balances execution</li> <li>Useful for A/B testing</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#instance-groups","title":"Instance Groups","text":"<ul> <li>Multiple instances of same model</li> <li>Load balancing across instances</li> <li>Can specify different GPUs per instance</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#9-interview-qa","title":"9. Interview Q&amp;A","text":"<p>Q: When to use Triton over vLLM/TGI?  A: When you need multi-framework support, complex model pipelines, or enterprise features (model versioning, ensembles). For pure LLM serving, vLLM/TGI are simpler.</p> <p>Q: How does Triton's vLLM backend differ from standalone vLLM?  A: Same core engine, but Triton adds: model versioning, ensemble pipelines, enterprise monitoring, multi-framework support. Trade-off: extra abstraction layer with slight overhead.</p> <p>Q: What's the benefit of model ensembles?  A: Single API call for multi-stage pipelines. Triton handles scheduling, batching, and data passing between stages. Reduces latency vs multiple network hops.</p> <p>Q: How does dynamic batching work in Triton?  A: Accumulates requests for up to max_batch_size or max_delay_ms. Simpler than continuous batching (no iteration-level scheduling). Better for CV/audio models than LLMs.</p> <p>Q: Why use Triton for LLMs when vLLM exists?  A: Multi-model serving (embeddings + LLM + reranker), existing NVIDIA infrastructure, need for A/B testing across model versions, enterprise governance requirements.</p> <p>Q: How does Triton handle model updates?  A: Model repository polling detects new versions. Can load new version without stopping server. Traffic routing supports gradual rollout (e.g., 90% v1, 10% v2).</p>"},{"location":"serving_frameworks/vllm/","title":"vLLM","text":""},{"location":"serving_frameworks/vllm/#1-core-innovation-pagedattention","title":"1. Core Innovation: PagedAttention","text":"<p>Problem: Traditional engines pre-allocate contiguous memory for max sequence length  - 60-80% GPU memory wasted on over-reservation - Internal fragmentation from unused allocated space</p> <p>Solution: Paged memory management for KV cache  - KV cache split into fixed-size blocks (pages) - Non-contiguous physical memory mapped via block tables - Reduces waste to &lt;4%, enables 2-3x larger batch sizes</p> <p>Memory Formula: <pre><code>KV Memory \u2248 2 \u00d7 L \u00d7 T \u00d7 H \u00d7 D_h \u00d7 B\n</code></pre></p> <p>For Llama-3 8B (L=32, D=4096, FP16): ~0.5 MB per token  - 2k tokens \u2192 ~1 GB - 8k tokens \u2192 ~4 GB</p>"},{"location":"serving_frameworks/vllm/#2-continuous-batching","title":"2. Continuous Batching","text":"<p>vs Static Batching: Waits for entire batch to complete before accepting new requests</p> <p>vLLM Approach: Iteration-level scheduling  - New requests fill slots freed by completed sequences immediately - Eliminates GPU idle time (\"bubbles\") - Increases throughput by 20-30%</p>"},{"location":"serving_frameworks/vllm/#3-prefill-vs-decode-phases","title":"3. Prefill vs Decode Phases","text":"Phase Processing Bottleneck vLLM Optimization Prefill Parallel over tokens Compute-bound Chunked prefill Decode Sequential per token Memory-bandwidth PagedAttention <p>Chunked Prefill: Breaks large prompts into chunks to prevent blocking decode operations</p>"},{"location":"serving_frameworks/vllm/#4-modern-features-2025-2026","title":"4. Modern Features (2025-2026)","text":""},{"location":"serving_frameworks/vllm/#speculative-decoding","title":"Speculative Decoding","text":"<ul> <li>Small draft model generates k tokens</li> <li>Large target model verifies in single forward pass</li> <li>2-3x latency reduction for heavy models</li> </ul>"},{"location":"serving_frameworks/vllm/#automatic-prefix-caching-apc","title":"Automatic Prefix Caching (APC)","text":"<ul> <li>Shared KV blocks for common prefixes (system prompts, RAG contexts)</li> <li>Multiple requests reference same physical memory</li> <li>Critical for multi-turn chat and RAG applications</li> </ul>"},{"location":"serving_frameworks/vllm/#multi-lora-support","title":"Multi-LoRA Support","text":"<ul> <li>Serve base model + hundreds of LoRA adapters simultaneously</li> <li>SGMV kernels enable batched computation across different adapters</li> <li>Ideal for multi-tenant SaaS deployments</li> </ul>"},{"location":"serving_frameworks/vllm/#5-memory-pressure-handling","title":"5. Memory Pressure Handling","text":"<p>Preemption Strategies: 1. Swap: Move KV blocks to CPU memory (slower, preserves compute) 2. Recompute: Drop blocks and recalculate later (faster on modern GPUs)</p> <p>Strategy selection based on GPU compute vs memory bandwidth ratio.</p>"},{"location":"serving_frameworks/vllm/#6-interview-qa","title":"6. Interview Q&amp;A","text":"<p>Q: Why does PagedAttention improve throughput?  A: Eliminates memory fragmentation, allowing more concurrent requests to fit in GPU memory. With 60-80% waste reduced to &lt;4%, effective batch size increases 2-3x.</p> <p>Q: When is prefill the bottleneck vs decode?  A: Prefill dominates for short outputs with long prompts (summarization). Decode dominates for long generations (creative writing). vLLM uses chunked prefill to balance both.</p> <p>Q: How does vLLM handle variable-length sequences in a batch?  A: Continuous batching removes completed sequences and adds new ones at iteration boundaries. Block tables allow each sequence to use non-contiguous memory independently.</p> <p>Q: Why use recompute over swap for preemption?  A: On H100/A100 GPUs with high compute, recomputing KV cache is faster than PCIe transfer to CPU. Swap preferred for older GPUs or when CPU memory is abundant.</p> <p>Q: How does APC differ from traditional caching?  A: Traditional caching stores entire request results. APC caches KV blocks at sub-request granularity, enabling partial reuse across different requests with shared prefixes.</p>"}]}