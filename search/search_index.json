{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"decoding/decoding_strategies/","text":"\ud83d\udce6 Decoding Strategies 1. Overview Large Language Models output a probability distribution over the vocabulary at each decoding step. A decoding strategy defines how the next token is selected from this distribution. This page covers five commonly used decoding strategies: Greedy decoding Beam search Temperature sampling Top-k sampling Top-p sampling (nucleus sampling) 2. Decoding Strategies Explained with examples Toy probability distribution used in examples. Assume the model predicts the next token after: **\"The cat sat on the\"** Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 2.1. Greedy Decoding Idea Always select the token with the highest probability. Algorithm next_token = argmax(probabilities) Highest probability token is `mat`. Output: The cat sat on the mat Edge Case If probabilities are very close: A: 0.31, B: 0.30, C: 0.29 Greedy decoding always selects `A`, even when the model is uncertain. This often leads to repetitive or dull outputs. When to use Debugging Baselines Deterministic generation Python example import torch probs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]) next_token = torch.argmax(probs) print(next_token.item()) 2.2 Beam Search Idea Beam search keeps multiple candidate sequences at each decoding step instead of a single one. It selects the sequence with the highest overall probability , not just the best local choice. Algorithm Maintain B beams , where B is the beam width At each step, expand every beam with all possible next tokens Compute cumulative log probability for each expanded sequence Keep the top B sequences Repeat until an end condition is met Example Assume the next-token probabilities after: \"The cat sat on the\" Token Probability mat 0.40 floor 0.25 sofa 0.15 Beam width = 2 Step 1 Beam 1: \"mat\" score = log(0.40) Beam 2: \"floor\" score = log(0.25) Step 2 \"mat \u2192 quietly\" score = log(0.40) + log(0.30) \"floor \u2192 loudly\" score = log(0.25) + log(0.50) Even if \"quietly\" was locally better, \"floor \u2192 loudly\" may win due to higher cumulative probability. Final output is the sequence with the highest total score. Edge Case Beam search tends to favor safe, high-probability continuations , which can reduce diversity. This behavior becomes obvious in conversational or creative tasks. Assume the model is generating the next phrase after: \"I think that\" At a certain step, the model assigns probabilities like: Token Probability the 0.35 we 0.30 this 0.15 pizza 0.10 unicorn 0.10 With beam width = 3 : All beams will keep continuations starting with: \"the\" \"we\" \"this\" Tokens like \"pizza\" and \"unicorn\" are discarded early because their probabilities are lower. As decoding continues, beams converge to similar phrases: I think that the best way to... I think that we should... I think that this is... All beams are grammatically correct but nearly identical . If top-p sampling is used instead: Tokens like \"pizza\" or \"unicorn\" may occasionally be sampled Outputs become more diverse: I think that pizza could solve this I think that unicorn stories are fun When to use beam search Machine translation Speech recognition Structured text generation Tasks where correctness matters more than diversity When not to use beam search Chatbots Story generation Creative writing Conversational agents Python example (simplified) from heapq import nlargest import math def beam_search_step(beams, probs, beam_width): new_beams = [] for seq, score in beams: for i, p in enumerate(probs): new_seq = seq + [i] new_score = score + math.log(p) new_beams.append((new_seq, new_score)) return nlargest(beam_width, new_beams, key=lambda x: x[1]) # Initial beam beams = [([], 0.0)] probs = [0.40, 0.25, 0.15] beams = beam_search_step(beams, probs, beam_width=2) print(beams) 2.3 Temperature Sampling Idea Temperature controls how random the next-token selection is by scaling the model logits before applying softmax. It does not change which tokens are possible. It changes how strongly the model prefers high-probability tokens . Formula $$p_i = \\text{softmax}(\\text{logits}_i / T)$$ Where: T is the temperature lower T sharpens the distribution higher T flattens the distribution Effect of temperature Temperature Behavior T < 1 More deterministic T = 1 Original distribution T > 1 More random Example Assume the next-token probabilities are: Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 Low temperature (T = 0.3) Distribution becomes very sharp mat dominates even more Output: The cat sat on the mat This behaves almost like greedy decoding. High temperature (T = 1.5) Distribution becomes flatter Low-probability tokens become more likely Possible output: The cat sat on the moon Edge Case With very high temperature: Token Probability mat 0.18 floor 0.17 sofa 0.16 bed 0.15 roof 0.14 moon 0.10 pizza 0.10 The model loses strong preferences and may generate incoherent text: The cat sat on pizza quantum sky When temperature helps Creative writing Brainstorming Open-ended dialogue When temperature hurts Factual tasks Code generation Structured outputs Python example import torch logits = torch.log(torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])) temperature = 1.2 scaled_logits = logits / temperature probs = torch.softmax(scaled_logits, dim=0) next_token = torch.multinomial(probs, 1) print(next_token.item()) Note: Temperature controls randomness, not feasibility. It is usually combined with top-p or top-k sampling to avoid incoherent outputs. 2.4 Top-k Sampling Idea Top-k sampling restricts the model to sample only from the K most probable tokens at each decoding step. This prevents extremely unlikely tokens from being selected while still allowing randomness. Algorithm Sort all tokens by probability Keep only the top K tokens Renormalize their probabilities Sample one token Example Assume the next-token probabilities are: Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 Top-k with k = 3 Tokens kept: mat floor sofa Tokens removed: bed, roof, moon, pizza Possible output: The cat sat on the sofa Edge Case Flat probability distribution Assume: A: 0.11, B: 0.10, C: 0.10, D: 0.10, E: 0.10, F: 0.10, G: 0.10 With k = 3 : Only A, B, C are considered D, E, F, G are removed despite being equally likely This makes top-k sensitive to the choice of K and blind to the shape of the distribution. When top-k works well Moderate creativity with controlled randomness General text generation Chat systems with fixed diversity constraints When top-k works poorly Highly uncertain distributions Long-form creative writing Prompts with many equally valid continuations Python example import torch def top_k_sampling(probs, k): topk_probs, topk_idx = torch.topk(probs, k) topk_probs = topk_probs / topk_probs.sum() sampled = torch.multinomial(topk_probs, 1) return topk_idx[sampled] probs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]) token = top_k_sampling(probs, k=3) print(token.item()) Note: Top-k sampling fixes the number of candidate tokens regardless of model confidence. This makes it simpler than top-p but less adaptive in practice. 2.5 Top-p Sampling (Nucleus Sampling) Idea Top-p sampling selects the smallest possible set of tokens whose cumulative probability is at least p , then samples from that set. Unlike top-k, the number of candidate tokens changes dynamically based on model confidence. Algorithm Sort tokens by probability in descending order Add tokens until cumulative probability \u2265 p Renormalize probabilities within this set Sample one token Example Assume the next-token probabilities are: Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 Top-p with p = 0.9 Cumulative probability: mat \u2192 0.40 floor \u2192 0.65 sofa \u2192 0.80 bed \u2192 0.90 Tokens selected: mat floor sofa bed Possible output: The cat sat on the bed Edge Case (Key Difference from Top-k) Highly confident model Assume: A: 0.85, B: 0.07, C: 0.03, D: 0.03, E: 0.02 With p = 0.9 : Selected tokens: A, B Effective K = 2 With top-k (k = 5): Selected tokens: A, B, C, D, E Top-p automatically reduces randomness when the model is confident. Another Edge Case Uncertain model Assume: A: 0.20, B: 0.20, C: 0.20, D: 0.20, E: 0.20 With p = 0.9 : Selected tokens: A, B, C, D, E Effective K = 5 Top-p expands the candidate set when uncertainty is high. When top-p works well Conversational agents Long-form text generation Creative writing with coherence When top-p works poorly Strictly deterministic tasks Code generation with exact formatting requirements Python example import torch def top_p_sampling(probs, p): sorted_probs, sorted_idx = torch.sort(probs, descending=True) cumulative = torch.cumsum(sorted_probs, dim=0) cutoff_mask = cumulative <= p cutoff_mask[cutoff_mask.sum()] = True filtered_probs = sorted_probs[cutoff_mask] filtered_probs = filtered_probs / filtered_probs.sum() sampled = torch.multinomial(filtered_probs, 1) return sorted_idx[cutoff_mask][sampled] probs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]) token = top_p_sampling(probs, p=0.9) print(token.item()) Note : Top-p sampling adapts to the probability distribution shape, making it more robust than top-k for real-world language generation. 3. Pros and Cons of Decoding Strategies in Large Language Models 3.1 Greedy Decoding Pros Extremely fast and simple Fully deterministic and reproducible Easy to debug and analyze Works well when the model is very confident Cons No diversity at all Easily gets stuck in repetitive loops Early mistakes cannot be corrected Often produces dull or incomplete responses Poor performance for long or open-ended generation 3.2 Beam Search Pros Optimizes global sequence likelihood Reduces early local decision errors Produces fluent and grammatically correct text Effective for tasks with a single correct output Cons Computationally expensive Produces generic and safe outputs Very low diversity All beams often converge to similar sequences Performs poorly for dialogue and creative tasks 3.3 Temperature Sampling Pros Simple and intuitive control over randomness Enables creative and diverse outputs Easy to combine with other sampling methods Useful for brainstorming and storytelling Cons High temperature can cause incoherent text Low temperature collapses to greedy behavior Does not prevent sampling of very unlikely tokens Sensitive to temperature tuning 3.4 Top-k Sampling Pros Prevents extremely low-probability tokens Provides controlled randomness Simple to implement More diverse than greedy and beam search Cons Fixed K ignores distribution shape Sensitive to the choice of K Removes valid tokens in flat distributions Not adaptive to model confidence 3.5 Top-p Sampling (Nucleus Sampling) Pros Adapts automatically to model confidence Better diversity-quality tradeoff than top-k Stable across different prompts Widely used in modern chat models Cons Slightly more complex than top-k Still stochastic and non-deterministic Can include many tokens in very flat distributions Less suitable for strictly deterministic tasks 4. High-level Comparison Strategy Diversity Determinism Adaptivity Typical Usage Greedy Very low High No Baselines, debugging Beam Search Low Medium No Translation, ASR Temperature Medium to high Low Partial Creative text Top-k Medium Low No General generation Top-p Medium to high Low Yes Chat and dialogue","title":"Decoding strategies"},{"location":"decoding/decoding_strategies/#decoding-strategies","text":"","title":"\ud83d\udce6 Decoding Strategies"},{"location":"decoding/decoding_strategies/#1-overview","text":"Large Language Models output a probability distribution over the vocabulary at each decoding step. A decoding strategy defines how the next token is selected from this distribution. This page covers five commonly used decoding strategies: Greedy decoding Beam search Temperature sampling Top-k sampling Top-p sampling (nucleus sampling)","title":"1. Overview"},{"location":"decoding/decoding_strategies/#2-decoding-strategies-explained-with-examples","text":"Toy probability distribution used in examples. Assume the model predicts the next token after: **\"The cat sat on the\"** Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02","title":"2. Decoding Strategies Explained with examples"},{"location":"decoding/decoding_strategies/#21-greedy-decoding","text":"","title":"2.1. Greedy Decoding"},{"location":"decoding/decoding_strategies/#idea","text":"Always select the token with the highest probability.","title":"Idea"},{"location":"decoding/decoding_strategies/#algorithm","text":"next_token = argmax(probabilities) Highest probability token is `mat`. Output: The cat sat on the mat","title":"Algorithm"},{"location":"decoding/decoding_strategies/#edge-case","text":"If probabilities are very close: A: 0.31, B: 0.30, C: 0.29 Greedy decoding always selects `A`, even when the model is uncertain. This often leads to repetitive or dull outputs.","title":"Edge Case"},{"location":"decoding/decoding_strategies/#when-to-use","text":"Debugging Baselines Deterministic generation","title":"When to use"},{"location":"decoding/decoding_strategies/#python-example","text":"import torch probs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]) next_token = torch.argmax(probs) print(next_token.item())","title":"Python example"},{"location":"decoding/decoding_strategies/#22-beam-search","text":"","title":"2.2 Beam Search"},{"location":"decoding/decoding_strategies/#idea_1","text":"Beam search keeps multiple candidate sequences at each decoding step instead of a single one. It selects the sequence with the highest overall probability , not just the best local choice.","title":"Idea"},{"location":"decoding/decoding_strategies/#algorithm_1","text":"Maintain B beams , where B is the beam width At each step, expand every beam with all possible next tokens Compute cumulative log probability for each expanded sequence Keep the top B sequences Repeat until an end condition is met","title":"Algorithm"},{"location":"decoding/decoding_strategies/#example","text":"Assume the next-token probabilities after: \"The cat sat on the\" Token Probability mat 0.40 floor 0.25 sofa 0.15 Beam width = 2 Step 1 Beam 1: \"mat\" score = log(0.40) Beam 2: \"floor\" score = log(0.25) Step 2 \"mat \u2192 quietly\" score = log(0.40) + log(0.30) \"floor \u2192 loudly\" score = log(0.25) + log(0.50) Even if \"quietly\" was locally better, \"floor \u2192 loudly\" may win due to higher cumulative probability. Final output is the sequence with the highest total score.","title":"Example"},{"location":"decoding/decoding_strategies/#edge-case_1","text":"Beam search tends to favor safe, high-probability continuations , which can reduce diversity. This behavior becomes obvious in conversational or creative tasks. Assume the model is generating the next phrase after: \"I think that\" At a certain step, the model assigns probabilities like: Token Probability the 0.35 we 0.30 this 0.15 pizza 0.10 unicorn 0.10 With beam width = 3 : All beams will keep continuations starting with: \"the\" \"we\" \"this\" Tokens like \"pizza\" and \"unicorn\" are discarded early because their probabilities are lower. As decoding continues, beams converge to similar phrases: I think that the best way to... I think that we should... I think that this is... All beams are grammatically correct but nearly identical . If top-p sampling is used instead: Tokens like \"pizza\" or \"unicorn\" may occasionally be sampled Outputs become more diverse: I think that pizza could solve this I think that unicorn stories are fun","title":"Edge Case"},{"location":"decoding/decoding_strategies/#when-to-use-beam-search","text":"Machine translation Speech recognition Structured text generation Tasks where correctness matters more than diversity","title":"When to use beam search"},{"location":"decoding/decoding_strategies/#when-not-to-use-beam-search","text":"Chatbots Story generation Creative writing Conversational agents","title":"When not to use beam search"},{"location":"decoding/decoding_strategies/#python-example-simplified","text":"from heapq import nlargest import math def beam_search_step(beams, probs, beam_width): new_beams = [] for seq, score in beams: for i, p in enumerate(probs): new_seq = seq + [i] new_score = score + math.log(p) new_beams.append((new_seq, new_score)) return nlargest(beam_width, new_beams, key=lambda x: x[1]) # Initial beam beams = [([], 0.0)] probs = [0.40, 0.25, 0.15] beams = beam_search_step(beams, probs, beam_width=2) print(beams)","title":"Python example (simplified)"},{"location":"decoding/decoding_strategies/#23-temperature-sampling","text":"","title":"2.3 Temperature Sampling"},{"location":"decoding/decoding_strategies/#idea_2","text":"Temperature controls how random the next-token selection is by scaling the model logits before applying softmax. It does not change which tokens are possible. It changes how strongly the model prefers high-probability tokens .","title":"Idea"},{"location":"decoding/decoding_strategies/#formula","text":"$$p_i = \\text{softmax}(\\text{logits}_i / T)$$ Where: T is the temperature lower T sharpens the distribution higher T flattens the distribution","title":"Formula"},{"location":"decoding/decoding_strategies/#effect-of-temperature","text":"Temperature Behavior T < 1 More deterministic T = 1 Original distribution T > 1 More random","title":"Effect of temperature"},{"location":"decoding/decoding_strategies/#example_1","text":"Assume the next-token probabilities are: Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 Low temperature (T = 0.3) Distribution becomes very sharp mat dominates even more Output: The cat sat on the mat This behaves almost like greedy decoding. High temperature (T = 1.5) Distribution becomes flatter Low-probability tokens become more likely Possible output: The cat sat on the moon","title":"Example"},{"location":"decoding/decoding_strategies/#edge-case_2","text":"With very high temperature: Token Probability mat 0.18 floor 0.17 sofa 0.16 bed 0.15 roof 0.14 moon 0.10 pizza 0.10 The model loses strong preferences and may generate incoherent text: The cat sat on pizza quantum sky","title":"Edge Case"},{"location":"decoding/decoding_strategies/#when-temperature-helps","text":"Creative writing Brainstorming Open-ended dialogue","title":"When temperature helps"},{"location":"decoding/decoding_strategies/#when-temperature-hurts","text":"Factual tasks Code generation Structured outputs","title":"When temperature hurts"},{"location":"decoding/decoding_strategies/#python-example_1","text":"import torch logits = torch.log(torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])) temperature = 1.2 scaled_logits = logits / temperature probs = torch.softmax(scaled_logits, dim=0) next_token = torch.multinomial(probs, 1) print(next_token.item()) Note: Temperature controls randomness, not feasibility. It is usually combined with top-p or top-k sampling to avoid incoherent outputs.","title":"Python example"},{"location":"decoding/decoding_strategies/#24-top-k-sampling","text":"","title":"2.4 Top-k Sampling"},{"location":"decoding/decoding_strategies/#idea_3","text":"Top-k sampling restricts the model to sample only from the K most probable tokens at each decoding step. This prevents extremely unlikely tokens from being selected while still allowing randomness.","title":"Idea"},{"location":"decoding/decoding_strategies/#algorithm_2","text":"Sort all tokens by probability Keep only the top K tokens Renormalize their probabilities Sample one token","title":"Algorithm"},{"location":"decoding/decoding_strategies/#example_2","text":"Assume the next-token probabilities are: Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 Top-k with k = 3 Tokens kept: mat floor sofa Tokens removed: bed, roof, moon, pizza Possible output: The cat sat on the sofa","title":"Example"},{"location":"decoding/decoding_strategies/#edge-case_3","text":"Flat probability distribution Assume: A: 0.11, B: 0.10, C: 0.10, D: 0.10, E: 0.10, F: 0.10, G: 0.10 With k = 3 : Only A, B, C are considered D, E, F, G are removed despite being equally likely This makes top-k sensitive to the choice of K and blind to the shape of the distribution.","title":"Edge Case"},{"location":"decoding/decoding_strategies/#when-top-k-works-well","text":"Moderate creativity with controlled randomness General text generation Chat systems with fixed diversity constraints","title":"When top-k works well"},{"location":"decoding/decoding_strategies/#when-top-k-works-poorly","text":"Highly uncertain distributions Long-form creative writing Prompts with many equally valid continuations","title":"When top-k works poorly"},{"location":"decoding/decoding_strategies/#python-example_2","text":"import torch def top_k_sampling(probs, k): topk_probs, topk_idx = torch.topk(probs, k) topk_probs = topk_probs / topk_probs.sum() sampled = torch.multinomial(topk_probs, 1) return topk_idx[sampled] probs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]) token = top_k_sampling(probs, k=3) print(token.item()) Note: Top-k sampling fixes the number of candidate tokens regardless of model confidence. This makes it simpler than top-p but less adaptive in practice.","title":"Python example"},{"location":"decoding/decoding_strategies/#25-top-p-sampling-nucleus-sampling","text":"","title":"2.5 Top-p Sampling (Nucleus Sampling)"},{"location":"decoding/decoding_strategies/#idea_4","text":"Top-p sampling selects the smallest possible set of tokens whose cumulative probability is at least p , then samples from that set. Unlike top-k, the number of candidate tokens changes dynamically based on model confidence.","title":"Idea"},{"location":"decoding/decoding_strategies/#algorithm_3","text":"Sort tokens by probability in descending order Add tokens until cumulative probability \u2265 p Renormalize probabilities within this set Sample one token","title":"Algorithm"},{"location":"decoding/decoding_strategies/#example_3","text":"Assume the next-token probabilities are: Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 Top-p with p = 0.9 Cumulative probability: mat \u2192 0.40 floor \u2192 0.65 sofa \u2192 0.80 bed \u2192 0.90 Tokens selected: mat floor sofa bed Possible output: The cat sat on the bed","title":"Example"},{"location":"decoding/decoding_strategies/#edge-case-key-difference-from-top-k","text":"Highly confident model Assume: A: 0.85, B: 0.07, C: 0.03, D: 0.03, E: 0.02 With p = 0.9 : Selected tokens: A, B Effective K = 2 With top-k (k = 5): Selected tokens: A, B, C, D, E Top-p automatically reduces randomness when the model is confident.","title":"Edge Case (Key Difference from Top-k)"},{"location":"decoding/decoding_strategies/#another-edge-case","text":"Uncertain model Assume: A: 0.20, B: 0.20, C: 0.20, D: 0.20, E: 0.20 With p = 0.9 : Selected tokens: A, B, C, D, E Effective K = 5 Top-p expands the candidate set when uncertainty is high.","title":"Another Edge Case"},{"location":"decoding/decoding_strategies/#when-top-p-works-well","text":"Conversational agents Long-form text generation Creative writing with coherence","title":"When top-p works well"},{"location":"decoding/decoding_strategies/#when-top-p-works-poorly","text":"Strictly deterministic tasks Code generation with exact formatting requirements","title":"When top-p works poorly"},{"location":"decoding/decoding_strategies/#python-example_3","text":"import torch def top_p_sampling(probs, p): sorted_probs, sorted_idx = torch.sort(probs, descending=True) cumulative = torch.cumsum(sorted_probs, dim=0) cutoff_mask = cumulative <= p cutoff_mask[cutoff_mask.sum()] = True filtered_probs = sorted_probs[cutoff_mask] filtered_probs = filtered_probs / filtered_probs.sum() sampled = torch.multinomial(filtered_probs, 1) return sorted_idx[cutoff_mask][sampled] probs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]) token = top_p_sampling(probs, p=0.9) print(token.item()) Note : Top-p sampling adapts to the probability distribution shape, making it more robust than top-k for real-world language generation.","title":"Python example"},{"location":"decoding/decoding_strategies/#3-pros-and-cons-of-decoding-strategies-in-large-language-models","text":"","title":"3. Pros and Cons of Decoding Strategies in Large Language Models"},{"location":"decoding/decoding_strategies/#31-greedy-decoding","text":"","title":"3.1 Greedy Decoding"},{"location":"decoding/decoding_strategies/#pros","text":"Extremely fast and simple Fully deterministic and reproducible Easy to debug and analyze Works well when the model is very confident","title":"Pros"},{"location":"decoding/decoding_strategies/#cons","text":"No diversity at all Easily gets stuck in repetitive loops Early mistakes cannot be corrected Often produces dull or incomplete responses Poor performance for long or open-ended generation","title":"Cons"},{"location":"decoding/decoding_strategies/#32-beam-search","text":"","title":"3.2 Beam Search"},{"location":"decoding/decoding_strategies/#pros_1","text":"Optimizes global sequence likelihood Reduces early local decision errors Produces fluent and grammatically correct text Effective for tasks with a single correct output","title":"Pros"},{"location":"decoding/decoding_strategies/#cons_1","text":"Computationally expensive Produces generic and safe outputs Very low diversity All beams often converge to similar sequences Performs poorly for dialogue and creative tasks","title":"Cons"},{"location":"decoding/decoding_strategies/#33-temperature-sampling","text":"","title":"3.3 Temperature Sampling"},{"location":"decoding/decoding_strategies/#pros_2","text":"Simple and intuitive control over randomness Enables creative and diverse outputs Easy to combine with other sampling methods Useful for brainstorming and storytelling","title":"Pros"},{"location":"decoding/decoding_strategies/#cons_2","text":"High temperature can cause incoherent text Low temperature collapses to greedy behavior Does not prevent sampling of very unlikely tokens Sensitive to temperature tuning","title":"Cons"},{"location":"decoding/decoding_strategies/#34-top-k-sampling","text":"","title":"3.4 Top-k Sampling"},{"location":"decoding/decoding_strategies/#pros_3","text":"Prevents extremely low-probability tokens Provides controlled randomness Simple to implement More diverse than greedy and beam search","title":"Pros"},{"location":"decoding/decoding_strategies/#cons_3","text":"Fixed K ignores distribution shape Sensitive to the choice of K Removes valid tokens in flat distributions Not adaptive to model confidence","title":"Cons"},{"location":"decoding/decoding_strategies/#35-top-p-sampling-nucleus-sampling","text":"","title":"3.5 Top-p Sampling (Nucleus Sampling)"},{"location":"decoding/decoding_strategies/#pros_4","text":"Adapts automatically to model confidence Better diversity-quality tradeoff than top-k Stable across different prompts Widely used in modern chat models","title":"Pros"},{"location":"decoding/decoding_strategies/#cons_4","text":"Slightly more complex than top-k Still stochastic and non-deterministic Can include many tokens in very flat distributions Less suitable for strictly deterministic tasks","title":"Cons"},{"location":"decoding/decoding_strategies/#4-high-level-comparison","text":"Strategy Diversity Determinism Adaptivity Typical Usage Greedy Very low High No Baselines, debugging Beam Search Low Medium No Translation, ASR Temperature Medium to high Low Partial Creative text Top-k Medium Low No General generation Top-p Medium to high Low Yes Chat and dialogue","title":"4. High-level Comparison"},{"location":"memory/kv_caching/","text":"\ud83d\udce61. Self Attention Recap Given hidden states $X \\in \\mathbb{R}^{T \\times d}$: $$ Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V $$ Per head attention: $$ \\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V $$ Autoregressive decoding generates one token at a time with causal masking. \ud83d\udce62. Why KV Cache Is Needed At decoding step $t$, keys and values for tokens $1 \\ldots t-1$ are unchanged but would be recomputed without caching. This repeated computation dominates inference latency and wastes FLOPs. \ud83d\udce63. KV Cache Mechanism For each transformer layer $\\ell$: $$ \\text{KVCache} \\ell = {K \\ell^{1:t}, V_\\ell^{1:t}} $$ At decoding step $t$: Compute $Q_t, K_t, V_t$ Append $K_t, V_t$ to the cache Attend over all cached keys and values $$ \\text{Attn} t = \\text{softmax}\\left(\\frac{Q_t K {1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} $$ Only the current token requires new computation. \ud83d\udce64. Toy Example Prompt: \"I like neural\" Step 1: generate \"networks\" Compute and cache keys and values for the prompt Attend to all cached tokens Step 2: generate \"models\" Reuse cached keys and values Compute keys and values only for \"networks\" Previously generated tokens are never recomputed. \ud83d\udce65. Complexity Analysis Notation $T$: number of generated tokens $L$: number of transformer layers $H$: number of attention heads $d_h$: head dimension Without KV Cache At each decoding step, attention is recomputed for all previous tokens: $$ O(L \\cdot H \\cdot d_h \\cdot T^3) $$ With KV Cache Only attention against cached keys and values is computed: $$ O(L \\cdot H \\cdot d_h \\cdot T^2) $$ KV caching removes one full factor of $T$ from decoding complexity. \ud83d\udce66. Memory Cost Each layer stores: $$ K, V \\in \\mathbb{R}^{H \\times T \\times d_h} $$ Total KV cache memory across all layers: $$ O(L \\cdot H \\cdot T \\cdot d_h) $$ For long context inference, KV cache memory is often the dominant bottleneck. \ud83d\udce67. Inference v/s Training Usage 7.1 During Inference This is the most common and important usage. Inference Workflow Encode prompt Initialize empty KV cache per layer For each generated token: Compute $Q_t, K_t, V_t$ Append $K_t, V_t$ to cache Compute attention using cached tensors Practical Benefits Faster decoding Lower FLOPs Enables long context generation Essential for streaming and chat systems 7.2 During Training KV caching is not used in standard full sequence training. Why? Training processes full sequences in parallel All tokens attend to each other simultaneously No repeated computation across steps \ud83d\udce68. Scaling KV Cache for Long Context Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length. 8.1 Sliding Window Attention Only retain keys and values for the most recent $W$ tokens: $$ K_{t-W:t}, V_{t-W:t} $$ This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible. 8.2 KV Cache Quantization KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage. What Gets Quantized Both keys and values can be quantized, but they have different sensitivity: Keys (K) directly affect attention scores $QK^T$ Values (V) affect the weighted sum after softmax As a result: Keys usually require higher precision Values tolerate more aggressive quantization Common Quantization Schemes Component Typical Format Notes Keys FP16 / BF16 Preserves attention score stability Values INT8 Large memory reduction with minimal quality loss Both INT8 or INT4 Used for extreme long context scenarios Mixed precision KV cache is widely used in practice. Quantization Granularity KV cache quantization can be applied at different levels: Per tensor : One scale for entire K or V tensor Per head : Separate scale per attention head Per channel : Separate scale per head dimension Finer granularity improves accuracy but increases metadata and compute overhead. Dequantization During Attention At decoding step $t$: Load quantized $K, V$ from cache Dequantize to FP16 or BF16 Compute attention normally: $$ \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} $$ Dequantization cost is small compared to memory bandwidth savings. Impact on Performance Benefits: 2x to 4x KV memory reduction Higher batch size and longer context Improved inference throughput due to reduced memory traffic Tradeoffs: Slight loss in generation quality Additional dequantization overhead In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning. Interaction with Other Optimizations GQA further reduces KV cache size and works well with quantization Paged KV cache benefits from smaller KV blocks FlashAttention amortizes dequantization overhead inside fused kernels 8.3 Prefix Caching When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts. 8.4 Paged KV Cache KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging. \ud83d\udce69. Grouped Query Attention (GQA) Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads. 9.1 Head Configuration $$ H_q > H_k = H_v $$ Example: Query heads $H_q = 32$ Key value heads $H_k = 8$ This reduces KV cache memory by a factor of $H_q / H_k$. 9.2 QK Computation with Mismatched Heads Each key value head is shared by a fixed group of query heads. Let: $$ g = \\frac{H_q}{H_k} $$ Each key value head serves $g$ query heads. For query head $i$, the corresponding key value head index is: $$ \\left\\lfloor \\frac{i}{g} \\right\\rfloor $$ The attention computation becomes: $$ \\text{Attn} i = \\text{softmax}\\left(\\frac{Q_i K {\\left\\lfloor i/g \\right\\rfloor}^T}{\\sqrt{d_h}}\\right)V_{\\left\\lfloor i/g \\right\\rfloor} $$ Keys and values are reused directly without additional projection or averaging. 9.3 Why GQA Is Effective Query heads retain expressive power Keys and values capture shared context KV cache size and memory bandwidth are significantly reduced GQA is widely used in production LLMs. \ud83d\udce610. Other Common Optimizations FlashAttention FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together. Chunked Prefill Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill. Speculative Decoding Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.","title":"Kv caching"},{"location":"memory/kv_caching/#1-self-attention-recap","text":"Given hidden states $X \\in \\mathbb{R}^{T \\times d}$: $$ Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V $$ Per head attention: $$ \\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V $$ Autoregressive decoding generates one token at a time with causal masking.","title":"\ud83d\udce61. Self Attention Recap"},{"location":"memory/kv_caching/#2-why-kv-cache-is-needed","text":"At decoding step $t$, keys and values for tokens $1 \\ldots t-1$ are unchanged but would be recomputed without caching. This repeated computation dominates inference latency and wastes FLOPs.","title":"\ud83d\udce62. Why KV Cache Is Needed"},{"location":"memory/kv_caching/#3-kv-cache-mechanism","text":"For each transformer layer $\\ell$: $$ \\text{KVCache} \\ell = {K \\ell^{1:t}, V_\\ell^{1:t}} $$ At decoding step $t$: Compute $Q_t, K_t, V_t$ Append $K_t, V_t$ to the cache Attend over all cached keys and values $$ \\text{Attn} t = \\text{softmax}\\left(\\frac{Q_t K {1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} $$ Only the current token requires new computation.","title":"\ud83d\udce63. KV Cache Mechanism"},{"location":"memory/kv_caching/#4-toy-example","text":"Prompt: \"I like neural\" Step 1: generate \"networks\" Compute and cache keys and values for the prompt Attend to all cached tokens Step 2: generate \"models\" Reuse cached keys and values Compute keys and values only for \"networks\" Previously generated tokens are never recomputed.","title":"\ud83d\udce64. Toy Example"},{"location":"memory/kv_caching/#5-complexity-analysis","text":"","title":"\ud83d\udce65. Complexity Analysis"},{"location":"memory/kv_caching/#notation","text":"$T$: number of generated tokens $L$: number of transformer layers $H$: number of attention heads $d_h$: head dimension","title":"Notation"},{"location":"memory/kv_caching/#without-kv-cache","text":"At each decoding step, attention is recomputed for all previous tokens: $$ O(L \\cdot H \\cdot d_h \\cdot T^3) $$","title":"Without KV Cache"},{"location":"memory/kv_caching/#with-kv-cache","text":"Only attention against cached keys and values is computed: $$ O(L \\cdot H \\cdot d_h \\cdot T^2) $$ KV caching removes one full factor of $T$ from decoding complexity.","title":"With KV Cache"},{"location":"memory/kv_caching/#6-memory-cost","text":"Each layer stores: $$ K, V \\in \\mathbb{R}^{H \\times T \\times d_h} $$ Total KV cache memory across all layers: $$ O(L \\cdot H \\cdot T \\cdot d_h) $$ For long context inference, KV cache memory is often the dominant bottleneck.","title":"\ud83d\udce66. Memory Cost"},{"location":"memory/kv_caching/#7-inference-vs-training-usage","text":"","title":"\ud83d\udce67. Inference v/s Training Usage"},{"location":"memory/kv_caching/#71-during-inference","text":"This is the most common and important usage. Inference Workflow Encode prompt Initialize empty KV cache per layer For each generated token: Compute $Q_t, K_t, V_t$ Append $K_t, V_t$ to cache Compute attention using cached tensors Practical Benefits Faster decoding Lower FLOPs Enables long context generation Essential for streaming and chat systems","title":"7.1 During Inference"},{"location":"memory/kv_caching/#72-during-training","text":"KV caching is not used in standard full sequence training. Why? Training processes full sequences in parallel All tokens attend to each other simultaneously No repeated computation across steps","title":"7.2 During Training"},{"location":"memory/kv_caching/#8-scaling-kv-cache-for-long-context","text":"Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.","title":"\ud83d\udce68. Scaling KV Cache for Long Context"},{"location":"memory/kv_caching/#81-sliding-window-attention","text":"Only retain keys and values for the most recent $W$ tokens: $$ K_{t-W:t}, V_{t-W:t} $$ This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.","title":"8.1 Sliding Window Attention"},{"location":"memory/kv_caching/#82-kv-cache-quantization","text":"KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.","title":"8.2 KV Cache Quantization"},{"location":"memory/kv_caching/#what-gets-quantized","text":"Both keys and values can be quantized, but they have different sensitivity: Keys (K) directly affect attention scores $QK^T$ Values (V) affect the weighted sum after softmax As a result: Keys usually require higher precision Values tolerate more aggressive quantization","title":"What Gets Quantized"},{"location":"memory/kv_caching/#common-quantization-schemes","text":"Component Typical Format Notes Keys FP16 / BF16 Preserves attention score stability Values INT8 Large memory reduction with minimal quality loss Both INT8 or INT4 Used for extreme long context scenarios Mixed precision KV cache is widely used in practice.","title":"Common Quantization Schemes"},{"location":"memory/kv_caching/#quantization-granularity","text":"KV cache quantization can be applied at different levels: Per tensor : One scale for entire K or V tensor Per head : Separate scale per attention head Per channel : Separate scale per head dimension Finer granularity improves accuracy but increases metadata and compute overhead.","title":"Quantization Granularity"},{"location":"memory/kv_caching/#dequantization-during-attention","text":"At decoding step $t$: Load quantized $K, V$ from cache Dequantize to FP16 or BF16 Compute attention normally: $$ \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} $$ Dequantization cost is small compared to memory bandwidth savings.","title":"Dequantization During Attention"},{"location":"memory/kv_caching/#impact-on-performance","text":"Benefits: 2x to 4x KV memory reduction Higher batch size and longer context Improved inference throughput due to reduced memory traffic Tradeoffs: Slight loss in generation quality Additional dequantization overhead In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.","title":"Impact on Performance"},{"location":"memory/kv_caching/#interaction-with-other-optimizations","text":"GQA further reduces KV cache size and works well with quantization Paged KV cache benefits from smaller KV blocks FlashAttention amortizes dequantization overhead inside fused kernels","title":"Interaction with Other Optimizations"},{"location":"memory/kv_caching/#83-prefix-caching","text":"When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.","title":"8.3 Prefix Caching"},{"location":"memory/kv_caching/#84-paged-kv-cache","text":"KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.","title":"8.4 Paged KV Cache"},{"location":"memory/kv_caching/#9-grouped-query-attention-gqa","text":"Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.","title":"\ud83d\udce69. Grouped Query Attention (GQA)"},{"location":"memory/kv_caching/#91-head-configuration","text":"$$ H_q > H_k = H_v $$ Example: Query heads $H_q = 32$ Key value heads $H_k = 8$ This reduces KV cache memory by a factor of $H_q / H_k$.","title":"9.1 Head Configuration"},{"location":"memory/kv_caching/#92-qk-computation-with-mismatched-heads","text":"Each key value head is shared by a fixed group of query heads. Let: $$ g = \\frac{H_q}{H_k} $$ Each key value head serves $g$ query heads. For query head $i$, the corresponding key value head index is: $$ \\left\\lfloor \\frac{i}{g} \\right\\rfloor $$ The attention computation becomes: $$ \\text{Attn} i = \\text{softmax}\\left(\\frac{Q_i K {\\left\\lfloor i/g \\right\\rfloor}^T}{\\sqrt{d_h}}\\right)V_{\\left\\lfloor i/g \\right\\rfloor} $$ Keys and values are reused directly without additional projection or averaging.","title":"9.2 QK Computation with Mismatched Heads"},{"location":"memory/kv_caching/#93-why-gqa-is-effective","text":"Query heads retain expressive power Keys and values capture shared context KV cache size and memory bandwidth are significantly reduced GQA is widely used in production LLMs.","title":"9.3 Why GQA Is Effective"},{"location":"memory/kv_caching/#10-other-common-optimizations","text":"","title":"\ud83d\udce610. Other Common Optimizations"},{"location":"memory/kv_caching/#flashattention","text":"FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.","title":"FlashAttention"},{"location":"memory/kv_caching/#chunked-prefill","text":"Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.","title":"Chunked Prefill"},{"location":"memory/kv_caching/#speculative-decoding","text":"Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.","title":"Speculative Decoding"},{"location":"speed/flash_attention/","text":"Flash Attention 1. Overview FlashAttention is a fast and memory-efficient implementation of the attention mechanism used in Transformer models. This repository explains what FlashAttention is, why it is faster than standard attention, and how it works under the hood, with a focus on interview preparation and practical understanding. 2. Motivation Attention is the core operation behind Transformers, but standard attention becomes a major bottleneck for long sequences. The main problem is not only compute, but memory movement , which is often the true limiter on modern GPUs. FlashAttention was introduced to: Reduce memory usage Minimize expensive GPU memory reads and writes Scale efficiently to long sequences 3. Standard Attention and Its Limitations Given query, key, and value matrices: $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$ While simple and elegant, this formulation has serious performance and memory issues for long sequences. 3.1 Quadratic Memory Growth Assume: Sequence length (N = 16{,}384) FP16 precision (2 bytes per element) The attention score matrix (QK^T) has: $$ N^2 = 16{,}384^2 \\approx 268 \\text{ million elements} $$ Memory required just for the attention matrix: $$ 268\\text{M} \\times 2 \\text{ bytes} \\approx 512 \\text{ MB} $$ This does not include the softmax output, gradients during training, or activations from other layers, which can easily exceed GPU memory limits. 3.2 Excessive Memory Traffic Standard attention performs multiple memory-heavy steps: Compute $QK^T$ and write to GPU global memory Read $QK^T$ back to apply softmax Write softmax output back to memory Read softmax output again to compute weighted sum with (V) Even with fast compute, repeated global memory reads and writes dominate runtime, making GPUs often memory-bound rather than compute-bound. 3.3 Inefficient for Long Sequences (Code Example) A simplified PyTorch-style implementation: import torch import math # Q, K, V shape: (batch, seq_len, num_heads, head_dim) scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d) attn = torch.softmax(scores, dim=-1) output = torch.matmul(attn, V) What happens internally: scores materializes a full $N\u00d7N$ tensor attn creates another $N\u00d7N$ tensor Both tensors live in global memory As N grows, memory usage and latency grow quadratically. 3.4 Numerical Issues with Low Precision With FP16 or BF16: Large dot products in $QK^T$ can overflow Small values can underflow to zero Standard attention often requires casting to FP32 for stability, which further increases memory usage and slows execution. 4. What Is FlashAttention and How It Works FlashAttention is an exact, memory-efficient attention algorithm . It computes the same result as standard attention but avoids materializing the full $N \\times N$ attention matrix. This makes it much faster and reduces GPU memory usage, especially for long sequences. Key advantages: Handles long sequences efficiently (e.g., 4k+ tokens) Works in FP16 and BF16 without numerical issues Reduces memory bandwidth usage with minimal extra compute FlashAttention achieves this through three main ideas: tiling , fused computation , and single-pass attention with online softmax . 4.1 Tiling Instead of computing attention for the full sequence at once, FlashAttention splits the query, key, and value matrices into small tiles that fit into GPU shared memory. Example: Sequence length: $N = 16{,}384$ Tile size: $B = 128$ Memory usage for a tile: $128 \\times 128 = 16{,}384$ elements (much smaller than $(16{,}384)^2$) Code-style intuition: # pseudo-code for tiling for q_tile in Q_tiles: for k_tile, v_tile in zip(K_tiles, V_tiles): partial_scores = q_tile @ k_tile.T # accumulate results incrementally Benefit: Only a small block is in memory at a time, reducing GPU memory footprint dramatically. 4.2 Fused Computation FlashAttention fuses multiple steps into a single kernel: Matrix multiplication $(Q \\cdot K^T)$ Scaling by $(1/\\sqrt{d})$ Softmax computation Weighted sum with $(V)$ Why this matters: Standard attention performs each step separately, writing intermediate results to global memory. FlashAttention keeps all intermediate computations in shared memory , avoiding costly reads/writes. Example intuition: # pseudo-code for fused attention output_tile = flash_attention(q_tile, k_tile, v_tile) Here, flash_attention does all four steps at once, producing the final output for that tile. 4.3 Single-Pass Attention and Online Softmax FlashAttention computes attention in one streaming pass: Compute partial scores for each tile Update running maximum and normalization term for softmax Accumulate output incrementally This allows numerically stable softmax in FP16/BF16 without ever storing the full attention matrix. Example numerical intuition: Tile 1 contributes scores [0.1, 0.5, 0.3] Tile 2 contributes [0.2, 0.4, 0.1] Running softmax computes the final normalized weights across tiles incrementally Benefit: Exact same result as full attention Avoids overflow/underflow in low precision Reduces memory reads/writes drastically 4.4 Practical Impact Memory complexity reduced from $O(N^2) \u2192 O(N\u22c5B)$ where $B$ is tile size Enables training with longer sequences or larger batch sizes Provides 2\u20134x speedups for long sequences on modern GPUs Code example using PyTorch API: from flash_attn import flash_attn_func # q, k, v shape: (batch, seq_len, num_heads, head_dim) output = flash_attn_func(q, k, v, dropout_p=0.0, causal=False) This produces exact attention results while being faster and more memory-efficient than standard attention. 5. When FlashAttention Helps (and When It Does Not) Works best when: Sequence length is large (typically 2k tokens or more) Using FP16 or BF16 Running on modern NVIDIA GPUs with fast shared memory Less useful when: Sequence length is very short CPU-based inference Custom attention patterns not supported by FlashAttention kernels 6. Why is online softmax needed? 6.1. Numerical Stability Problem Standard softmax is computed as: $$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$ Issue in FP16/BF16: FP16 has limited precision (~3\u20134 decimal digits) and a small exponent range. Large values of $(x_i)$ (e.g., 50) cause (e^{x_i}) to overflow . Very negative values of $(x_i)$ (e.g., -50) cause $(e^{x_i})$ to underflow to zero. Long sequences exacerbate the problem because summing hundreds or thousands of exponentials increases the risk of overflow/underflow. Without precautions, computing softmax in FP16 can produce NaNs or zeros , breaking both training and inference. 6.2. Why \u201cOnline\u201d Softmax Helps FlashAttention computes attention tile by tile , so it cannot store the full $N \\times N$ attention matrix. To compute softmax correctly across the entire sequence in FP16/BF16, it uses online softmax . How It Works Maintain a running maximum $m$ across tiles. Shift scores before exponentiating: $e^{x_i - m}$ Prevents overflow in exponential. Maintain a running sum of exponentials across tiles. Partial sums from each tile are combined incrementally. Ensures correct normalization for the softmax over the full sequence. Compute the weighted sum with $V$ incrementally . No full softmax matrix is stored in memory. Output is accumulated as each tile is processed. Example Suppose we have 2 tiles with attention scores: Tile 1: [0.1, 0.5, 0.3] Tile 2: [0.2, 0.4, 0.1] Standard softmax (if we could store all scores): $$ \\text{softmax}([0.1, 0.5, 0.3, 0.2, 0.4, 0.1]) $$ Online softmax computation: Tile 1 Running max (m = 0.5) Compute shifted exponentials: [exp(0.1-0.5), exp(0.5-0.5), exp(0.3-0.5)] \u2248 [0.67, 1.0, 0.82] Running sum (s = 0.67 + 1.0 + 0.82 = 2.49) Partial weighted sum with $V$ stored in output Tile 2 New max $m = \\max(0.5, 0.4) = 0.5$ (same in this case) Shifted exponentials: [exp(0.2-0.5), exp(0.4-0.5), exp(0.1-0.5)] \u2248 [0.74, 0.90, 0.61] Update running sum: (s = 2.49 + 0.74 + 0.90 + 0.61 = 4.74) Accumulate weighted sum with $V$ Normalization Each accumulated output is divided by the final sum (s = 4.74) Produces exact same softmax result as computing on the full sequence Key Benefits Computes exact attention even in FP16/BF16 Works efficiently with long sequences and large tiles Avoids storing huge intermediate matrices Reduces GPU memory usage and memory bandwidth overhead In short: Online softmax allows FlashAttention to compute attention tile by tile while staying numerically stable and memory-efficient. 7. End-to-End FlashAttention Example Suppose we have: Sequence length $N = 8$ (small for simplicity) Head dimension $d = 2$ Tile size $B = 4$ We want to compute attention for a single head: $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$ Step 1: Prepare Q, K, V import torch import math Q = torch.tensor([[0.1, 0.2], [0.3, 0.1], [0.0, 0.4], [0.5, 0.2], [0.3, 0.3], [0.1, 0.5], [0.4, 0.0], [0.2, 0.1]]) # shape: (8, 2) K = Q.clone() # for simplicity V = torch.arange(8*2).reshape(8,2).float() # dummy value matrix Step 2: Split into Tiles To reduce memory usage, FlashAttention splits the sequence into smaller tiles that fit into GPU shared memory. Tile size (B=4) \u2192 2 tiles along the sequence # Split Q, K, V into tiles Q_tiles = [Q[:4], Q[4:]] # tile 1 and tile 2 K_tiles = [K[:4], K[4:]] V_tiles = [V[:4], V[4:]] Benefit: Only a small portion of the sequence is in memory at a time, avoiding the need to materialize the full attention matrix. Step 3: Process Tile 1 Compute partial scores in shared memory: $$ \\text{scores} = Q_\\text{tile1} \\cdot K_\\text{tile1}^T / \\sqrt{d} $$ python scores_tile1 = Q_tiles[0] @ K_tiles[0].T / math.sqrt(2) Apply online softmax: Compute max of scores: m = scores_tile1.max(dim=1) Shift and exponentiate: exp_scores = torch.exp(scores_tile1 - m) Running sum: s = exp_scores.sum(dim=1) Partial weighted sum with V: output_tile1 = (exp_scores @ V_tiles[0]) / s Memory benefit: only a 4\u00d74 matrix exists at a time. Step 4: Process Tile 2 Incrementally Compute partial scores of Q_tile1 \u00d7 K_tile2^T Update running max and running sum for online softmax Accumulate weighted outputs with V_tile2 Repeat for Q_tile2 \u00d7 K_tile1^T and Q_tile2 \u00d7 K_tile2^T No full 8\u00d78 attention matrix is ever materialized. Step 5: Accumulate Output Incrementally compute the weighted sum across all tiles Resulting output shape (8, 2) matches standard attention Softmax computed exactly using online normalization","title":"Flash Attention"},{"location":"speed/flash_attention/#flash-attention","text":"","title":"Flash Attention"},{"location":"speed/flash_attention/#1-overview","text":"FlashAttention is a fast and memory-efficient implementation of the attention mechanism used in Transformer models. This repository explains what FlashAttention is, why it is faster than standard attention, and how it works under the hood, with a focus on interview preparation and practical understanding.","title":"1. Overview"},{"location":"speed/flash_attention/#2-motivation","text":"Attention is the core operation behind Transformers, but standard attention becomes a major bottleneck for long sequences. The main problem is not only compute, but memory movement , which is often the true limiter on modern GPUs. FlashAttention was introduced to: Reduce memory usage Minimize expensive GPU memory reads and writes Scale efficiently to long sequences","title":"2. Motivation"},{"location":"speed/flash_attention/#3-standard-attention-and-its-limitations","text":"Given query, key, and value matrices: $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$ While simple and elegant, this formulation has serious performance and memory issues for long sequences.","title":"3. Standard Attention and Its Limitations"},{"location":"speed/flash_attention/#31-quadratic-memory-growth","text":"Assume: Sequence length (N = 16{,}384) FP16 precision (2 bytes per element) The attention score matrix (QK^T) has: $$ N^2 = 16{,}384^2 \\approx 268 \\text{ million elements} $$ Memory required just for the attention matrix: $$ 268\\text{M} \\times 2 \\text{ bytes} \\approx 512 \\text{ MB} $$ This does not include the softmax output, gradients during training, or activations from other layers, which can easily exceed GPU memory limits.","title":"3.1 Quadratic Memory Growth"},{"location":"speed/flash_attention/#32-excessive-memory-traffic","text":"Standard attention performs multiple memory-heavy steps: Compute $QK^T$ and write to GPU global memory Read $QK^T$ back to apply softmax Write softmax output back to memory Read softmax output again to compute weighted sum with (V) Even with fast compute, repeated global memory reads and writes dominate runtime, making GPUs often memory-bound rather than compute-bound.","title":"3.2 Excessive Memory Traffic"},{"location":"speed/flash_attention/#33-inefficient-for-long-sequences-code-example","text":"A simplified PyTorch-style implementation: import torch import math # Q, K, V shape: (batch, seq_len, num_heads, head_dim) scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d) attn = torch.softmax(scores, dim=-1) output = torch.matmul(attn, V) What happens internally: scores materializes a full $N\u00d7N$ tensor attn creates another $N\u00d7N$ tensor Both tensors live in global memory As N grows, memory usage and latency grow quadratically.","title":"3.3 Inefficient for Long Sequences (Code Example)"},{"location":"speed/flash_attention/#34-numerical-issues-with-low-precision","text":"With FP16 or BF16: Large dot products in $QK^T$ can overflow Small values can underflow to zero Standard attention often requires casting to FP32 for stability, which further increases memory usage and slows execution.","title":"3.4 Numerical Issues with Low Precision"},{"location":"speed/flash_attention/#4-what-is-flashattention-and-how-it-works","text":"FlashAttention is an exact, memory-efficient attention algorithm . It computes the same result as standard attention but avoids materializing the full $N \\times N$ attention matrix. This makes it much faster and reduces GPU memory usage, especially for long sequences. Key advantages: Handles long sequences efficiently (e.g., 4k+ tokens) Works in FP16 and BF16 without numerical issues Reduces memory bandwidth usage with minimal extra compute FlashAttention achieves this through three main ideas: tiling , fused computation , and single-pass attention with online softmax .","title":"4. What Is FlashAttention and How It Works"},{"location":"speed/flash_attention/#41-tiling","text":"Instead of computing attention for the full sequence at once, FlashAttention splits the query, key, and value matrices into small tiles that fit into GPU shared memory. Example: Sequence length: $N = 16{,}384$ Tile size: $B = 128$ Memory usage for a tile: $128 \\times 128 = 16{,}384$ elements (much smaller than $(16{,}384)^2$) Code-style intuition: # pseudo-code for tiling for q_tile in Q_tiles: for k_tile, v_tile in zip(K_tiles, V_tiles): partial_scores = q_tile @ k_tile.T # accumulate results incrementally Benefit: Only a small block is in memory at a time, reducing GPU memory footprint dramatically.","title":"4.1 Tiling"},{"location":"speed/flash_attention/#42-fused-computation","text":"FlashAttention fuses multiple steps into a single kernel: Matrix multiplication $(Q \\cdot K^T)$ Scaling by $(1/\\sqrt{d})$ Softmax computation Weighted sum with $(V)$ Why this matters: Standard attention performs each step separately, writing intermediate results to global memory. FlashAttention keeps all intermediate computations in shared memory , avoiding costly reads/writes. Example intuition: # pseudo-code for fused attention output_tile = flash_attention(q_tile, k_tile, v_tile) Here, flash_attention does all four steps at once, producing the final output for that tile.","title":"4.2 Fused Computation"},{"location":"speed/flash_attention/#43-single-pass-attention-and-online-softmax","text":"FlashAttention computes attention in one streaming pass: Compute partial scores for each tile Update running maximum and normalization term for softmax Accumulate output incrementally This allows numerically stable softmax in FP16/BF16 without ever storing the full attention matrix. Example numerical intuition: Tile 1 contributes scores [0.1, 0.5, 0.3] Tile 2 contributes [0.2, 0.4, 0.1] Running softmax computes the final normalized weights across tiles incrementally Benefit: Exact same result as full attention Avoids overflow/underflow in low precision Reduces memory reads/writes drastically","title":"4.3 Single-Pass Attention and Online Softmax"},{"location":"speed/flash_attention/#44-practical-impact","text":"Memory complexity reduced from $O(N^2) \u2192 O(N\u22c5B)$ where $B$ is tile size Enables training with longer sequences or larger batch sizes Provides 2\u20134x speedups for long sequences on modern GPUs Code example using PyTorch API: from flash_attn import flash_attn_func # q, k, v shape: (batch, seq_len, num_heads, head_dim) output = flash_attn_func(q, k, v, dropout_p=0.0, causal=False) This produces exact attention results while being faster and more memory-efficient than standard attention.","title":"4.4 Practical Impact"},{"location":"speed/flash_attention/#5-when-flashattention-helps-and-when-it-does-not","text":"Works best when: Sequence length is large (typically 2k tokens or more) Using FP16 or BF16 Running on modern NVIDIA GPUs with fast shared memory Less useful when: Sequence length is very short CPU-based inference Custom attention patterns not supported by FlashAttention kernels","title":"5. When FlashAttention Helps (and When It Does Not)"},{"location":"speed/flash_attention/#6-why-is-online-softmax-needed","text":"","title":"6. Why is online softmax needed?"},{"location":"speed/flash_attention/#61-numerical-stability-problem","text":"Standard softmax is computed as: $$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$ Issue in FP16/BF16: FP16 has limited precision (~3\u20134 decimal digits) and a small exponent range. Large values of $(x_i)$ (e.g., 50) cause (e^{x_i}) to overflow . Very negative values of $(x_i)$ (e.g., -50) cause $(e^{x_i})$ to underflow to zero. Long sequences exacerbate the problem because summing hundreds or thousands of exponentials increases the risk of overflow/underflow. Without precautions, computing softmax in FP16 can produce NaNs or zeros , breaking both training and inference.","title":"6.1. Numerical Stability Problem"},{"location":"speed/flash_attention/#62-why-online-softmax-helps","text":"FlashAttention computes attention tile by tile , so it cannot store the full $N \\times N$ attention matrix. To compute softmax correctly across the entire sequence in FP16/BF16, it uses online softmax .","title":"6.2. Why \u201cOnline\u201d Softmax Helps"},{"location":"speed/flash_attention/#how-it-works","text":"Maintain a running maximum $m$ across tiles. Shift scores before exponentiating: $e^{x_i - m}$ Prevents overflow in exponential. Maintain a running sum of exponentials across tiles. Partial sums from each tile are combined incrementally. Ensures correct normalization for the softmax over the full sequence. Compute the weighted sum with $V$ incrementally . No full softmax matrix is stored in memory. Output is accumulated as each tile is processed.","title":"How It Works"},{"location":"speed/flash_attention/#example","text":"Suppose we have 2 tiles with attention scores: Tile 1: [0.1, 0.5, 0.3] Tile 2: [0.2, 0.4, 0.1] Standard softmax (if we could store all scores): $$ \\text{softmax}([0.1, 0.5, 0.3, 0.2, 0.4, 0.1]) $$ Online softmax computation: Tile 1 Running max (m = 0.5) Compute shifted exponentials: [exp(0.1-0.5), exp(0.5-0.5), exp(0.3-0.5)] \u2248 [0.67, 1.0, 0.82] Running sum (s = 0.67 + 1.0 + 0.82 = 2.49) Partial weighted sum with $V$ stored in output Tile 2 New max $m = \\max(0.5, 0.4) = 0.5$ (same in this case) Shifted exponentials: [exp(0.2-0.5), exp(0.4-0.5), exp(0.1-0.5)] \u2248 [0.74, 0.90, 0.61] Update running sum: (s = 2.49 + 0.74 + 0.90 + 0.61 = 4.74) Accumulate weighted sum with $V$ Normalization Each accumulated output is divided by the final sum (s = 4.74) Produces exact same softmax result as computing on the full sequence","title":"Example"},{"location":"speed/flash_attention/#key-benefits","text":"Computes exact attention even in FP16/BF16 Works efficiently with long sequences and large tiles Avoids storing huge intermediate matrices Reduces GPU memory usage and memory bandwidth overhead In short: Online softmax allows FlashAttention to compute attention tile by tile while staying numerically stable and memory-efficient.","title":"Key Benefits"},{"location":"speed/flash_attention/#7-end-to-end-flashattention-example","text":"Suppose we have: Sequence length $N = 8$ (small for simplicity) Head dimension $d = 2$ Tile size $B = 4$ We want to compute attention for a single head: $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$","title":"7. End-to-End FlashAttention Example"},{"location":"speed/flash_attention/#step-1-prepare-q-k-v","text":"import torch import math Q = torch.tensor([[0.1, 0.2], [0.3, 0.1], [0.0, 0.4], [0.5, 0.2], [0.3, 0.3], [0.1, 0.5], [0.4, 0.0], [0.2, 0.1]]) # shape: (8, 2) K = Q.clone() # for simplicity V = torch.arange(8*2).reshape(8,2).float() # dummy value matrix","title":"Step 1: Prepare Q, K, V"},{"location":"speed/flash_attention/#step-2-split-into-tiles","text":"To reduce memory usage, FlashAttention splits the sequence into smaller tiles that fit into GPU shared memory. Tile size (B=4) \u2192 2 tiles along the sequence # Split Q, K, V into tiles Q_tiles = [Q[:4], Q[4:]] # tile 1 and tile 2 K_tiles = [K[:4], K[4:]] V_tiles = [V[:4], V[4:]] Benefit: Only a small portion of the sequence is in memory at a time, avoiding the need to materialize the full attention matrix.","title":"Step 2: Split into Tiles"},{"location":"speed/flash_attention/#step-3-process-tile-1","text":"Compute partial scores in shared memory: $$ \\text{scores} = Q_\\text{tile1} \\cdot K_\\text{tile1}^T / \\sqrt{d} $$ python scores_tile1 = Q_tiles[0] @ K_tiles[0].T / math.sqrt(2) Apply online softmax: Compute max of scores: m = scores_tile1.max(dim=1) Shift and exponentiate: exp_scores = torch.exp(scores_tile1 - m) Running sum: s = exp_scores.sum(dim=1) Partial weighted sum with V: output_tile1 = (exp_scores @ V_tiles[0]) / s Memory benefit: only a 4\u00d74 matrix exists at a time.","title":"Step 3: Process Tile 1"},{"location":"speed/flash_attention/#step-4-process-tile-2-incrementally","text":"Compute partial scores of Q_tile1 \u00d7 K_tile2^T Update running max and running sum for online softmax Accumulate weighted outputs with V_tile2 Repeat for Q_tile2 \u00d7 K_tile1^T and Q_tile2 \u00d7 K_tile2^T No full 8\u00d78 attention matrix is ever materialized.","title":"Step 4: Process Tile 2 Incrementally"},{"location":"speed/flash_attention/#step-5-accumulate-output","text":"Incrementally compute the weighted sum across all tiles Resulting output shape (8, 2) matches standard attention Softmax computed exactly using online normalization","title":"Step 5: Accumulate Output"},{"location":"speed/speculative_decoding/","text":"\ud83d\udce61. Speculative Decoding: Overview Speculative decoding reduces inference latency in decoder-only LLMs while preserving the exact output distribution of a large target model. 1.1 Why standard decoding is slow In standard autoregressive decoding: The target model generates one token per forward pass The model is large and expensive Latency grows linearly with output length KV cache reduces computation but does not remove the sequential bottleneck. 1.2 Core idea Speculative decoding separates token proposal from token verification : A draft model proposes multiple tokens cheaply A target model verifies them efficiently If most draft tokens are accepted, multiple tokens are generated per expensive target model forward pass. \ud83d\udce62. Background: How Decoding Works in Decoder-Only LLMs Before speculative decoding, it is critical to understand standard autoregressive decoding. 2.1 Autoregressive modeling assumption A decoder-only LLM models a sequence of tokens using the following factorization: $$ P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\dots, x_{t-1}) $$ Key points: Tokens are generated left to right Each new token depends on all previous tokens There is no notion of predicting multiple future tokens independently 2.2 What happens in one forward pass Assume the current sequence length is $T$. Step 1: Embedding Each token $x_t$ is mapped to a vector representation: $$ \\mathbf{e}_t = \\text{TokenEmbed}(x_t) + \\text{PosEmbed}(t) $$ Step 2: Masked self-attention For each token position $t$: $$ \\mathbf{q}_t = \\mathbf{e}_t W_Q,\\quad \\mathbf{k}_t = \\mathbf{e}_t W_K,\\quad \\mathbf{v}_t = \\mathbf{e}_t W_V $$ Attention scores are computed as: $$ \\alpha_{t,i} = \\frac{\\mathbf{q}_t \\cdot \\mathbf{k}_i}{\\sqrt{d_k}} $$ A causal mask ensures token $t$ can only attend to tokens $i \\le t$. The attended representation is: $$ \\mathbf{a} t = \\sum {i=1}^{t} \\text{softmax}(\\alpha_{t,i}) \\mathbf{v}_i $$ This is followed by a linear projection: $$ \\mathbf{o}_t = \\mathbf{a}_t W_O $$ Step 3: Feed Forward Network (FFN) Each token is processed independently: $$ \\mathbf{h}_t = \\text{FFN}(\\mathbf{o}_t) $$ Residual connections and layer normalization are applied around both attention and FFN blocks. This completes one decoder layer. The same process repeats across multiple stacked layers. 2.3 Computing logits and selecting the next token After the final decoder layer, each token position has a hidden state $\\mathbf{h}_t$. These are projected to vocabulary logits: $$ \\mathbf{z} t = \\mathbf{h}_t W {\\text{vocab}} \\quad \\text{where } \\mathbf{z}_t \\in \\mathbb{R}^{|V|} $$ Important clarification: Logits are computed for every token position Softmax is applied over the vocabulary During decoding, only the last position is used $$ P(x_{T+1} \\mid x_{\\le T}) = \\text{softmax}(\\mathbf{z}_T) $$ A token is selected using greedy decoding or sampling and appended to the sequence. 2.4 Autoregressive decoding loop For each generated token: Run the Transformer forward pass Take logits from the last token position Apply softmax over the vocabulary Select one token Append it to the sequence Repeat This process continues until an end-of-sequence token is produced or a maximum length is reached. 2.5 Key limitation of standard decoding Each generated token requires a new forward pass of the model Latency grows linearly with output l \ud83d\udce63. Step-by-Step Algorithm for Speculative Decoding This section describes the speculative decoding algorithm precisely, step by step, focusing on what each model does and why it is needed. Assume: Prompt tokens: $x$ Draft model: $q$ Target model: $p$ Draft length: $k$ Drafted tokens: $y_1, y_2, \\dots, y_k$ Step 1: Draft model proposes tokens The draft model generates tokens autoregressively, starting from the prompt: $$ y_i \\sim q(\\cdot \\mid x, y_{<i}) \\quad \\text{for } i = 1 \\dots k $$ Key points: This step is fast because the draft model is small Tokens are sampled, not greedily selected The draft model also records the probability of each sampled token Step 2: Target model verifies the draft The target model is run once on the combined sequence: $[x, y1, y2, ..., yk]$ This produces target model probabilities: $$ p(y_i \\mid x, y_{<i}) \\quad \\text{for } i = 1 \\dots k $$ Important clarification: The target model naturally computes logits for all positions Only logits corresponding to the drafted tokens are used Logits for the prompt tokens are ignored Step 3: Acceptance test Each drafted token is accepted independently using: $$ \\alpha_i = \\min\\left(1, \\frac{p(y_i \\mid x, y_{<i})}{q(y_i \\mid x, y_{<i})}\\right) $$ Procedure: Sample $u \\sim \\text{Uniform}(0, 1)$ Accept token $y_i$ if $u \\le \\alpha_i$ Stop at the first rejected token Step 4: Rejection handling and fallback If token $y_j$ is rejected: Tokens $y_j, y_{j+1}, \\dots, y_k$ are discarded The next token is sampled directly from the target model: $$ x_{\\text{next}} \\sim p(\\cdot \\mid x, y_{<j}) $$ Speculative decoding restarts from the new prefix If no token is rejected, all $k$ draft tokens are accepted. Why this preserves correctness The acceptance rule implements rejection sampling Bias introduced by the draft model is corrected The final output distribution exactly matches the target model This guarantees that speculative decoding is statistically equivalent to standard decoding with the target model. \ud83d\udce64. Why Speculative Decoding Is Faster Speculative decoding reduces inference latency by decreasing how often the expensive target model must be executed. 4.1 Standard decoding vs speculative decoding Standard decoding One target model forward pass per generated token For $N$ tokens, $N$ forward passes are required Speculative decoding The draft model proposes $k$ tokens cheaply A single target model forward pass verifies up to $k$ tokens Multiple tokens can be generated per target model invocation 4.2 Source of the speedup The speedup comes from two properties: The draft model is significantly cheaper than the target model The target model can evaluate multiple draft tokens in parallel If the acceptance rate is high, the target model is called much less frequently. 4.3 What speculative decoding does not optimize Speculative decoding does not reduce: The total number of FLOPs in the target model The per-token computation inside the Transformer It primarily reduces latency , not theoretical compute. 4.4 Practical speedups In practice, speculative decoding often achieves: 1.5x to 3x latency improvement Higher gains when draft and target distributions are close Actual speedup depends on model sizes, hardware, and acceptance rate. 5. Relationship to Logits for All Tokens Speculative decoding does not introduce a new requirement to compute logits for all tokens. 5.1 Standard Transformer behavior A Transformer forward pass naturally produces: One hidden state per token position One vocabulary logits vector per token position This is true during both training and inference. 5.2 How logits are used in speculative decoding During verification: The target model is run once on the prompt plus draft tokens Logits for prompt tokens are ignored Only logits corresponding to the draft token positions are used Speculative decoding simply reuses standard per-position logits. 5.3 What speculative decoding does not do Speculative decoding does not: Perform softmax over token positions Predict future tokens independently Generate tokens in parallel from the target model The target model still defines an autoregressive distribution. 6. Interaction with KV Cache KV cache improves performance in speculative decoding but does not change the algorithm. 6.1 KV cache in the draft model The draft model maintains its own KV cache Draft tokens are generated autoregressively KV cache allows fast token proposal 6.2 KV cache in the target model The target model computes KV cache for the entire speculative window KV states corresponding to accepted tokens are reused KV states for rejected tokens are discarded 6.3 Why KV cache matters KV cache: Avoids recomputing attention for previously processed tokens Reduces per-step computation Improves practical throughput KV cache affects efficiency only, not correctness.","title":"Speculative decoding"},{"location":"speed/speculative_decoding/#1-speculative-decoding-overview","text":"Speculative decoding reduces inference latency in decoder-only LLMs while preserving the exact output distribution of a large target model.","title":"\ud83d\udce61. Speculative Decoding: Overview"},{"location":"speed/speculative_decoding/#11-why-standard-decoding-is-slow","text":"In standard autoregressive decoding: The target model generates one token per forward pass The model is large and expensive Latency grows linearly with output length KV cache reduces computation but does not remove the sequential bottleneck.","title":"1.1 Why standard decoding is slow"},{"location":"speed/speculative_decoding/#12-core-idea","text":"Speculative decoding separates token proposal from token verification : A draft model proposes multiple tokens cheaply A target model verifies them efficiently If most draft tokens are accepted, multiple tokens are generated per expensive target model forward pass.","title":"1.2 Core idea"},{"location":"speed/speculative_decoding/#2-background-how-decoding-works-in-decoder-only-llms","text":"Before speculative decoding, it is critical to understand standard autoregressive decoding.","title":"\ud83d\udce62. Background: How Decoding Works in Decoder-Only LLMs"},{"location":"speed/speculative_decoding/#21-autoregressive-modeling-assumption","text":"A decoder-only LLM models a sequence of tokens using the following factorization: $$ P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\dots, x_{t-1}) $$ Key points: Tokens are generated left to right Each new token depends on all previous tokens There is no notion of predicting multiple future tokens independently","title":"2.1 Autoregressive modeling assumption"},{"location":"speed/speculative_decoding/#22-what-happens-in-one-forward-pass","text":"Assume the current sequence length is $T$.","title":"2.2 What happens in one forward pass"},{"location":"speed/speculative_decoding/#step-1-embedding","text":"Each token $x_t$ is mapped to a vector representation: $$ \\mathbf{e}_t = \\text{TokenEmbed}(x_t) + \\text{PosEmbed}(t) $$","title":"Step 1: Embedding"},{"location":"speed/speculative_decoding/#step-2-masked-self-attention","text":"For each token position $t$: $$ \\mathbf{q}_t = \\mathbf{e}_t W_Q,\\quad \\mathbf{k}_t = \\mathbf{e}_t W_K,\\quad \\mathbf{v}_t = \\mathbf{e}_t W_V $$ Attention scores are computed as: $$ \\alpha_{t,i} = \\frac{\\mathbf{q}_t \\cdot \\mathbf{k}_i}{\\sqrt{d_k}} $$ A causal mask ensures token $t$ can only attend to tokens $i \\le t$. The attended representation is: $$ \\mathbf{a} t = \\sum {i=1}^{t} \\text{softmax}(\\alpha_{t,i}) \\mathbf{v}_i $$ This is followed by a linear projection: $$ \\mathbf{o}_t = \\mathbf{a}_t W_O $$","title":"Step 2: Masked self-attention"},{"location":"speed/speculative_decoding/#step-3-feed-forward-network-ffn","text":"Each token is processed independently: $$ \\mathbf{h}_t = \\text{FFN}(\\mathbf{o}_t) $$ Residual connections and layer normalization are applied around both attention and FFN blocks. This completes one decoder layer. The same process repeats across multiple stacked layers.","title":"Step 3: Feed Forward Network (FFN)"},{"location":"speed/speculative_decoding/#23-computing-logits-and-selecting-the-next-token","text":"After the final decoder layer, each token position has a hidden state $\\mathbf{h}_t$. These are projected to vocabulary logits: $$ \\mathbf{z} t = \\mathbf{h}_t W {\\text{vocab}} \\quad \\text{where } \\mathbf{z}_t \\in \\mathbb{R}^{|V|} $$ Important clarification: Logits are computed for every token position Softmax is applied over the vocabulary During decoding, only the last position is used $$ P(x_{T+1} \\mid x_{\\le T}) = \\text{softmax}(\\mathbf{z}_T) $$ A token is selected using greedy decoding or sampling and appended to the sequence.","title":"2.3 Computing logits and selecting the next token"},{"location":"speed/speculative_decoding/#24-autoregressive-decoding-loop","text":"For each generated token: Run the Transformer forward pass Take logits from the last token position Apply softmax over the vocabulary Select one token Append it to the sequence Repeat This process continues until an end-of-sequence token is produced or a maximum length is reached.","title":"2.4 Autoregressive decoding loop"},{"location":"speed/speculative_decoding/#25-key-limitation-of-standard-decoding","text":"Each generated token requires a new forward pass of the model Latency grows linearly with output l","title":"2.5 Key limitation of standard decoding"},{"location":"speed/speculative_decoding/#3-step-by-step-algorithm-for-speculative-decoding","text":"This section describes the speculative decoding algorithm precisely, step by step, focusing on what each model does and why it is needed. Assume: Prompt tokens: $x$ Draft model: $q$ Target model: $p$ Draft length: $k$ Drafted tokens: $y_1, y_2, \\dots, y_k$","title":"\ud83d\udce63. Step-by-Step Algorithm for Speculative Decoding"},{"location":"speed/speculative_decoding/#step-1-draft-model-proposes-tokens","text":"The draft model generates tokens autoregressively, starting from the prompt: $$ y_i \\sim q(\\cdot \\mid x, y_{<i}) \\quad \\text{for } i = 1 \\dots k $$ Key points: This step is fast because the draft model is small Tokens are sampled, not greedily selected The draft model also records the probability of each sampled token","title":"Step 1: Draft model proposes tokens"},{"location":"speed/speculative_decoding/#step-2-target-model-verifies-the-draft","text":"The target model is run once on the combined sequence: $[x, y1, y2, ..., yk]$ This produces target model probabilities: $$ p(y_i \\mid x, y_{<i}) \\quad \\text{for } i = 1 \\dots k $$ Important clarification: The target model naturally computes logits for all positions Only logits corresponding to the drafted tokens are used Logits for the prompt tokens are ignored","title":"Step 2: Target model verifies the draft"},{"location":"speed/speculative_decoding/#step-3-acceptance-test","text":"Each drafted token is accepted independently using: $$ \\alpha_i = \\min\\left(1, \\frac{p(y_i \\mid x, y_{<i})}{q(y_i \\mid x, y_{<i})}\\right) $$ Procedure: Sample $u \\sim \\text{Uniform}(0, 1)$ Accept token $y_i$ if $u \\le \\alpha_i$ Stop at the first rejected token","title":"Step 3: Acceptance test"},{"location":"speed/speculative_decoding/#step-4-rejection-handling-and-fallback","text":"If token $y_j$ is rejected: Tokens $y_j, y_{j+1}, \\dots, y_k$ are discarded The next token is sampled directly from the target model: $$ x_{\\text{next}} \\sim p(\\cdot \\mid x, y_{<j}) $$ Speculative decoding restarts from the new prefix If no token is rejected, all $k$ draft tokens are accepted.","title":"Step 4: Rejection handling and fallback"},{"location":"speed/speculative_decoding/#why-this-preserves-correctness","text":"The acceptance rule implements rejection sampling Bias introduced by the draft model is corrected The final output distribution exactly matches the target model This guarantees that speculative decoding is statistically equivalent to standard decoding with the target model.","title":"Why this preserves correctness"},{"location":"speed/speculative_decoding/#4-why-speculative-decoding-is-faster","text":"Speculative decoding reduces inference latency by decreasing how often the expensive target model must be executed.","title":"\ud83d\udce64. Why Speculative Decoding Is Faster"},{"location":"speed/speculative_decoding/#41-standard-decoding-vs-speculative-decoding","text":"Standard decoding One target model forward pass per generated token For $N$ tokens, $N$ forward passes are required Speculative decoding The draft model proposes $k$ tokens cheaply A single target model forward pass verifies up to $k$ tokens Multiple tokens can be generated per target model invocation","title":"4.1 Standard decoding vs speculative decoding"},{"location":"speed/speculative_decoding/#42-source-of-the-speedup","text":"The speedup comes from two properties: The draft model is significantly cheaper than the target model The target model can evaluate multiple draft tokens in parallel If the acceptance rate is high, the target model is called much less frequently.","title":"4.2 Source of the speedup"},{"location":"speed/speculative_decoding/#43-what-speculative-decoding-does-not-optimize","text":"Speculative decoding does not reduce: The total number of FLOPs in the target model The per-token computation inside the Transformer It primarily reduces latency , not theoretical compute.","title":"4.3 What speculative decoding does not optimize"},{"location":"speed/speculative_decoding/#44-practical-speedups","text":"In practice, speculative decoding often achieves: 1.5x to 3x latency improvement Higher gains when draft and target distributions are close Actual speedup depends on model sizes, hardware, and acceptance rate.","title":"4.4 Practical speedups"},{"location":"speed/speculative_decoding/#5-relationship-to-logits-for-all-tokens","text":"Speculative decoding does not introduce a new requirement to compute logits for all tokens.","title":"5. Relationship to Logits for All Tokens"},{"location":"speed/speculative_decoding/#51-standard-transformer-behavior","text":"A Transformer forward pass naturally produces: One hidden state per token position One vocabulary logits vector per token position This is true during both training and inference.","title":"5.1 Standard Transformer behavior"},{"location":"speed/speculative_decoding/#52-how-logits-are-used-in-speculative-decoding","text":"During verification: The target model is run once on the prompt plus draft tokens Logits for prompt tokens are ignored Only logits corresponding to the draft token positions are used Speculative decoding simply reuses standard per-position logits.","title":"5.2 How logits are used in speculative decoding"},{"location":"speed/speculative_decoding/#53-what-speculative-decoding-does-not-do","text":"Speculative decoding does not: Perform softmax over token positions Predict future tokens independently Generate tokens in parallel from the target model The target model still defines an autoregressive distribution.","title":"5.3 What speculative decoding does not do"},{"location":"speed/speculative_decoding/#6-interaction-with-kv-cache","text":"KV cache improves performance in speculative decoding but does not change the algorithm.","title":"6. Interaction with KV Cache"},{"location":"speed/speculative_decoding/#61-kv-cache-in-the-draft-model","text":"The draft model maintains its own KV cache Draft tokens are generated autoregressively KV cache allows fast token proposal","title":"6.1 KV cache in the draft model"},{"location":"speed/speculative_decoding/#62-kv-cache-in-the-target-model","text":"The target model computes KV cache for the entire speculative window KV states corresponding to accepted tokens are reused KV states for rejected tokens are discarded","title":"6.2 KV cache in the target model"},{"location":"speed/speculative_decoding/#63-why-kv-cache-matters","text":"KV cache: Avoids recomputing attention for previously processed tokens Reduces per-step computation Improves practical throughput KV cache affects efficiency only, not correctness.","title":"6.3 Why KV cache matters"},{"location":"speed/vllm/","text":"1. Overview vLLM is an open-source inference engine optimized for high-throughput, low-latency serving of large language models. Its core innovation, PagedAttention , rethinks KV cache management by applying principles similar to virtual memory systems in operating systems. The key contribution of vLLM is not faster kernels alone, but dramatically improved GPU memory utilization , enabling higher batch sizes, better multi-tenancy, and more predictable latency under load. 2. The KV Cache Bottleneck in LLM Inference 2.1 Why the KV Cache Dominates During autoregressive decoding, each generated token appends Key and Value tensors for every transformer layer. Over long sequences or many concurrent requests, the KV cache quickly becomes the dominant consumer of GPU memory. Key properties: - Grows linearly with sequence length - Must be retained across decoding steps - Is memory-bandwidth bound during decode 3. The Core Technology: PagedAttention The KV cache (the memory storing previous tokens to predict the next one) is the primary bottleneck in scaling LLMs. The Problem: Fragmentation Standard inference engines allocate a contiguous \"max-length\" chunk of memory for every request. Over-reservation: Reserving 2048 tokens for a request that only generates 50. Internal Fragmentation: Memory wasted inside the reserved block. Waste: Up to 60-80% of VRAM is often left unused but \"reserved.\" The Solution: PagedAttention vLLM breaks the KV cache into fixed-size blocks (pages). Logical Blocks: Sequential tokens in the prompt. Physical Blocks: Non-contiguous memory addresses on the GPU. Block Table: A mapping system that allows the model to access these blocks as if they were one continuous string. Result: Waste is reduced to <4% , allowing for significantly larger batch sizes. 4. Scheduling: Continuous Batching Traditional engines use \"Static Batching,\" where the entire batch must finish before new requests start. Iteration-Level Scheduling: vLLM schedules at the level of individual iterations. Mechanism: As soon as one sequence in a batch hits an <EOS> (End of Sentence) token, a new request from the queue is inserted into its spot in the next iteration. Outcome: Eliminates \"bubbles\" (idle time) in GPU utilization. 5. Modern (2025-2026) Advanced Features A. Speculative Decoding vLLM implements speculative decoding where a smaller draft model (e.g., a 100M parameter model) predicts several tokens, and a larger target model (e.g., Llama 3 70B) verifies them in a single pass. * Benefit: Reduces latency by 2-3x for heavy models. B. Automatic Prefix Caching (APC) For RAG or multi-turn chat, vLLM caches the KV blocks of common prefixes (like system prompts). * If two users share the same 1,000-token system prompt, vLLM stores it once in physical memory, and both requests point to the same blocks. C. Multi-LoRA Support vLLM can serve one base model with hundreds of different fine-tuned \"adapters\" (LoRAs) simultaneously. * It uses specialized SGMV (Shrink-Generalized Matrix-Vector) kernels to compute multiple different LoRAs in a single batch without a significant performance hit. 6. Prefill vs Decode: Performance Characteristics Phase Characteristics Bottleneck Prefill Parallel over tokens Compute-bound Decode Sequential token-by-token Memory-bandwidth-bound vLLM introduces Chunked Prefill to prevent long prompts from blocking decode for other users. 8. Memory Pressure and Preemption When GPU memory becomes constrained, vLLM supports preemption strategies: Swap: Move KV blocks to CPU memory Recompute: Drop KV blocks and recompute them later Recompute trades extra compute for lower memory pressure and is often preferred on fast GPUs. 9. Architectural Comparison Feature vLLM Hugging Face TGI NVIDIA TensorRT-LLM Memory Mgmt PagedAttention FlashAttention Paged KV Cache Ease of Use High (Pythonic) Medium (Rust/Go) Low (Complex Build) Best For General Throughput Stability/HF ecosystem Peak NVIDIA Perf 10. Useful Memory Approximation Approximate KV cache memory usage in bytes: $$ \\text{KV Memory} \\;\\approx\\; 2 \\times L \\times T \\times H \\times D_h \\times B $$ Where: - 2 accounts for Keys and Values - $L$ is the number of layers - $H$ is the number of attention heads - $B$ is the number of bytes per element (2 for FP16, 1 for FP8) - $D_h$ is the per-head hidden dimension - $T$ is the sequence length (number of cached tokens) Example: For Llama-3 8B : $L = 32$ $D_{\\text{model}} = 4096$ $B = 2$ (FP16) Per token KV cache memory: $$ 2 \\times 32 \\times 4096 \\times 2 \\approx 524{,}288 \\text{ bytes} \\approx 0.5 \\text{ MB} $$ Approximate totals: - 2k tokens \u2192 ~1 GB KV cache - 8k tokens \u2192 ~4 GB KV cache This linear scaling with sequence length explains why KV cache memory becomes the dominant bottleneck and motivates techniques such as PagedAttention in vLLM. Q & As Q1: How does vLLM handle a situation where the GPU runs out of memory during a request? * A: It uses Preemption . It can either \"Swap\" (move blocks to CPU RAM) or \"Recompute\" (drop the blocks and re-calculate them later when memory is free). Q2: What is the difference between the 'Prefill' and 'Decode' phases? * A: Prefill processes the input prompt (parallel/compute-bound). Decode generates tokens one by one (sequential/memory-bound). vLLM uses Chunked Prefill to prevent large prompts from stalling the generation of other users. Q3: Why is vLLM better for multi-tenant SaaS? * A: Because of PagedAttention and Multi-LoRA support. It allows hosting many different \"specialized\" models on a single GPU cluster with minimal overhead.","title":"Vllm"},{"location":"speed/vllm/#1-overview","text":"vLLM is an open-source inference engine optimized for high-throughput, low-latency serving of large language models. Its core innovation, PagedAttention , rethinks KV cache management by applying principles similar to virtual memory systems in operating systems. The key contribution of vLLM is not faster kernels alone, but dramatically improved GPU memory utilization , enabling higher batch sizes, better multi-tenancy, and more predictable latency under load.","title":"1. Overview"},{"location":"speed/vllm/#2-the-kv-cache-bottleneck-in-llm-inference","text":"","title":"2. The KV Cache Bottleneck in LLM Inference"},{"location":"speed/vllm/#21-why-the-kv-cache-dominates","text":"During autoregressive decoding, each generated token appends Key and Value tensors for every transformer layer. Over long sequences or many concurrent requests, the KV cache quickly becomes the dominant consumer of GPU memory. Key properties: - Grows linearly with sequence length - Must be retained across decoding steps - Is memory-bandwidth bound during decode","title":"2.1 Why the KV Cache Dominates"},{"location":"speed/vllm/#3-the-core-technology-pagedattention","text":"The KV cache (the memory storing previous tokens to predict the next one) is the primary bottleneck in scaling LLMs.","title":"3. The Core Technology: PagedAttention"},{"location":"speed/vllm/#the-problem-fragmentation","text":"Standard inference engines allocate a contiguous \"max-length\" chunk of memory for every request. Over-reservation: Reserving 2048 tokens for a request that only generates 50. Internal Fragmentation: Memory wasted inside the reserved block. Waste: Up to 60-80% of VRAM is often left unused but \"reserved.\"","title":"The Problem: Fragmentation"},{"location":"speed/vllm/#the-solution-pagedattention","text":"vLLM breaks the KV cache into fixed-size blocks (pages). Logical Blocks: Sequential tokens in the prompt. Physical Blocks: Non-contiguous memory addresses on the GPU. Block Table: A mapping system that allows the model to access these blocks as if they were one continuous string. Result: Waste is reduced to <4% , allowing for significantly larger batch sizes.","title":"The Solution: PagedAttention"},{"location":"speed/vllm/#4-scheduling-continuous-batching","text":"Traditional engines use \"Static Batching,\" where the entire batch must finish before new requests start. Iteration-Level Scheduling: vLLM schedules at the level of individual iterations. Mechanism: As soon as one sequence in a batch hits an <EOS> (End of Sentence) token, a new request from the queue is inserted into its spot in the next iteration. Outcome: Eliminates \"bubbles\" (idle time) in GPU utilization.","title":"4. Scheduling: Continuous Batching"},{"location":"speed/vllm/#5-modern-2025-2026-advanced-features","text":"","title":"5. Modern (2025-2026) Advanced Features"},{"location":"speed/vllm/#a-speculative-decoding","text":"vLLM implements speculative decoding where a smaller draft model (e.g., a 100M parameter model) predicts several tokens, and a larger target model (e.g., Llama 3 70B) verifies them in a single pass. * Benefit: Reduces latency by 2-3x for heavy models.","title":"A. Speculative Decoding"},{"location":"speed/vllm/#b-automatic-prefix-caching-apc","text":"For RAG or multi-turn chat, vLLM caches the KV blocks of common prefixes (like system prompts). * If two users share the same 1,000-token system prompt, vLLM stores it once in physical memory, and both requests point to the same blocks.","title":"B. Automatic Prefix Caching (APC)"},{"location":"speed/vllm/#c-multi-lora-support","text":"vLLM can serve one base model with hundreds of different fine-tuned \"adapters\" (LoRAs) simultaneously. * It uses specialized SGMV (Shrink-Generalized Matrix-Vector) kernels to compute multiple different LoRAs in a single batch without a significant performance hit.","title":"C. Multi-LoRA Support"},{"location":"speed/vllm/#6-prefill-vs-decode-performance-characteristics","text":"Phase Characteristics Bottleneck Prefill Parallel over tokens Compute-bound Decode Sequential token-by-token Memory-bandwidth-bound vLLM introduces Chunked Prefill to prevent long prompts from blocking decode for other users.","title":"6. Prefill vs Decode: Performance Characteristics"},{"location":"speed/vllm/#8-memory-pressure-and-preemption","text":"When GPU memory becomes constrained, vLLM supports preemption strategies: Swap: Move KV blocks to CPU memory Recompute: Drop KV blocks and recompute them later Recompute trades extra compute for lower memory pressure and is often preferred on fast GPUs.","title":"8. Memory Pressure and Preemption"},{"location":"speed/vllm/#9-architectural-comparison","text":"Feature vLLM Hugging Face TGI NVIDIA TensorRT-LLM Memory Mgmt PagedAttention FlashAttention Paged KV Cache Ease of Use High (Pythonic) Medium (Rust/Go) Low (Complex Build) Best For General Throughput Stability/HF ecosystem Peak NVIDIA Perf","title":"9. Architectural Comparison"},{"location":"speed/vllm/#10-useful-memory-approximation","text":"Approximate KV cache memory usage in bytes: $$ \\text{KV Memory} \\;\\approx\\; 2 \\times L \\times T \\times H \\times D_h \\times B $$ Where: - 2 accounts for Keys and Values - $L$ is the number of layers - $H$ is the number of attention heads - $B$ is the number of bytes per element (2 for FP16, 1 for FP8) - $D_h$ is the per-head hidden dimension - $T$ is the sequence length (number of cached tokens) Example: For Llama-3 8B : $L = 32$ $D_{\\text{model}} = 4096$ $B = 2$ (FP16) Per token KV cache memory: $$ 2 \\times 32 \\times 4096 \\times 2 \\approx 524{,}288 \\text{ bytes} \\approx 0.5 \\text{ MB} $$ Approximate totals: - 2k tokens \u2192 ~1 GB KV cache - 8k tokens \u2192 ~4 GB KV cache This linear scaling with sequence length explains why KV cache memory becomes the dominant bottleneck and motivates techniques such as PagedAttention in vLLM.","title":"10. Useful Memory Approximation"},{"location":"speed/vllm/#q-as","text":"Q1: How does vLLM handle a situation where the GPU runs out of memory during a request? * A: It uses Preemption . It can either \"Swap\" (move blocks to CPU RAM) or \"Recompute\" (drop the blocks and re-calculate them later when memory is free). Q2: What is the difference between the 'Prefill' and 'Decode' phases? * A: Prefill processes the input prompt (parallel/compute-bound). Decode generates tokens one by one (sequential/memory-bound). vLLM uses Chunked Prefill to prevent large prompts from stalling the generation of other users. Q3: Why is vLLM better for multi-tenant SaaS? * A: Because of PagedAttention and Multi-LoRA support. It allows hosting many different \"specialized\" models on a single GPU cluster with minimal overhead.","title":"Q &amp; As"}]}