{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"batching_strategies/","title":"Batching strategies","text":"<p>static-batching.md \u2502   \u251c\u2500\u2500 dynamic-batching.md \u2502   \u251c\u2500\u2500 continuous-batching.md \u2502   \u251c\u2500\u2500 in-flight-batching.md \u2502   \u2514\u2500\u2500 prefix-caching.md</p>"},{"location":"attention_optimization/flash_attention/","title":"Flash Attention","text":""},{"location":"attention_optimization/flash_attention/#1-overview","title":"1. Overview","text":"<p>FlashAttention is a fast and memory-efficient attention algorithm that computes exact attention without materializing the full \\(N \\times N\\) attention matrix. It's especially critical for long sequences (4k+ tokens) in modern LLMs.</p> <p>Key insight: The bottleneck in attention isn't compute\u2014it's memory bandwidth (moving data between GPU memory hierarchies).</p>"},{"location":"attention_optimization/flash_attention/#2-why-standard-attention-is-slow","title":"2. Why Standard Attention is Slow","text":"<p>Standard attention formula: $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$</p>"},{"location":"attention_optimization/flash_attention/#problem-1-quadratic-memory-growth","title":"Problem 1: Quadratic Memory Growth","text":"<p>For sequence length \\(N = 16{,}384\\) in FP16: - Attention matrix: \\(N^2 = 268M\\) elements - Memory: \\(268M \\times 2\\) bytes \\(\u2248 512\\) MB (per layer, per head!)</p>"},{"location":"attention_optimization/flash_attention/#problem-2-excessive-memory-traffic","title":"Problem 2: Excessive Memory Traffic","text":"<p>Standard attention performs multiple memory-heavy steps: 1. Compute \\(QK^T\\) \u2192 write to global memory 2. Read \\(QK^T\\) \u2192 apply softmax \u2192 write back 3. Read softmax output \u2192 compute with \\(V\\) \u2192 write output</p> <p>Result: GPUs become memory-bound, not compute-bound.</p>"},{"location":"attention_optimization/flash_attention/#problem-3-numerical-instability-in-fp16","title":"Problem 3: Numerical Instability in FP16","text":"<ul> <li>Large values in \\(QK^T\\) cause overflow in \\(e^x\\)</li> <li>Small values underflow to zero</li> <li>Standard attention often requires FP32, increasing memory usage</li> </ul>"},{"location":"attention_optimization/flash_attention/#3-how-flashattention-works","title":"3. How FlashAttention Works","text":"<p>FlashAttention uses three key techniques:</p>"},{"location":"attention_optimization/flash_attention/#31-tiling","title":"3.1 Tiling","text":"<p>Split Q, K, V into small tiles that fit in GPU shared memory (SRAM).</p> <p>Example: - Sequence length: \\(N = 16{,}384\\) - Tile size: \\(B = 128\\) - Memory per tile: \\(128 \\times 128 = 16{,}384\\) elements (vs. \\(268M\\) for full matrix)</p> <pre><code># Conceptual tiling\nfor q_tile in Q_tiles:\n    for k_tile, v_tile in zip(K_tiles, V_tiles):\n        partial_scores = q_tile @ k_tile.T\n        # accumulate incrementally\n</code></pre>"},{"location":"attention_optimization/flash_attention/#32-kernel-fusion","title":"3.2 Kernel Fusion","text":"<p>Fuse all operations into a single kernel to keep intermediate results in fast shared memory: 1. Matrix multiplication (\\(Q \\cdot K^T\\)) 2. Scaling (\\(1/\\sqrt{d}\\)) 3. Softmax 4. Weighted sum with \\(V\\)</p> <p>Standard attention writes/reads from global memory between each step. FlashAttention does everything in one pass.</p>"},{"location":"attention_optimization/flash_attention/#33-online-softmax","title":"3.3 Online Softmax","text":"<p>Compute softmax incrementally across tiles without storing the full attention matrix.</p> <p>Numerically stable approach: 1. Maintain running maximum \\(m\\) across tiles    - Compute: \\(e^{x_i - m}\\) (prevents overflow) 2. Maintain running sum of exponentials 3. Accumulate weighted output incrementally</p> <p>Example with 2 tiles:</p> <p>Tile 1: <code>[0.1, 0.5, 0.3]</code>, Tile 2: <code>[0.2, 0.4, 0.1]</code></p> <p>Processing: 1. Tile 1: \\(m = 0.5\\), shifted exps: \\([e^{-0.4}, e^{0}, e^{-0.2}]\\), running sum \\(s_1\\) 2. Tile 2: update \\(m\\), reweight previous results, add new exps, update sum \\(s_2\\) 3. Final: divide accumulated output by \\(s_2\\)</p> <p>Result: Exact same output as standard attention, but in FP16/BF16 without overflow.</p>"},{"location":"attention_optimization/flash_attention/#4-performance-impact","title":"4. Performance Impact","text":""},{"location":"attention_optimization/flash_attention/#memory-complexity","title":"Memory Complexity","text":"<ul> <li>Standard: \\(O(N^2)\\)</li> <li>FlashAttention: \\(O(N \\cdot B)\\) where \\(B\\) is tile size</li> </ul>"},{"location":"attention_optimization/flash_attention/#speedup","title":"Speedup","text":"<ul> <li>2\u20134x faster for long sequences on modern GPUs</li> <li>Enables 2\u20134x longer sequences or larger batch sizes</li> </ul>"},{"location":"attention_optimization/flash_attention/#usage","title":"Usage","text":"<pre><code>from flash_attn import flash_attn_func\n\n# q, k, v shape: (batch, seq_len, num_heads, head_dim)\noutput = flash_attn_func(q, k, v, dropout_p=0.0, causal=False)\n</code></pre>"},{"location":"attention_optimization/flash_attention/#when-flashattention-helps-most","title":"When FlashAttention Helps Most","text":"<p>\u2705 Long sequences (2k+ tokens) \u2705 FP16/BF16 precision \u2705 Modern NVIDIA GPUs with fast shared memory  </p> <p>\u274c Very short sequences \u274c CPU-based inference \u274c Custom attention patterns not supported by the kernels  </p>"},{"location":"attention_optimization/flash_attention/#5-interview-questions","title":"5. Interview Questions","text":""},{"location":"attention_optimization/flash_attention/#q1-why-is-flashattention-faster-than-standard-attention","title":"Q1: Why is FlashAttention faster than standard attention?","text":"<p>Answer: The bottleneck is memory bandwidth, not compute. Standard attention writes intermediate results (attention matrix, softmax output) to slow GPU global memory and reads them back multiple times. FlashAttention uses tiling and kernel fusion to keep all intermediate computations in fast shared memory, drastically reducing memory traffic.</p>"},{"location":"attention_optimization/flash_attention/#q2-does-flashattention-approximate-attention","title":"Q2: Does FlashAttention approximate attention?","text":"<p>Answer: No, it computes exact attention. It produces identical results to standard attention by using online softmax to correctly compute the softmax normalization across tiles without storing the full attention matrix.</p>"},{"location":"attention_optimization/flash_attention/#q3-explain-online-softmax-why-is-it-needed","title":"Q3: Explain online softmax. Why is it needed?","text":"<p>Answer: When processing tiles, we can't store the full \\(N \\times N\\) attention matrix. Online softmax maintains a running maximum and running sum across tiles to compute the exact softmax incrementally. This also provides numerical stability in FP16/BF16 by shifting scores before exponentiation to prevent overflow: \\(e^{x_i - m}\\) instead of \\(e^{x_i}\\).</p>"},{"location":"attention_optimization/flash_attention/#q4-what-is-the-memory-complexity-of-flashattention-vs-standard-attention","title":"Q4: What is the memory complexity of FlashAttention vs standard attention?","text":"<p>Answer: - Standard attention: \\(O(N^2)\\) to store full attention matrix - FlashAttention: \\(O(N \\cdot B)\\) where \\(B\\) is tile size (typically 128-256) - For \\(N=16k\\), this reduces memory from ~512MB to ~4-8MB per head</p>"},{"location":"attention_optimization/flash_attention/#q5-can-flashattention-be-used-for-any-attention-mechanism","title":"Q5: Can FlashAttention be used for any attention mechanism?","text":"<p>Answer: FlashAttention works best for standard scaled dot-product attention. Variants exist for: - Causal (autoregressive) attention \u2705 - Cross-attention \u2705 - Sparse attention patterns \u26a0\ufe0f (limited support, depends on sparsity structure)</p> <p>Custom attention patterns may require specialized kernels.</p>"},{"location":"attention_optimization/flash_attention/#q6-why-does-flashattention-require-modern-gpus","title":"Q6: Why does FlashAttention require modern GPUs?","text":"<p>Answer: FlashAttention relies on: 1. Fast shared memory (SRAM) - to store tiles and perform fused operations 2. High memory bandwidth - to maximize benefit from reduced memory traffic 3. Tensor cores - for fast matrix multiplications</p> <p>Older GPUs or CPUs don't have the same memory hierarchy, so the benefits are minimal.</p>"},{"location":"attention_optimization/flash_attention/#q7-walk-through-how-flashattention-processes-a-single-tile-pair","title":"Q7: Walk through how FlashAttention processes a single tile pair.","text":"<p>Answer: 1. Load \\(Q_{tile}\\) and \\(K_{tile}\\) into shared memory 2. Compute scores: \\(S = Q_{tile} \\cdot K_{tile}^T / \\sqrt{d}\\) 3. Track running max \\(m\\) for numerical stability 4. Compute: \\(e^{S - m}\\) (stays in shared memory) 5. Update running sum for normalization 6. Load \\(V_{tile}\\), compute weighted sum, accumulate to output 7. Move to next tile, repeat</p> <p>All intermediate values stay in fast SRAM, not global memory.</p>"},{"location":"attention_optimization/flash_attention/#q8-what-trade-offs-does-flashattention-make","title":"Q8: What trade-offs does FlashAttention make?","text":"<p>Answer: - \u2705 Gains: 2-4x speedup, drastically reduced memory - \u26a0\ufe0f Complexity: More complex implementation than standard attention - \u26a0\ufe0f Flexibility: Limited support for custom sparse attention patterns - \u26a0\ufe0f Hardware: Requires modern GPUs to realize full benefits</p> <p>The trade-offs are generally worth it for production LLM serving and training.</p>"},{"location":"attention_optimization/flash_attention/#q9-how-does-tiling-affect-the-computational-complexity","title":"Q9: How does tiling affect the computational complexity?","text":"<p>Answer: Tiling doesn't change the computational complexity (still \\(O(N^2)\\) FLOPs), but it changes the I/O complexity: - Standard: \\(O(N^2)\\) memory reads/writes - FlashAttention: \\(O(N^2/B)\\) memory reads/writes, where \\(B\\) is tile size</p> <p>Since memory bandwidth is the bottleneck, this provides significant speedup.</p>"},{"location":"attention_optimization/flash_attention/#q10-can-you-explain-the-difference-between-shared-memory-and-global-memory","title":"Q10: Can you explain the difference between shared memory and global memory?","text":"<p>Answer: - Shared memory (SRAM): Fast (~20 TB/s), small (~100 KB per SM), explicitly managed - Global memory (HBM): Slower (~1-2 TB/s), large (16-80 GB), high latency</p> <p>FlashAttention keeps working data in shared memory to minimize expensive global memory accesses. This is the key to its performance gains.</p>"},{"location":"attention_optimization/flash_attention/#6-key-takeaways-for-interviews","title":"6. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Avoid materializing the \\(N \\times N\\) attention matrix by using tiling and kernel fusion</li> <li>Core techniques: Tiling + Kernel Fusion + Online Softmax</li> <li>Why it's fast: Reduces memory bandwidth usage (the real bottleneck)</li> <li>Memory savings: \\(O(N^2) \u2192 O(N \\cdot B)\\)</li> <li>Exact computation: Not an approximation\u2014produces identical results to standard attention</li> <li>Numerical stability: Online softmax enables stable FP16/BF16 computation for long sequences</li> </ol>"},{"location":"attention_optimization/flash_attention_2/","title":"Flash attention 2","text":""},{"location":"attention_optimization/flash_attention_2/#1-overview","title":"1. Overview","text":"<p>FlashAttention-2 is an improved version of FlashAttention that achieves 2x speedup over FlashAttention-1 through better GPU utilization. It maintains the same exact attention computation while being even faster and more efficient.</p> <p>Key improvement: Better work partitioning across GPU threads to reduce idle time and maximize hardware utilization.</p>"},{"location":"attention_optimization/flash_attention_2/#2-what-was-wrong-with-flashattention-1","title":"2. What Was Wrong with FlashAttention-1?","text":"<p>Despite being much faster than standard attention, FlashAttention-1 had suboptimal GPU utilization:</p>"},{"location":"attention_optimization/flash_attention_2/#problem-1-poor-work-partitioning","title":"Problem 1: Poor Work Partitioning","text":"<ul> <li>Each thread block processed one query tile across all key/value tiles</li> <li>Led to unbalanced workload and thread block idle time</li> <li>Didn't fully saturate GPU compute resources</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#problem-2-non-coalesced-memory-accesses","title":"Problem 2: Non-Coalesced Memory Accesses","text":"<ul> <li>Memory accesses weren't optimally aligned for GPU memory coalescing</li> <li>Caused unnecessary memory bandwidth waste</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#problem-3-limited-parallelism","title":"Problem 3: Limited Parallelism","text":"<ul> <li>Parallelism was only across batch, heads, and query sequence</li> <li>Didn't parallelize across key/value sequence dimension</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#3-key-improvements-in-flashattention-2","title":"3. Key Improvements in FlashAttention-2","text":""},{"location":"attention_optimization/flash_attention_2/#31-better-parallelism-strategy","title":"3.1 Better Parallelism Strategy","text":"<p>FlashAttention-1: Parallelize over <code>(batch, heads, query_tiles)</code> <pre><code>Thread Block 1 \u2192 processes Q_tile_1 across all K,V tiles\nThread Block 2 \u2192 processes Q_tile_2 across all K,V tiles\n</code></pre></p> <p>FlashAttention-2: Parallelize over <code>(batch, heads, query_tiles, kv_tiles)</code> <pre><code>Thread Block 1 \u2192 processes (Q_tile_1, K_tile_1, V_tile_1)\nThread Block 2 \u2192 processes (Q_tile_1, K_tile_2, V_tile_2)\nThread Block 3 \u2192 processes (Q_tile_2, K_tile_1, V_tile_1)\n</code></pre></p> <p>Benefit: More thread blocks doing work simultaneously \u2192 better GPU occupancy \u2192 less idle time</p>"},{"location":"attention_optimization/flash_attention_2/#32-improved-work-partitioning-within-thread-blocks","title":"3.2 Improved Work Partitioning Within Thread Blocks","text":"<p>FlashAttention-1: Each warp handled different queries within a tile - Led to imbalanced work when softmax required different amounts of computation</p> <p>FlashAttention-2: Each warp handles same query, split across K dimension - More balanced work distribution - Better load balancing across warps</p>"},{"location":"attention_optimization/flash_attention_2/#33-memory-access-optimizations","title":"3.3 Memory Access Optimizations","text":"<ul> <li>Improved memory coalescing patterns</li> <li>Better cache utilization</li> <li>Reduced redundant memory loads</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#4-performance-impact","title":"4. Performance Impact","text":""},{"location":"attention_optimization/flash_attention_2/#speedup-over-flashattention-1","title":"Speedup Over FlashAttention-1","text":"<ul> <li>~2x faster on average for typical sequence lengths</li> <li>Up to 2.3x on A100 GPUs for long sequences</li> <li>Better scaling with sequence length</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#gpu-utilization","title":"GPU Utilization","text":"<ul> <li>FlashAttention-1: ~35-50% of peak FLOPS</li> <li>FlashAttention-2: ~50-70% of peak FLOPS</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Same \\(O(N \\cdot B)\\) memory complexity</li> <li>Better bandwidth utilization due to improved access patterns</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#5-implementation-details","title":"5. Implementation Details","text":""},{"location":"attention_optimization/flash_attention_2/#thread-block-structure","title":"Thread Block Structure","text":"<pre><code># Conceptual partitioning\nfor batch_idx in batches:\n    for head_idx in heads:\n        for q_tile_idx in query_tiles:\n            for kv_tile_idx in kv_tiles:  # NEW: also parallelize here\n                # Each (q_tile, kv_tile) pair gets its own thread block\n                thread_block.process(Q[q_tile_idx], K[kv_tile_idx], V[kv_tile_idx])\n                # Accumulate partial results\n</code></pre>"},{"location":"attention_optimization/flash_attention_2/#synchronization","title":"Synchronization","text":"<ul> <li>Requires careful synchronization when accumulating partial outputs</li> <li>Uses atomic operations or reduction trees to combine results from different KV tiles</li> </ul>"},{"location":"attention_optimization/flash_attention_2/#6-interview-questions","title":"6. Interview Questions","text":""},{"location":"attention_optimization/flash_attention_2/#q1-whats-the-main-difference-between-flashattention-1-and-flashattention-2","title":"Q1: What's the main difference between FlashAttention-1 and FlashAttention-2?","text":"<p>Answer: FlashAttention-2 improves parallelism by also parallelizing across the key/value sequence dimension, not just query sequence. This means more thread blocks work simultaneously, reducing idle time and achieving ~2x speedup while computing the exact same result.</p>"},{"location":"attention_optimization/flash_attention_2/#q2-does-flashattention-2-change-the-algorithm-or-just-the-implementation","title":"Q2: Does FlashAttention-2 change the algorithm or just the implementation?","text":"<p>Answer: It's purely an implementation improvement. The algorithm (tiling, online softmax, kernel fusion) remains the same. FlashAttention-2 just distributes work more efficiently across GPU threads to maximize hardware utilization.</p>"},{"location":"attention_optimization/flash_attention_2/#q3-why-does-parallelizing-across-kv-tiles-improve-performance","title":"Q3: Why does parallelizing across KV tiles improve performance?","text":"<p>Answer: In FlashAttention-1, each thread block processes one Q tile sequentially across all KV tiles. This limits parallelism. FlashAttention-2 launches separate thread blocks for each (Q_tile, KV_tile) pair, enabling many more blocks to run concurrently, better saturating the GPU's compute resources.</p>"},{"location":"attention_optimization/flash_attention_2/#q4-whats-the-trade-off-with-this-increased-parallelism","title":"Q4: What's the trade-off with this increased parallelism?","text":"<p>Answer: More synchronization overhead. Since multiple thread blocks now compute partial outputs for the same query tile (from different KV tiles), we need to carefully accumulate and normalize these partial results. However, the performance gain from parallelism far outweighs this cost.</p>"},{"location":"attention_optimization/flash_attention_2/#q5-how-does-flashattention-2-handle-the-accumulation-of-partial-results","title":"Q5: How does FlashAttention-2 handle the accumulation of partial results?","text":"<p>Answer: Each thread block computes a partial attention output for its (Q_tile, KV_tile) pair along with partial softmax statistics (max, sum). These partials are then combined using: - Atomic operations, or - Reduction trees, or - Final pass to accumulate stored partials</p> <p>The online softmax technique ensures correct normalization.</p>"},{"location":"attention_optimization/flash_attention_2/#q6-what-gpu-features-does-flashattention-2-rely-on-more-heavily","title":"Q6: What GPU features does FlashAttention-2 rely on more heavily?","text":"<p>Answer: - High thread block occupancy - needs many concurrent blocks - Fast atomic operations - for accumulating partials - Shared memory bandwidth - still crucial like FA-1 - Warp-level primitives - for efficient intra-block communication</p> <p>Modern GPUs (A100, H100) have better support for these, maximizing FA-2's benefits.</p>"},{"location":"attention_optimization/flash_attention_2/#q7-why-doesnt-flashattention-2-achieve-100-of-peak-flops","title":"Q7: Why doesn't FlashAttention-2 achieve 100% of peak FLOPS?","text":"<p>Answer: Several factors: - Memory bandwidth still matters (can't fully hide all memory latency) - Synchronization overhead from accumulating partials - Load imbalance across thread blocks (some finish before others) - Non-uniform work per tile (softmax computation varies)</p> <p>50-70% utilization is actually quite good for memory-intensive operations.</p>"},{"location":"attention_optimization/flash_attention_2/#q8-how-does-sequence-length-affect-fa-2s-speedup-over-fa-1","title":"Q8: How does sequence length affect FA-2's speedup over FA-1?","text":"<p>Answer: FlashAttention-2's advantage increases with sequence length: - Longer sequences \u2192 more tiles \u2192 more parallelism opportunities - Better amortization of synchronization overhead - At very short sequences, FA-1 and FA-2 are similar (not enough parallelism to exploit)</p>"},{"location":"attention_optimization/flash_attention_2/#q9-can-flashattention-2s-techniques-be-applied-to-other-operations","title":"Q9: Can FlashAttention-2's techniques be applied to other operations?","text":"<p>Answer: Yes! The key insight\u2014parallelizing over both input and computation dimensions\u2014applies to any operation that processes tiles/blocks: - Other attention variants (sparse, local attention) - Convolutions with tiling - Matrix multiplications with block processing</p> <p>The principle is: maximize parallel work to reduce GPU idle time.</p>"},{"location":"attention_optimization/flash_attention_2/#q10-whats-the-memory-complexity-of-flashattention-2","title":"Q10: What's the memory complexity of FlashAttention-2?","text":"<p>Answer: Same as FlashAttention-1: \\(O(N \\cdot B)\\) where \\(B\\) is tile size. The improvement is in speed (better parallelism and memory access patterns), not memory usage. Both avoid materializing the full \\(N \\times N\\) attention matrix.</p>"},{"location":"attention_optimization/flash_attention_2/#7-flashattention-1-vs-flashattention-2-summary","title":"7. FlashAttention-1 vs FlashAttention-2 Summary","text":"Aspect FlashAttention-1 FlashAttention-2 Parallelism Over batch, heads, query tiles Over batch, heads, query tiles, KV tiles Work per block One Q tile \u00d7 all KV tiles One (Q tile, KV tile) pair GPU utilization 35-50% of peak FLOPS 50-70% of peak FLOPS Speedup vs standard 2-4x 4-8x Speedup vs FA-1 - ~2x Memory complexity \\(O(N \\cdot B)\\) \\(O(N \\cdot B)\\) Algorithm Tiling + online softmax + fusion Same Synchronization Simpler More complex (atomic/reduction)"},{"location":"attention_optimization/flash_attention_2/#8-key-takeaways-for-interviews","title":"8. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Same algorithm, better parallelism by also parallelizing across KV dimension</li> <li>Performance: ~2x faster than FA-1 through better GPU utilization</li> <li>Trade-off: More synchronization overhead, but worth it for the speedup</li> <li>Memory: Same \\(O(N \\cdot B)\\) complexity, just faster execution</li> <li>When it matters most: Long sequences where more parallelism can be exploited</li> <li>Hardware dependency: Benefits scale with GPU's ability to run many thread blocks concurrently</li> </ol>"},{"location":"attention_optimization/kv_caching/","title":"Kv caching","text":""},{"location":"attention_optimization/kv_caching/#1-self-attention-recap","title":"\ud83d\udce61. Self Attention Recap","text":"<p>Given hidden states \\(X \\in \\mathbb{R}^{T \\times d}\\):</p> \\[ Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V \\] <p>Per head attention:</p> \\[ \\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V \\] <p>Autoregressive decoding generates one token at a time with causal masking.</p>"},{"location":"attention_optimization/kv_caching/#2-why-kv-cache-is-needed","title":"\ud83d\udce62. Why KV Cache Is Needed","text":"<p>At decoding step \\(t\\), keys and values for tokens \\(1 \\ldots t-1\\) are unchanged but would be recomputed without caching.</p> <p>This repeated computation dominates inference latency and wastes FLOPs.</p>"},{"location":"attention_optimization/kv_caching/#3-kv-cache-mechanism","title":"\ud83d\udce63. KV Cache Mechanism","text":"<p>For each transformer layer \\(\\ell\\):</p> \\[ \\text{KVCache}_\\ell = \\{K_\\ell^{1:t}, V_\\ell^{1:t}\\} \\] <p>At decoding step \\(t\\):</p> <ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to the cache</li> <li>Attend over all cached keys and values</li> </ul> \\[ \\text{Attn}_t = \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Only the current token requires new computation.</p>"},{"location":"attention_optimization/kv_caching/#4-toy-example","title":"\ud83d\udce64. Toy Example","text":"<p>Prompt: \"I like neural\"</p> <p>Step 1: generate <code>\"networks\"</code></p> <ul> <li>Compute and cache keys and values for the prompt</li> <li>Attend to all cached tokens</li> </ul> <p>Step 2: generate <code>\"models\"</code></p> <ul> <li>Reuse cached keys and values</li> <li>Compute keys and values only for <code>\"networks\"</code></li> </ul> <p>Previously generated tokens are never recomputed.</p>"},{"location":"attention_optimization/kv_caching/#5-complexity-analysis","title":"\ud83d\udce65. Complexity Analysis","text":""},{"location":"attention_optimization/kv_caching/#notation","title":"Notation","text":"<ul> <li>\\(T\\): number of generated tokens</li> <li>\\(L\\): number of transformer layers</li> <li>\\(H\\): number of attention heads</li> <li>\\(d_h\\): head dimension</li> </ul>"},{"location":"attention_optimization/kv_caching/#without-kv-cache","title":"Without KV Cache","text":"<p>At each decoding step, attention is recomputed for all previous tokens:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^3) \\]"},{"location":"attention_optimization/kv_caching/#with-kv-cache","title":"With KV Cache","text":"<p>Only attention against cached keys and values is computed:</p> \\[ O(L \\cdot H \\cdot d_h \\cdot T^2) \\] <p>KV caching removes one full factor of \\(T\\) from decoding complexity.</p>"},{"location":"attention_optimization/kv_caching/#6-memory-cost","title":"\ud83d\udce66. Memory Cost","text":"<p>Each layer stores:</p> \\[ K, V \\in \\mathbb{R}^{H \\times T \\times d_h} \\] <p>Total KV cache memory across all layers:</p> \\[ O(L \\cdot H \\cdot T \\cdot d_h) \\] <p>For long context inference, KV cache memory is often the dominant bottleneck.</p>"},{"location":"attention_optimization/kv_caching/#7-inference-vs-training-usage","title":"\ud83d\udce67. Inference v/s Training Usage","text":""},{"location":"attention_optimization/kv_caching/#71-during-inference","title":"7.1 During Inference","text":"<p>This is the most common and important usage.</p> <p>Inference Workflow</p> <ul> <li>Encode prompt</li> <li>Initialize empty KV cache per layer</li> <li>For each generated token:<ul> <li>Compute \\(Q_t, K_t, V_t\\)</li> <li>Append \\(K_t, V_t\\) to cache</li> </ul> </li> <li>Compute attention using cached tensors</li> </ul> <p>Practical Benefits</p> <ul> <li>Faster decoding</li> <li>Lower FLOPs</li> <li>Enables long context generation</li> <li>Essential for streaming and chat systems</li> </ul>"},{"location":"attention_optimization/kv_caching/#72-during-training","title":"7.2 During Training","text":"<p>KV caching is not used in standard full sequence training.</p> <p>Why?</p> <ul> <li>Training processes full sequences in parallel</li> <li>All tokens attend to each other simultaneously</li> <li>No repeated computation across steps</li> </ul>"},{"location":"attention_optimization/kv_caching/#8-scaling-kv-cache-for-long-context","title":"\ud83d\udce68. Scaling KV Cache for Long Context","text":"<p>Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.</p>"},{"location":"attention_optimization/kv_caching/#81-sliding-window-attention","title":"8.1 Sliding Window Attention","text":"<p>Only retain keys and values for the most recent \\(W\\) tokens:</p> \\[ K_{t-W:t}, V_{t-W:t} \\] <p>This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.</p>"},{"location":"attention_optimization/kv_caching/#82-kv-cache-quantization","title":"8.2 KV Cache Quantization","text":"<p>KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.</p>"},{"location":"attention_optimization/kv_caching/#what-gets-quantized","title":"What Gets Quantized","text":"<p>Both keys and values can be quantized, but they have different sensitivity:</p> <ul> <li>Keys (K) directly affect attention scores \\(QK^T\\)</li> <li>Values (V) affect the weighted sum after softmax</li> </ul> <p>As a result:</p> <ul> <li>Keys usually require higher precision</li> <li>Values tolerate more aggressive quantization</li> </ul>"},{"location":"attention_optimization/kv_caching/#common-quantization-schemes","title":"Common Quantization Schemes","text":"Component Typical Format Notes Keys FP16 / BF16 Preserves attention score stability Values INT8 Large memory reduction with minimal quality loss Both INT8 or INT4 Used for extreme long context scenarios <p>Mixed precision KV cache is widely used in practice.</p>"},{"location":"attention_optimization/kv_caching/#quantization-granularity","title":"Quantization Granularity","text":"<p>KV cache quantization can be applied at different levels:</p> <ul> <li>Per tensor: One scale for entire K or V tensor</li> <li>Per head: Separate scale per attention head</li> <li>Per channel: Separate scale per head dimension</li> </ul> <p>Finer granularity improves accuracy but increases metadata and compute overhead.</p>"},{"location":"attention_optimization/kv_caching/#dequantization-during-attention","title":"Dequantization During Attention","text":"<p>At decoding step \\(t\\):</p> <ol> <li>Load quantized \\(K, V\\) from cache</li> <li>Dequantize to FP16 or BF16</li> <li>Compute attention normally:</li> </ol> \\[ \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_h}}\\right)V_{1:t} \\] <p>Dequantization cost is small compared to memory bandwidth savings.</p>"},{"location":"attention_optimization/kv_caching/#impact-on-performance","title":"Impact on Performance","text":"<p>Benefits:</p> <ul> <li>2x to 4x KV memory reduction</li> <li>Higher batch size and longer context</li> <li>Improved inference throughput due to reduced memory traffic</li> </ul> <p>Tradeoffs:</p> <ul> <li>Slight loss in generation quality</li> <li>Additional dequantization overhead</li> </ul> <p>In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.</p>"},{"location":"attention_optimization/kv_caching/#interaction-with-other-optimizations","title":"Interaction with Other Optimizations","text":"<ul> <li>GQA further reduces KV cache size and works well with quantization</li> <li>Paged KV cache benefits from smaller KV blocks</li> <li>FlashAttention amortizes dequantization overhead inside fused kernels</li> </ul>"},{"location":"attention_optimization/kv_caching/#83-prefix-caching","title":"8.3 Prefix Caching","text":"<p>When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.</p>"},{"location":"attention_optimization/kv_caching/#84-paged-kv-cache","title":"8.4 Paged KV Cache","text":"<p>KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.</p>"},{"location":"attention_optimization/kv_caching/#9-grouped-query-attention-gqa","title":"\ud83d\udce69. Grouped Query Attention (GQA)","text":"<p>Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.</p>"},{"location":"attention_optimization/kv_caching/#91-head-configuration","title":"9.1 Head Configuration","text":"\\[ H_q &gt; H_k = H_v \\] <p>Example:</p> <ul> <li>Query heads \\(H_q = 32\\)</li> <li>Key value heads \\(H_k = 8\\)</li> </ul> <p>This reduces KV cache memory by a factor of \\(H_q / H_k\\).</p>"},{"location":"attention_optimization/kv_caching/#92-qk-computation-with-mismatched-heads","title":"9.2 QK Computation with Mismatched Heads","text":"<p>Each key value head is shared by a fixed group of query heads.</p> <p>Let:</p> \\[ g = \\frac{H_q}{H_k} \\] <p>Each key value head serves \\(g\\) query heads.</p> <p>For query head \\(i\\), the corresponding key value head index is:</p> \\[ \\left\\lfloor \\frac{i}{g} \\right\\rfloor \\] <p>The attention computation becomes:</p> \\[ \\text{Attn}_i = \\text{softmax}\\left(\\frac{Q_i K_{\\left\\lfloor i/g \\right\\rfloor}^T}{\\sqrt{d_h}}\\right)V_{\\left\\lfloor i/g \\right\\rfloor} \\] <p>Keys and values are reused directly without additional projection or averaging.</p>"},{"location":"attention_optimization/kv_caching/#93-why-gqa-is-effective","title":"9.3 Why GQA Is Effective","text":"<ul> <li>Query heads retain expressive power</li> <li>Keys and values capture shared context</li> <li>KV cache size and memory bandwidth are significantly reduced</li> </ul> <p>GQA is widely used in production LLMs.</p>"},{"location":"attention_optimization/kv_caching/#10-other-common-optimizations","title":"\ud83d\udce610. Other Common Optimizations","text":""},{"location":"attention_optimization/kv_caching/#flashattention","title":"FlashAttention","text":"<p>FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.</p>"},{"location":"attention_optimization/kv_caching/#chunked-prefill","title":"Chunked Prefill","text":"<p>Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.</p>"},{"location":"attention_optimization/kv_caching/#speculative-decoding","title":"Speculative Decoding","text":"<p>Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.</p>"},{"location":"attention_optimization/paged_attention/","title":"Paged attention","text":""},{"location":"attention_optimization/paged_attention/#1-overview","title":"1. Overview","text":"<p>PagedAttention is a memory management technique for efficient LLM serving that stores KV cache in non-contiguous memory blocks (pages). It's the core innovation behind vLLM, enabling near-zero waste in KV cache memory and higher throughput.</p> <p>Key insight: Treat KV cache like virtual memory in operating systems\u2014use paging to eliminate fragmentation and enable flexible memory sharing.</p>"},{"location":"attention_optimization/paged_attention/#2-the-kv-cache-memory-problem","title":"2. The KV Cache Memory Problem","text":""},{"location":"attention_optimization/paged_attention/#what-is-kv-cache","title":"What is KV Cache?","text":"<p>In autoregressive generation, transformers reuse Key and Value tensors from previous tokens: - Without cache: Recompute K, V for all previous tokens at each step (wasteful) - With cache: Store K, V tensors and only compute for new token</p> <p>For a sequence of length \\(N\\) with \\(L\\) layers, \\(H\\) heads, and dimension \\(d\\): $$ \\text{KV cache size} = 2 \\times N \\times L \\times H \\times d $$</p> <p>Example: LLaMA-13B with 2048 tokens \u2248 800 MB per sequence</p>"},{"location":"attention_optimization/paged_attention/#problems-with-traditional-kv-cache","title":"Problems with Traditional KV Cache","text":""},{"location":"attention_optimization/paged_attention/#problem-1-memory-fragmentation","title":"Problem 1: Memory Fragmentation","text":"<p>Issue: Must pre-allocate contiguous memory for maximum sequence length</p> <pre><code>Sequence 1: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591] (800 tokens, allocated for 2048)\nSequence 2: [\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] (300 tokens, allocated for 2048)\nSequence 3: [\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591] (600 tokens, allocated for 2048)\n</code></pre> <ul> <li>Actual usage: 1700 tokens</li> <li>Allocated: 6144 slots (3 \u00d7 2048)</li> <li>Waste: 72% of memory unused!</li> </ul>"},{"location":"attention_optimization/paged_attention/#problem-2-no-memory-sharing","title":"Problem 2: No Memory Sharing","text":"<ul> <li>Cannot share KV cache across sequences (even with identical prompts)</li> <li>Parallel sampling requires duplicating entire cache</li> <li>Beam search creates multiple copies</li> </ul>"},{"location":"attention_optimization/paged_attention/#problem-3-static-allocation","title":"Problem 3: Static Allocation","text":"<ul> <li>Must allocate for worst case (max sequence length)</li> <li>Can't dynamically adjust based on actual needs</li> <li>Limits batch size and throughput</li> </ul>"},{"location":"attention_optimization/paged_attention/#3-how-pagedattention-works","title":"3. How PagedAttention Works","text":""},{"location":"attention_optimization/paged_attention/#31-core-concept-paging","title":"3.1 Core Concept: Paging","text":"<p>Divide KV cache into fixed-size blocks (pages), similar to OS virtual memory:</p> <pre><code>Logical sequence: [Token 0, Token 1, ..., Token N]\n                         \u2193\nPhysical memory:  [Block 0] \u2192 [Block 5] \u2192 [Block 2] (non-contiguous)\n</code></pre> <p>Key properties: - Block size: Typically 16-64 tokens - Blocks can be anywhere in physical memory - Mapping tracked via block table (like OS page table)</p>"},{"location":"attention_optimization/paged_attention/#32-block-table","title":"3.2 Block Table","text":"<p>Each sequence has a block table mapping logical blocks to physical blocks:</p> <pre><code>Sequence 1:\n  Logical Block 0 \u2192 Physical Block 3\n  Logical Block 1 \u2192 Physical Block 7\n  Logical Block 2 \u2192 Physical Block 1\n\nSequence 2:\n  Logical Block 0 \u2192 Physical Block 3  (shared with Seq 1!)\n  Logical Block 1 \u2192 Physical Block 9\n</code></pre>"},{"location":"attention_optimization/paged_attention/#33-dynamic-allocation","title":"3.3 Dynamic Allocation","text":"<p>Blocks are allocated on-demand as sequences grow:</p> <pre><code># Conceptual allocation\ndef generate_token(sequence):\n    if sequence.last_block_is_full():\n        new_block = allocate_free_block()\n        sequence.block_table.append(new_block)\n\n    # Compute attention using block table\n    output = paged_attention(Q, sequence.block_table)\n    return output\n</code></pre> <p>Benefits: - Only allocate what's actually used - No pre-allocation for max length - Memory freed immediately when sequence completes</p>"},{"location":"attention_optimization/paged_attention/#34-memory-sharing-via-copy-on-write","title":"3.4 Memory Sharing via Copy-on-Write","text":"<p>Multiple sequences can share blocks (read-only):</p> <pre><code>Prompt: \"Translate to French: \"\n         \u2193\n[Block 0: \"Translate to French: \"] \u2190 Shared by all sequences\n\nSeq 1: [Block 0] \u2192 [Block 3: \"Hello \u2192 \"]\nSeq 2: [Block 0] \u2192 [Block 5: \"Goodbye \u2192 \"]\n</code></pre> <p>When modifying a shared block \u2192 copy-on-write: 1. Allocate new physical block 2. Copy contents 3. Update block table 4. Modify the copy</p>"},{"location":"attention_optimization/paged_attention/#4-attention-computation-with-paging","title":"4. Attention Computation with Paging","text":"<p>Standard attention accesses KV cache contiguously. PagedAttention accesses via block table:</p> <pre><code># Simplified PagedAttention\ndef paged_attention(Q, block_table, K_blocks, V_blocks):\n    output = 0\n    for logical_idx, physical_idx in enumerate(block_table):\n        # Fetch K, V from physical block\n        K_block = K_blocks[physical_idx]\n        V_block = V_blocks[physical_idx]\n\n        # Compute attention for this block\n        scores = Q @ K_block.T / sqrt(d)\n        attn = softmax(scores)\n        output += attn @ V_block\n\n    return output\n</code></pre> <p>Key insight: The indirection (block table lookup) has minimal overhead compared to memory savings.</p>"},{"location":"attention_optimization/paged_attention/#5-performance-impact","title":"5. Performance Impact","text":""},{"location":"attention_optimization/paged_attention/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Traditional: 20-40% KV cache utilization (60-80% waste)</li> <li>PagedAttention: 80-95% utilization (5-20% waste)</li> <li>2-3x more sequences in same memory</li> </ul>"},{"location":"attention_optimization/paged_attention/#throughput-improvement","title":"Throughput Improvement","text":"<ul> <li>vLLM with PagedAttention: 2-4x higher throughput vs traditional serving</li> <li>Batch size limited by memory \u2192 bigger batches with less waste</li> </ul>"},{"location":"attention_optimization/paged_attention/#latency","title":"Latency","text":"<ul> <li>Minimal overhead from block table lookups (&lt;5%)</li> <li>Often better latency due to higher batch efficiency</li> </ul>"},{"location":"attention_optimization/paged_attention/#6-interview-questions","title":"6. Interview Questions","text":""},{"location":"attention_optimization/paged_attention/#q1-what-problem-does-pagedattention-solve","title":"Q1: What problem does PagedAttention solve?","text":"<p>Answer: PagedAttention solves memory fragmentation and waste in KV cache management. Traditional approaches pre-allocate contiguous memory for max sequence length, wasting 60-80% of memory. PagedAttention uses non-contiguous blocks allocated on-demand, achieving 80-95% utilization and enabling 2-4x higher throughput.</p>"},{"location":"attention_optimization/paged_attention/#q2-how-is-pagedattention-similar-to-os-virtual-memory","title":"Q2: How is PagedAttention similar to OS virtual memory?","text":"<p>Answer: Both use paging: - Virtual memory: Maps virtual addresses to physical pages via page table - PagedAttention: Maps logical KV cache positions to physical blocks via block table</p> <p>Both enable non-contiguous allocation, on-demand paging, and copy-on-write sharing.</p>"},{"location":"attention_optimization/paged_attention/#q3-whats-a-typical-block-size-and-why","title":"Q3: What's a typical block size and why?","text":"<p>Answer: Typically 16-64 tokens. Trade-offs: - Too small: High block table overhead, more lookups during attention - Too large: Internal fragmentation (wasted space within partially-filled blocks) - Sweet spot: 16-64 balances overhead vs. fragmentation (similar to OS page sizes like 4KB)</p>"},{"location":"attention_optimization/paged_attention/#q4-how-does-pagedattention-enable-memory-sharing","title":"Q4: How does PagedAttention enable memory sharing?","text":"<p>Answer: Multiple sequences can point to the same physical blocks (read-only). Common use cases: - Shared prompts: All sequences share blocks containing the same prompt - Parallel sampling: Multiple outputs from same prompt share prefix blocks - Beam search: Different beams share common prefix</p> <p>When a shared block needs modification \u2192 copy-on-write: allocate new block, copy contents, update that sequence's block table.</p>"},{"location":"attention_optimization/paged_attention/#q5-whats-the-overhead-of-block-table-lookups","title":"Q5: What's the overhead of block table lookups?","text":"<p>Answer: Minimal (&lt;5% typically) because: - Block tables are small (fits in cache) - Lookups are simple integer indexing - Attention computation dominates (matrix ops on blocks) - Modern GPUs handle indirection efficiently</p> <p>The memory savings far outweigh this small overhead.</p>"},{"location":"attention_optimization/paged_attention/#q6-how-does-pagedattention-improve-throughput","title":"Q6: How does PagedAttention improve throughput?","text":"<p>Answer: By reducing memory waste: 1. Traditional: Can fit 10 sequences (60% waste) 2. PagedAttention: Can fit 25 sequences (10% waste) in same memory 3. Bigger batches \u2192 better GPU utilization \u2192 higher throughput</p> <p>Typical improvement: 2-4x more requests/second.</p>"},{"location":"attention_optimization/paged_attention/#q7-what-happens-when-we-run-out-of-physical-blocks","title":"Q7: What happens when we run out of physical blocks?","text":"<p>Answer: Memory management strategies: - Preemption: Evict lower-priority sequences, save their state - Swapping: Move blocks to CPU memory (like OS swap) - Recomputation: Drop blocks and recompute if needed - Blocking: Wait until blocks free up</p> <p>vLLM typically uses preemption for fairness and efficiency.</p>"},{"location":"attention_optimization/paged_attention/#q8-can-pagedattention-work-with-flashattention","title":"Q8: Can PagedAttention work with FlashAttention?","text":"<p>Answer: Yes! They're complementary: - FlashAttention: Optimizes attention computation (tiling, kernel fusion) - PagedAttention: Optimizes KV cache memory management (paging, sharing)</p> <p>You can use both together: FlashAttention for fast computation, PagedAttention for efficient memory. vLLM does exactly this.</p>"},{"location":"attention_optimization/paged_attention/#q9-whats-the-difference-between-block-size-and-tile-size","title":"Q9: What's the difference between block size and tile size?","text":"<p>Answer: - Block size (PagedAttention): Memory management granularity (16-64 tokens)   - Determines allocation unit for KV cache storage - Tile size (FlashAttention): Computation granularity (128-256 tokens)   - Determines how much data loads into shared memory at once</p> <p>They're independent concepts operating at different levels (memory management vs computation).</p>"},{"location":"attention_optimization/paged_attention/#q10-what-are-the-limitations-of-pagedattention","title":"Q10: What are the limitations of PagedAttention?","text":"<p>Answer: - Complexity: More complex implementation than contiguous allocation - Indirection overhead: Small cost from block table lookups - GPU kernel changes: Requires custom attention kernels that understand block tables - Internal fragmentation: Last block in sequence may be partially empty</p> <p>Despite these, benefits (2-4x throughput) far outweigh costs for LLM serving.</p>"},{"location":"attention_optimization/paged_attention/#7-pagedattention-in-practice-vllm","title":"7. PagedAttention in Practice (vLLM)","text":""},{"location":"attention_optimization/paged_attention/#key-features","title":"Key Features","text":"<pre><code># vLLM with PagedAttention\nfrom vllm import LLM\n\nllm = LLM(model=\"meta-llama/Llama-2-7b\")\n\n# Automatic memory management\noutputs = llm.generate(prompts, sampling_params)\n# - Blocks allocated on-demand\n# - Shared prompts reuse blocks\n# - Memory freed automatically\n</code></pre>"},{"location":"attention_optimization/paged_attention/#use-cases-where-it-shines","title":"Use Cases Where It Shines","text":"<p>\u2705 High-throughput serving (many concurrent requests) \u2705 Long sequences (less pre-allocation waste) \u2705 Parallel sampling / beam search (shared prefixes) \u2705 Shared system prompts across requests  </p> <p>\u274c Single-sequence inference (no sharing benefits) \u274c Very short sequences (overhead not amortized)  </p>"},{"location":"attention_optimization/paged_attention/#8-key-takeaways-for-interviews","title":"8. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Treat KV cache like OS virtual memory\u2014use paging for efficient, flexible allocation</li> <li>Problem solved: Memory fragmentation (60-80% waste \u2192 5-20% waste)</li> <li>Mechanism: Block table maps logical positions to physical memory blocks</li> <li>Sharing: Copy-on-write enables multiple sequences to share read-only blocks</li> <li>Performance: 2-4x throughput improvement in LLM serving</li> <li>Complementary: Works alongside FlashAttention (computation vs memory optimization)</li> </ol>"},{"location":"attention_optimization/paged_attention/#9-comparison-table","title":"9. Comparison Table","text":"Aspect Traditional KV Cache PagedAttention Allocation Contiguous, pre-allocated Non-contiguous, on-demand Memory waste 60-80% 5-20% Max sequences Limited by pre-allocation 2-4x more in same memory Sharing No sharing Copy-on-write sharing Complexity Simple More complex Overhead None &lt;5% (block lookups) Throughput Baseline 2-4x higher"},{"location":"decoding_strategies/beam_search/","title":"Beam Search - Interview Prep Guide","text":""},{"location":"decoding_strategies/beam_search/#1-overview","title":"1. Overview","text":"<p>Beam search maintains K candidate sequences (beams) at each decoding step and selects the sequence with the highest cumulative probability. It balances between greedy decoding (K=1) and exhaustive search (K=vocab_size).</p> <p>Key insight: Explore multiple promising paths simultaneously to avoid getting stuck in locally optimal but globally suboptimal sequences.</p>"},{"location":"decoding_strategies/beam_search/#2-how-it-works","title":"2. How It Works","text":""},{"location":"decoding_strategies/beam_search/#algorithm","title":"Algorithm","text":"<pre><code>def beam_search(model, prompt, beam_width=5, max_length=50):\n    # Initialize: One beam with the prompt\n    beams = [(prompt, 0.0)]  # (sequence, cumulative_log_prob)\n\n    for step in range(max_length):\n        candidates = []\n\n        # Expand each beam\n        for seq, score in beams:\n            if seq[-1] == EOS:  # Completed sequence\n                candidates.append((seq, score))\n                continue\n\n            logits = model(seq)\n            probs = softmax(logits)\n\n            # Consider all tokens\n            for token_id, prob in enumerate(probs):\n                new_seq = seq + [token_id]\n                new_score = score + log(prob)  # Cumulative log probability\n                candidates.append((new_seq, new_score))\n\n        # Keep top K beams\n        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Stop if all beams are complete\n        if all(seq[-1] == EOS for seq, _ in beams):\n            break\n\n    return beams[0][0]  # Return best sequence\n</code></pre>"},{"location":"decoding_strategies/beam_search/#core-concepts","title":"Core Concepts","text":"<p>Beam width (K): - K=1: Greedy decoding - K=5-10: Typical for translation - K=50+: Exhaustive (expensive)</p> <p>Cumulative log probability: - Use log probabilities to avoid numerical underflow - Score = log(P\u2081) + log(P\u2082) + ... + log(P\u2099) - Equivalent to log(P\u2081 \u00d7 P\u2082 \u00d7 ... \u00d7 P\u2099)</p>"},{"location":"decoding_strategies/beam_search/#3-example-walkthrough","title":"3. Example Walkthrough","text":"<p>Prompt: \"The cat sat on the\"</p> <p>Beam width: 3</p>"},{"location":"decoding_strategies/beam_search/#step-1-first-token","title":"Step 1: First Token","text":"<p>Model probabilities: | Token | Probability | Log Prob | |-------|-------------|----------| | mat   | 0.40        | -0.92    | | floor | 0.25        | -1.39    | | sofa  | 0.15        | -1.90    | | bed   | 0.10        | -2.30    |</p> <p>Top 3 beams: 1. \"The cat sat on the mat\" \u2192 score: -0.92 2. \"The cat sat on the floor\" \u2192 score: -1.39 3. \"The cat sat on the sofa\" \u2192 score: -1.90</p>"},{"location":"decoding_strategies/beam_search/#step-2-second-token","title":"Step 2: Second Token","text":"<p>Expand each beam:</p> <p>Beam 1: \"mat\" + next token - \"mat .\" \u2192 -0.92 + (-0.51) = -1.43 - \"mat and\" \u2192 -0.92 + (-1.61) = -2.53</p> <p>Beam 2: \"floor\" + next token - \"floor .\" \u2192 -1.39 + (-0.36) = -1.75 - \"floor while\" \u2192 -1.39 + (-1.20) = -2.59</p> <p>Beam 3: \"sofa\" + next token - \"sofa .\" \u2192 -1.90 + (-0.41) = -2.31 - \"sofa when\" \u2192 -1.90 + (-0.51) = -2.41</p> <p>New top 3 beams: 1. \"The cat sat on the mat .\" \u2192 -1.43 2. \"The cat sat on the floor .\" \u2192 -1.75 3. \"The cat sat on the sofa when\" \u2192 -2.41</p> <p>Final output: \"The cat sat on the mat.\"</p>"},{"location":"decoding_strategies/beam_search/#4-key-characteristics","title":"4. Key Characteristics","text":""},{"location":"decoding_strategies/beam_search/#explores-multiple-paths","title":"Explores Multiple Paths","text":"<p>Unlike greedy, beam search maintains K hypotheses: - Can recover from locally suboptimal choices - Considers alternative continuations - Better global optimization</p>"},{"location":"decoding_strategies/beam_search/#length-bias-problem","title":"Length Bias Problem","text":"<p>Longer sequences accumulate more negative log probabilities: <pre><code>Seq 1 (length 5): -0.5 + -0.6 + -0.4 + -0.5 + -0.3 = -2.3\nSeq 2 (length 3): -0.5 + -0.6 + -0.4 = -1.5\n</code></pre></p> <p>Seq 2 has higher score despite being incomplete!</p> <p>Solution: Length normalization <pre><code>normalized_score = score / length^\u03b1\n# \u03b1 = 0.6-0.8 typical\n# \u03b1 = 0: no normalization\n# \u03b1 = 1: full normalization\n</code></pre></p>"},{"location":"decoding_strategies/beam_search/#5-common-problems","title":"5. Common Problems","text":""},{"location":"decoding_strategies/beam_search/#problem-1-reduced-diversity","title":"Problem 1: Reduced Diversity","text":"<p>All beams often converge to similar outputs:</p> <pre><code>Prompt: \"I think that\"\n\nBeam 1: \"I think that we should focus on...\"\nBeam 2: \"I think that we need to consider...\"\nBeam 3: \"I think that this is important...\"\n</code></pre> <p>All beams start with safe, high-probability tokens \u2192 similar continuations.</p> <p>Why: Beam search favors safe, high-probability paths over diverse, creative paths.</p>"},{"location":"decoding_strategies/beam_search/#problem-2-generic-outputs","title":"Problem 2: Generic Outputs","text":"<p>In open-ended generation: <pre><code>Prompt: \"Tell me a story about\"\n\nGreedy: \"a boy who lived in\"\nBeam (K=5): \"a young boy who lived in a small town\"\n</code></pre></p> <p>Beam search produces grammatically perfect but boring text.</p>"},{"location":"decoding_strategies/beam_search/#problem-3-computational-cost","title":"Problem 3: Computational Cost","text":"<ul> <li>Memory: O(K \u00d7 T \u00d7 V) for storing beam candidates</li> <li>Time: O(K \u00d7 V) per step (vs O(V) for greedy)</li> <li>K=10 \u2192 10\u00d7 slower than greedy</li> </ul>"},{"location":"decoding_strategies/beam_search/#6-length-normalization","title":"6. Length Normalization","text":""},{"location":"decoding_strategies/beam_search/#without-normalization","title":"Without Normalization","text":"<pre><code># Shorter sequences preferred\nbeams = sorted(candidates, key=lambda x: x[1], reverse=True)\n</code></pre>"},{"location":"decoding_strategies/beam_search/#with-normalization","title":"With Normalization","text":"<pre><code>def length_penalty(length, alpha=0.6):\n    return ((5 + length) / 6) ** alpha  # Google NMT formula\n\n# Normalized score\nnormalized = score / length_penalty(len(seq), alpha=0.6)\nbeams = sorted(candidates, key=lambda x: normalized_score(x), reverse=True)\n</code></pre> <p>Effect: - Encourages longer, more complete sequences - Prevents premature termination - Essential for translation and summarization</p>"},{"location":"decoding_strategies/beam_search/#7-when-to-use-beam-search","title":"7. When to Use Beam Search","text":""},{"location":"decoding_strategies/beam_search/#good-use-cases","title":"\u2705 Good Use Cases","text":"<p>Structured tasks with clear objectives: - Machine translation - Automatic speech recognition (ASR) - Image captioning - Summarization - Question answering (extractive)</p> <p>When correctness matters more than creativity: - Technical documentation generation - Code comment generation - Medical report generation</p>"},{"location":"decoding_strategies/beam_search/#poor-use-cases","title":"\u274c Poor Use Cases","text":"<p>Creative or conversational tasks: - Story writing - Dialogue systems - Chatbots - Poetry generation</p> <p>Tasks requiring diversity: - Brainstorming - Multiple solution generation - Creative writing</p>"},{"location":"decoding_strategies/beam_search/#8-variants-and-improvements","title":"8. Variants and Improvements","text":""},{"location":"decoding_strategies/beam_search/#diverse-beam-search","title":"Diverse Beam Search","text":"<p>Force beams to be dissimilar using diversity penalty: <pre><code>diversity_penalty = 0.5\nfor beam_group in groups:\n    # Penalize tokens already chosen by other groups\n    adjusted_score = score - diversity_penalty * overlap_count\n</code></pre></p>"},{"location":"decoding_strategies/beam_search/#constrained-beam-search","title":"Constrained Beam Search","text":"<p>Force inclusion of specific tokens/phrases: <pre><code># E.g., must include \"climate change\" in summary\nconstraints = [\"climate\", \"change\"]\n# Only keep beams that satisfy constraints\n</code></pre></p>"},{"location":"decoding_strategies/beam_search/#stochastic-beam-search","title":"Stochastic Beam Search","text":"<p>Add randomness to beam selection for more diversity.</p>"},{"location":"decoding_strategies/beam_search/#9-interview-questions","title":"9. Interview Questions","text":""},{"location":"decoding_strategies/beam_search/#q1-what-is-beam-search-and-how-does-it-differ-from-greedy-decoding","title":"Q1: What is beam search and how does it differ from greedy decoding?","text":"<p>Answer: Beam search maintains K candidate sequences (beams) instead of just one. At each step, it expands all beams, scores all possible continuations, and keeps the top K. This allows exploration of multiple paths and can recover from locally suboptimal choices, unlike greedy which commits to a single path.</p>"},{"location":"decoding_strategies/beam_search/#q2-why-use-log-probabilities-instead-of-regular-probabilities","title":"Q2: Why use log probabilities instead of regular probabilities?","text":"<p>Answer: Two reasons: 1. Numerical stability: Multiplying many small probabilities (0.3 \u00d7 0.4 \u00d7 0.2...) quickly underflows to zero in floating point 2. Computational efficiency: Log transforms products to sums: log(P\u2081 \u00d7 P\u2082) = log(P\u2081) + log(P\u2082), which is more stable and efficient</p>"},{"location":"decoding_strategies/beam_search/#q3-what-is-the-length-bias-problem-in-beam-search","title":"Q3: What is the length bias problem in beam search?","text":"<p>Answer: Longer sequences accumulate more negative log probabilities, making them score lower than shorter sequences even if they're more complete. For example, a 10-token sequence might score -8.5 while a 5-token incomplete sequence scores -3.2. Length normalization (dividing by sequence length raised to \u03b1) addresses this by favoring complete sentences.</p>"},{"location":"decoding_strategies/beam_search/#q4-why-does-beam-search-produce-less-diverse-outputs-than-sampling","title":"Q4: Why does beam search produce less diverse outputs than sampling?","text":"<p>Answer: Beam search is deterministic and risk-averse\u2014it keeps the K highest-probability sequences. This means all beams tend to follow safe, high-probability paths, leading to similar outputs. Rare but creative continuations are discarded early. Sampling methods can explore lower-probability tokens, leading to more diversity.</p>"},{"location":"decoding_strategies/beam_search/#q5-whats-the-computational-complexity-of-beam-search","title":"Q5: What's the computational complexity of beam search?","text":"<p>Answer: - Time per step: O(K \u00d7 V) where K=beam width, V=vocab size   - Greedy: O(V), so beam is K\u00d7 slower - Memory: O(K \u00d7 T) to store K beams of length T - Total: For T steps, O(K \u00d7 V \u00d7 T) time</p> <p>Typical K=5-10 for translation, but this 5-10\u00d7 slowdown is significant.</p>"},{"location":"decoding_strategies/beam_search/#q6-how-do-you-choose-the-optimal-beam-width-k","title":"Q6: How do you choose the optimal beam width K?","text":"<p>Answer: Trade-off between quality and speed: - K=1: Greedy (fast, low quality) - K=5-10: Standard for translation (good balance) - K=50+: Diminishing returns, very slow</p> <p>Empirically, quality plateaus around K=10 for most tasks. Beyond that, you get marginal gains for significant computational cost.</p>"},{"location":"decoding_strategies/beam_search/#q7-can-beam-search-guarantee-finding-the-optimal-sequence","title":"Q7: Can beam search guarantee finding the optimal sequence?","text":"<p>Answer: No. Beam search is a heuristic that prunes the search space. It only explores the top K paths at each step, potentially discarding paths that could lead to the globally optimal sequence later. Full exhaustive search (K=|V|^T) is computationally infeasible, so beam search is a practical approximation.</p>"},{"location":"decoding_strategies/beam_search/#q8-what-is-diverse-beam-search-and-when-is-it-useful","title":"Q8: What is diverse beam search and when is it useful?","text":"<p>Answer: Diverse beam search forces different beam groups to explore different areas of the search space by penalizing similarity. It's useful when you need multiple distinct outputs (e.g., generating K different translations or summaries). Standard beam search often produces K very similar sequences, which isn't helpful for diversity.</p>"},{"location":"decoding_strategies/beam_search/#q9-when-would-you-use-beam-search-over-sampling-methods-like-top-p","title":"Q9: When would you use beam search over sampling methods like top-p?","text":"<p>Answer: Use beam search for: - Objective quality metrics (BLEU, ROUGE) that correlate with likelihood - Structured outputs (translation, ASR) with one correct answer - Deterministic requirements (reproducibility)</p> <p>Use sampling for: - Creative tasks requiring diversity - Conversational AI needing personality - Open-ended generation where many good answers exist</p>"},{"location":"decoding_strategies/beam_search/#q10-how-does-beam-search-handle-the-eos-token","title":"Q10: How does beam search handle the EOS token?","text":"<p>Answer: When a beam generates EOS (end-of-sequence), it's marked as complete: 1. Complete beams stop expanding 2. They remain in the candidate pool with their final score 3. Active beams continue generating 4. Search terminates when all K beams complete or max_length reached 5. Return the complete beam with highest normalized score</p> <p>Some implementations use length normalization to fairly compare complete sequences of different lengths.</p>"},{"location":"decoding_strategies/beam_search/#10-code-example","title":"10. Code Example","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom typing import List, Tuple\n\ndef beam_search(\n    model,\n    input_ids: torch.Tensor,\n    beam_width: int = 5,\n    max_length: int = 50,\n    length_penalty: float = 0.6,\n    eos_token_id: int = 2\n) -&gt; torch.Tensor:\n    \"\"\"\n    Beam search decoding.\n\n    Args:\n        model: Language model\n        input_ids: Starting tokens [1, seq_len]\n        beam_width: Number of beams\n        max_length: Maximum sequence length\n        length_penalty: Alpha for length normalization\n        eos_token_id: End of sequence token\n\n    Returns:\n        Best sequence found\n    \"\"\"\n    device = input_ids.device\n    batch_size = input_ids.size(0)\n\n    # Initialize beams: (sequence, score, finished)\n    beams = [(input_ids[0].tolist(), 0.0, False)]\n\n    for _ in range(max_length):\n        candidates = []\n\n        for seq, score, finished in beams:\n            if finished:\n                candidates.append((seq, score, finished))\n                continue\n\n            # Get next token probabilities\n            input_tensor = torch.tensor([seq], device=device)\n            with torch.no_grad():\n                outputs = model(input_tensor)\n                logits = outputs.logits[0, -1, :]\n                log_probs = F.log_softmax(logits, dim=-1)\n\n            # Expand beam with top-K tokens\n            top_k_probs, top_k_ids = torch.topk(log_probs, beam_width)\n\n            for prob, token_id in zip(top_k_probs, top_k_ids):\n                new_seq = seq + [token_id.item()]\n                new_score = score + prob.item()\n                is_finished = (token_id.item() == eos_token_id)\n                candidates.append((new_seq, new_score, is_finished))\n\n        # Apply length penalty and keep top beams\n        def normalized_score(item):\n            seq, score, _ = item\n            penalty = ((5 + len(seq)) / 6) ** length_penalty\n            return score / penalty\n\n        beams = sorted(candidates, key=normalized_score, reverse=True)[:beam_width]\n\n        # Early stopping if all beams finished\n        if all(finished for _, _, finished in beams):\n            break\n\n    # Return best sequence\n    best_seq, _, _ = beams[0]\n    return torch.tensor([best_seq], device=device)\n\n# Usage\n# output = beam_search(model, prompt_ids, beam_width=5)\n</code></pre>"},{"location":"decoding_strategies/beam_search/#11-key-takeaways-for-interviews","title":"11. Key Takeaways for Interviews","text":"<ol> <li>Definition: Maintains K candidate sequences, keeps top-K by cumulative probability</li> <li>Beam width K: Trade-off between quality (higher K) and speed (lower K)</li> <li>Length normalization: Essential to prevent bias toward shorter sequences</li> <li>Pros: Better than greedy, explores alternatives, good for structured tasks</li> <li>Cons: Computationally expensive (K\u00d7 slower), low diversity, generic outputs</li> <li>Best for: Translation, ASR, captioning, summarization</li> <li>Worst for: Creative writing, dialogue, brainstorming</li> <li>Complexity: O(K \u00d7 V \u00d7 T) time, O(K \u00d7 T) space</li> </ol>"},{"location":"decoding_strategies/beam_search/#references","title":"References","text":"<ul> <li>Google's Neural Machine Translation System - Length normalization formula</li> <li>The Curious Case of Neural Text Degeneration - Discusses diversity issues</li> <li>Diverse Beam Search</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/","title":"Decoding Strategies","text":""},{"location":"decoding_strategies/decoding_strategies/#decoding-strategies","title":"\ud83d\udce6 Decoding Strategies","text":""},{"location":"decoding_strategies/decoding_strategies/#1-overview","title":"1. Overview","text":"<p>Large Language Models output a probability distribution over the vocabulary at each decoding step. A decoding strategy defines how the next token is selected from this distribution.</p> <p>This page covers five commonly used decoding strategies:</p> <ol> <li>Greedy decoding  </li> <li>Beam search  </li> <li>Temperature sampling  </li> <li>Top-k sampling  </li> <li>Top-p sampling (nucleus sampling)</li> </ol>"},{"location":"decoding_strategies/decoding_strategies/#2-decoding-strategies-explained-with-examples","title":"2. Decoding Strategies Explained with examples","text":"<p>Toy probability distribution used in examples. Assume the model predicts the next token after:</p> <pre><code>**\"The cat sat on the\"**\n</code></pre> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02"},{"location":"decoding_strategies/decoding_strategies/#21-greedy-decoding","title":"2.1. Greedy Decoding","text":""},{"location":"decoding_strategies/decoding_strategies/#idea","title":"Idea","text":"<p>Always select the token with the highest probability.</p>"},{"location":"decoding_strategies/decoding_strategies/#algorithm","title":"Algorithm","text":"<pre><code>next_token = argmax(probabilities)\nHighest probability token is `mat`.\nOutput: The cat sat on the mat\n</code></pre>"},{"location":"decoding_strategies/decoding_strategies/#edge-case","title":"Edge Case","text":"<pre><code>If probabilities are very close: A: 0.31, B: 0.30, C: 0.29\nGreedy decoding always selects `A`, even when the model is uncertain.\n\nThis often leads to repetitive or dull outputs.\n</code></pre>"},{"location":"decoding_strategies/decoding_strategies/#when-to-use","title":"When to use","text":"<ul> <li>Debugging</li> <li>Baselines</li> <li>Deterministic generation</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#python-example","title":"Python example","text":"<pre><code>import torch\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\nnext_token = torch.argmax(probs)\nprint(next_token.item())\n</code></pre>"},{"location":"decoding_strategies/decoding_strategies/#22-beam-search","title":"2.2 Beam Search","text":""},{"location":"decoding_strategies/decoding_strategies/#idea_1","title":"Idea","text":"<p>Beam search keeps multiple candidate sequences at each decoding step instead of a single one. It selects the sequence with the highest overall probability, not just the best local choice.</p>"},{"location":"decoding_strategies/decoding_strategies/#algorithm_1","title":"Algorithm","text":"<ol> <li>Maintain B beams, where B is the beam width  </li> <li>At each step, expand every beam with all possible next tokens  </li> <li>Compute cumulative log probability for each expanded sequence  </li> <li>Keep the top B sequences  </li> <li>Repeat until an end condition is met</li> </ol>"},{"location":"decoding_strategies/decoding_strategies/#example","title":"Example","text":"<p>Assume the next-token probabilities after:</p> <p>\"The cat sat on the\"</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 <p>Beam width = 2</p> <p>Step 1</p> <ul> <li>Beam 1: <code>\"mat\"</code> score = log(0.40)</li> <li>Beam 2: <code>\"floor\"</code> score = log(0.25)</li> </ul> <p>Step 2</p> <ul> <li><code>\"mat \u2192 quietly\"</code> score = log(0.40) + log(0.30)</li> <li><code>\"floor \u2192 loudly\"</code> score = log(0.25) + log(0.50)</li> </ul> <p>Even if <code>\"quietly\"</code> was locally better, <code>\"floor \u2192 loudly\"</code> may win due to higher cumulative probability.</p> <p>Final output is the sequence with the highest total score.</p>"},{"location":"decoding_strategies/decoding_strategies/#edge-case_1","title":"Edge Case","text":"<p>Beam search tends to favor safe, high-probability continuations, which can reduce diversity. This behavior becomes obvious in conversational or creative tasks.</p> <p>Assume the model is generating the next phrase after:</p> <p>\"I think that\"</p> <p>At a certain step, the model assigns probabilities like:</p> Token Probability the 0.35 we 0.30 this 0.15 pizza 0.10 unicorn 0.10 <p>With beam width = 3:</p> <p>All beams will keep continuations starting with: <code>\"the\"</code> <code>\"we\"</code> <code>\"this\"</code></p> <p>Tokens like <code>\"pizza\"</code> and <code>\"unicorn\"</code> are discarded early because their probabilities are lower.</p> <p>As decoding continues, beams converge to similar phrases:</p> <ul> <li>I think that the best way to...</li> <li>I think that we should...</li> <li>I think that this is...</li> </ul> <p>All beams are grammatically correct but nearly identical.</p> <p>If top-p sampling is used instead:</p> <ul> <li>Tokens like <code>\"pizza\"</code> or <code>\"unicorn\"</code> may occasionally be sampled</li> <li> <p>Outputs become more diverse:</p> <ul> <li>I think that pizza could solve this</li> <li>I think that unicorn stories are fun</li> </ul> </li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#when-to-use-beam-search","title":"When to use beam search","text":"<ul> <li>Machine translation  </li> <li>Speech recognition  </li> <li>Structured text generation  </li> <li>Tasks where correctness matters more than diversity</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#when-not-to-use-beam-search","title":"When not to use beam search","text":"<ul> <li>Chatbots  </li> <li>Story generation  </li> <li>Creative writing  </li> <li>Conversational agents</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#python-example-simplified","title":"Python example (simplified)","text":"<pre><code>from heapq import nlargest\nimport math\n\ndef beam_search_step(beams, probs, beam_width):\n    new_beams = []\n    for seq, score in beams:\n        for i, p in enumerate(probs):\n            new_seq = seq + [i]\n            new_score = score + math.log(p)\n            new_beams.append((new_seq, new_score))\n    return nlargest(beam_width, new_beams, key=lambda x: x[1])\n\n# Initial beam\nbeams = [([], 0.0)]\nprobs = [0.40, 0.25, 0.15]\n\nbeams = beam_search_step(beams, probs, beam_width=2)\nprint(beams)\n</code></pre>"},{"location":"decoding_strategies/decoding_strategies/#23-temperature-sampling","title":"2.3 Temperature Sampling","text":""},{"location":"decoding_strategies/decoding_strategies/#idea_2","title":"Idea","text":"<p>Temperature controls how random the next-token selection is by scaling the model logits before applying softmax.</p> <p>It does not change which tokens are possible. It changes how strongly the model prefers high-probability tokens.</p>"},{"location":"decoding_strategies/decoding_strategies/#formula","title":"Formula","text":"\\[p_i = \\text{softmax}(\\text{logits}_i / T)\\] <p>Where:</p> <ul> <li><code>T</code> is the temperature</li> <li>lower <code>T</code> sharpens the distribution</li> <li>higher <code>T</code> flattens the distribution</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#effect-of-temperature","title":"Effect of temperature","text":"Temperature Behavior T &lt; 1 More deterministic T = 1 Original distribution T &gt; 1 More random"},{"location":"decoding_strategies/decoding_strategies/#example_1","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Low temperature (T = 0.3)</p> <ul> <li>Distribution becomes very sharp</li> <li><code>mat</code> dominates even more</li> </ul> <p>Output: The cat sat on the mat</p> <p>This behaves almost like greedy decoding.</p> <p>High temperature (T = 1.5)</p> <ul> <li>Distribution becomes flatter</li> <li>Low-probability tokens become more likely</li> </ul> <p>Possible output: The cat sat on the moon</p>"},{"location":"decoding_strategies/decoding_strategies/#edge-case_2","title":"Edge Case","text":"<p>With very high temperature:</p> Token Probability mat 0.18 floor 0.17 sofa 0.16 bed 0.15 roof 0.14 moon 0.10 pizza 0.10 <p>The model loses strong preferences and may generate incoherent text:</p> <pre><code>The cat sat on pizza quantum sky\n</code></pre>"},{"location":"decoding_strategies/decoding_strategies/#when-temperature-helps","title":"When temperature helps","text":"<ul> <li>Creative writing</li> <li>Brainstorming</li> <li>Open-ended dialogue</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#when-temperature-hurts","title":"When temperature hurts","text":"<ul> <li>Factual tasks</li> <li>Code generation</li> <li>Structured outputs</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#python-example_1","title":"Python example","text":"<pre><code>import torch\n\nlogits = torch.log(torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]))\ntemperature = 1.2\n\nscaled_logits = logits / temperature\nprobs = torch.softmax(scaled_logits, dim=0)\n\nnext_token = torch.multinomial(probs, 1)\nprint(next_token.item())\n</code></pre> <p>Note: Temperature controls randomness, not feasibility. It is usually combined with top-p or top-k sampling to avoid incoherent outputs.</p>"},{"location":"decoding_strategies/decoding_strategies/#24-top-k-sampling","title":"2.4 Top-k Sampling","text":""},{"location":"decoding_strategies/decoding_strategies/#idea_3","title":"Idea","text":"<p>Top-k sampling restricts the model to sample only from the K most probable tokens at each decoding step. This prevents extremely unlikely tokens from being selected while still allowing randomness.</p>"},{"location":"decoding_strategies/decoding_strategies/#algorithm_2","title":"Algorithm","text":"<ol> <li>Sort all tokens by probability  </li> <li>Keep only the top K tokens  </li> <li>Renormalize their probabilities  </li> <li>Sample one token  </li> </ol>"},{"location":"decoding_strategies/decoding_strategies/#example_2","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-k with k = 3</p> <p>Tokens kept:</p> <ul> <li>mat</li> <li>floor</li> <li>sofa</li> </ul> <p>Tokens removed:</p> <ul> <li>bed, roof, moon, pizza</li> </ul> <p>Possible output: The cat sat on the sofa</p>"},{"location":"decoding_strategies/decoding_strategies/#edge-case_3","title":"Edge Case","text":"<p>Flat probability distribution</p> <p>Assume: A: 0.11, B: 0.10, C: 0.10, D: 0.10, E: 0.10, F: 0.10, G: 0.10</p> <p>With <code>k = 3</code>:</p> <ul> <li>Only A, B, C are considered</li> <li>D, E, F, G are removed despite being equally likely</li> </ul> <p>This makes top-k sensitive to the choice of K and blind to the shape of the distribution.</p>"},{"location":"decoding_strategies/decoding_strategies/#when-top-k-works-well","title":"When top-k works well","text":"<ul> <li>Moderate creativity with controlled randomness</li> <li>General text generation</li> <li>Chat systems with fixed diversity constraints</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#when-top-k-works-poorly","title":"When top-k works poorly","text":"<ul> <li>Highly uncertain distributions</li> <li>Long-form creative writing</li> <li>Prompts with many equally valid continuations</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#python-example_2","title":"Python example","text":"<pre><code>import torch\n\ndef top_k_sampling(probs, k):\n    topk_probs, topk_idx = torch.topk(probs, k)\n    topk_probs = topk_probs / topk_probs.sum()\n    sampled = torch.multinomial(topk_probs, 1)\n    return topk_idx[sampled]\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\ntoken = top_k_sampling(probs, k=3)\nprint(token.item())\n</code></pre> <p>Note: Top-k sampling fixes the number of candidate tokens regardless of model confidence. This makes it simpler than top-p but less adaptive in practice.</p>"},{"location":"decoding_strategies/decoding_strategies/#25-top-p-sampling-nucleus-sampling","title":"2.5 Top-p Sampling (Nucleus Sampling)","text":""},{"location":"decoding_strategies/decoding_strategies/#idea_4","title":"Idea","text":"<p>Top-p sampling selects the smallest possible set of tokens whose cumulative probability is at least <code>p</code>, then samples from that set. Unlike top-k, the number of candidate tokens changes dynamically based on model confidence.</p>"},{"location":"decoding_strategies/decoding_strategies/#algorithm_3","title":"Algorithm","text":"<ol> <li>Sort tokens by probability in descending order  </li> <li>Add tokens until cumulative probability \u2265 <code>p</code> </li> <li>Renormalize probabilities within this set  </li> <li>Sample one token  </li> </ol>"},{"location":"decoding_strategies/decoding_strategies/#example_3","title":"Example","text":"<p>Assume the next-token probabilities are:</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-p with p = 0.9</p> <p>Cumulative probability:</p> <ul> <li>mat \u2192 0.40  </li> <li>floor \u2192 0.65  </li> <li>sofa \u2192 0.80  </li> <li>bed \u2192 0.90  </li> </ul> <p>Tokens selected:</p> <ul> <li>mat</li> <li>floor</li> <li>sofa</li> <li>bed</li> </ul> <p>Possible output: The cat sat on the bed</p>"},{"location":"decoding_strategies/decoding_strategies/#edge-case-key-difference-from-top-k","title":"Edge Case (Key Difference from Top-k)","text":"<p>Highly confident model</p> <p>Assume: A: 0.85, B: 0.07, C: 0.03, D: 0.03, E: 0.02</p> <p>With <code>p = 0.9</code>:</p> <ul> <li>Selected tokens: A, B  </li> <li>Effective K = 2</li> </ul> <p>With top-k (k = 5):</p> <ul> <li>Selected tokens: A, B, C, D, E  </li> </ul> <p>Top-p automatically reduces randomness when the model is confident.</p>"},{"location":"decoding_strategies/decoding_strategies/#another-edge-case","title":"Another Edge Case","text":"<p>Uncertain model</p> <p>Assume: A: 0.20, B: 0.20, C: 0.20, D: 0.20, E: 0.20</p> <p>With <code>p = 0.9</code>:</p> <ul> <li>Selected tokens: A, B, C, D, E  </li> <li>Effective K = 5</li> </ul> <p>Top-p expands the candidate set when uncertainty is high.</p>"},{"location":"decoding_strategies/decoding_strategies/#when-top-p-works-well","title":"When top-p works well","text":"<ul> <li>Conversational agents</li> <li>Long-form text generation</li> <li>Creative writing with coherence</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#when-top-p-works-poorly","title":"When top-p works poorly","text":"<ul> <li>Strictly deterministic tasks</li> <li>Code generation with exact formatting requirements</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#python-example_3","title":"Python example","text":"<pre><code>import torch\n\ndef top_p_sampling(probs, p):\n    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n    cumulative = torch.cumsum(sorted_probs, dim=0)\n\n    cutoff_mask = cumulative &lt;= p\n    cutoff_mask[cutoff_mask.sum()] = True\n\n    filtered_probs = sorted_probs[cutoff_mask]\n    filtered_probs = filtered_probs / filtered_probs.sum()\n\n    sampled = torch.multinomial(filtered_probs, 1)\n    return sorted_idx[cutoff_mask][sampled]\n\nprobs = torch.tensor([0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02])\ntoken = top_p_sampling(probs, p=0.9)\nprint(token.item())\n</code></pre> <p>Note : Top-p sampling adapts to the probability distribution shape, making it more robust than top-k for real-world language generation.</p>"},{"location":"decoding_strategies/decoding_strategies/#3-pros-and-cons-of-decoding-strategies-in-large-language-models","title":"3. Pros and Cons of Decoding Strategies in Large Language Models","text":""},{"location":"decoding_strategies/decoding_strategies/#31-greedy-decoding","title":"3.1 Greedy Decoding","text":""},{"location":"decoding_strategies/decoding_strategies/#pros","title":"Pros","text":"<ul> <li>Extremely fast and simple</li> <li>Fully deterministic and reproducible</li> <li>Easy to debug and analyze</li> <li>Works well when the model is very confident</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#cons","title":"Cons","text":"<ul> <li>No diversity at all</li> <li>Easily gets stuck in repetitive loops</li> <li>Early mistakes cannot be corrected</li> <li>Often produces dull or incomplete responses</li> <li>Poor performance for long or open-ended generation</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#32-beam-search","title":"3.2 Beam Search","text":""},{"location":"decoding_strategies/decoding_strategies/#pros_1","title":"Pros","text":"<ul> <li>Optimizes global sequence likelihood</li> <li>Reduces early local decision errors</li> <li>Produces fluent and grammatically correct text</li> <li>Effective for tasks with a single correct output</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#cons_1","title":"Cons","text":"<ul> <li>Computationally expensive</li> <li>Produces generic and safe outputs</li> <li>Very low diversity</li> <li>All beams often converge to similar sequences</li> <li>Performs poorly for dialogue and creative tasks</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#33-temperature-sampling","title":"3.3 Temperature Sampling","text":""},{"location":"decoding_strategies/decoding_strategies/#pros_2","title":"Pros","text":"<ul> <li>Simple and intuitive control over randomness</li> <li>Enables creative and diverse outputs</li> <li>Easy to combine with other sampling methods</li> <li>Useful for brainstorming and storytelling</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#cons_2","title":"Cons","text":"<ul> <li>High temperature can cause incoherent text</li> <li>Low temperature collapses to greedy behavior</li> <li>Does not prevent sampling of very unlikely tokens</li> <li>Sensitive to temperature tuning</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#34-top-k-sampling","title":"3.4 Top-k Sampling","text":""},{"location":"decoding_strategies/decoding_strategies/#pros_3","title":"Pros","text":"<ul> <li>Prevents extremely low-probability tokens</li> <li>Provides controlled randomness</li> <li>Simple to implement</li> <li>More diverse than greedy and beam search</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#cons_3","title":"Cons","text":"<ul> <li>Fixed K ignores distribution shape</li> <li>Sensitive to the choice of K</li> <li>Removes valid tokens in flat distributions</li> <li>Not adaptive to model confidence</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#35-top-p-sampling-nucleus-sampling","title":"3.5 Top-p Sampling (Nucleus Sampling)","text":""},{"location":"decoding_strategies/decoding_strategies/#pros_4","title":"Pros","text":"<ul> <li>Adapts automatically to model confidence</li> <li>Better diversity-quality tradeoff than top-k</li> <li>Stable across different prompts</li> <li>Widely used in modern chat models</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#cons_4","title":"Cons","text":"<ul> <li>Slightly more complex than top-k</li> <li>Still stochastic and non-deterministic</li> <li>Can include many tokens in very flat distributions</li> <li>Less suitable for strictly deterministic tasks</li> </ul>"},{"location":"decoding_strategies/decoding_strategies/#4-high-level-comparison","title":"4. High-level Comparison","text":"Strategy Diversity Determinism Adaptivity Typical Usage Greedy Very low High No Baselines, debugging Beam Search Low Medium No Translation, ASR Temperature Medium to high Low Partial Creative text Top-k Medium Low No General generation Top-p Medium to high Low Yes Chat and dialogue"},{"location":"decoding_strategies/greedy_decoding/","title":"Greedy Decoding - Interview Prep Guide","text":""},{"location":"decoding_strategies/greedy_decoding/#1-overview","title":"1. Overview","text":"<p>Greedy decoding selects the token with the highest probability at each step during autoregressive text generation. It's the simplest and fastest decoding strategy but lacks diversity.</p> <p>Key insight: Local optimization (best token at each step) doesn't guarantee global optimality (best overall sequence).</p>"},{"location":"decoding_strategies/greedy_decoding/#2-how-it-works","title":"2. How It Works","text":""},{"location":"decoding_strategies/greedy_decoding/#algorithm","title":"Algorithm","text":"<pre><code>def greedy_decode(model, prompt, max_length):\n    tokens = prompt\n    for _ in range(max_length):\n        logits = model(tokens)  # Shape: (vocab_size,)\n        next_token = argmax(logits)  # Pick highest probability\n        tokens.append(next_token)\n        if next_token == EOS:\n            break\n    return tokens\n</code></pre>"},{"location":"decoding_strategies/greedy_decoding/#example","title":"Example","text":"<p>Given probability distribution after \"The cat sat on the\":</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 <p>Greedy choice: <code>mat</code> (highest probability)</p> <p>Output: \"The cat sat on the mat\"</p>"},{"location":"decoding_strategies/greedy_decoding/#3-key-characteristics","title":"3. Key Characteristics","text":""},{"location":"decoding_strategies/greedy_decoding/#deterministic","title":"Deterministic","text":"<ul> <li>Same input \u2192 always same output</li> <li>No randomness involved</li> <li>Fully reproducible</li> </ul>"},{"location":"decoding_strategies/greedy_decoding/#myopic-short-sighted","title":"Myopic (Short-sighted)","text":"<p>Each decision is locally optimal but may lead to suboptimal sequences:</p> <pre><code>Step 1: \"I think\" \u2192 model assigns:\n  - \"that\" (0.6)\n  - \"the\" (0.4)\n\nGreedy picks: \"that\"\n\nStep 2: \"I think that\" \u2192 model assigns:\n  - \"is\" (0.3)\n  - \"maybe\" (0.25)\n\nBut if we had picked \"the\" at step 1:\n  \"I think the\" \u2192 \"best\" (0.7)\n\nFinal: \"I think that is...\" (score: 0.6 \u00d7 0.3 = 0.18)\nBetter: \"I think the best...\" (score: 0.4 \u00d7 0.7 = 0.28)\n</code></pre> <p>Greedy can't recover from early suboptimal choices.</p>"},{"location":"decoding_strategies/greedy_decoding/#4-common-problems","title":"4. Common Problems","text":""},{"location":"decoding_strategies/greedy_decoding/#problem-1-repetition-loops","title":"Problem 1: Repetition Loops","text":"<pre><code>Prompt: \"To be or not to\"\nOutput: \"be or not to be or not to be or not to be...\"\n</code></pre> <p>Why: If \"be\" has slightly higher probability than alternatives at each step, greedy gets stuck.</p>"},{"location":"decoding_strategies/greedy_decoding/#problem-2-generic-outputs","title":"Problem 2: Generic Outputs","text":"<pre><code>Prompt: \"Write a creative story about\"\nGreedy: \"a boy who lived in a small town and went to school...\"\n</code></pre> <p>Always picks safe, high-probability continuations \u2192 boring text.</p>"},{"location":"decoding_strategies/greedy_decoding/#problem-3-early-mistakes-propagate","title":"Problem 3: Early Mistakes Propagate","text":"<pre><code>Prompt: \"The capital of France is\"\nIf model assigns:\n  - \"Paris\" (0.45)\n  - \"Lyon\" (0.46)  \u2190 greedy picks this (wrong!)\n\nOutput: \"Lyon, which is known for...\"\n</code></pre> <p>Once the wrong token is chosen, subsequent tokens try to justify it.</p>"},{"location":"decoding_strategies/greedy_decoding/#5-when-to-use-greedy-decoding","title":"5. When to Use Greedy Decoding","text":""},{"location":"decoding_strategies/greedy_decoding/#good-use-cases","title":"\u2705 Good Use Cases","text":"<p>Deterministic tasks: - Math problem solving - Code generation (when exact output matters) - Factual Q&amp;A with clear answers - Translation of standardized text</p> <p>Debugging: - Baseline comparisons - Reproducible testing - Fastest inference for quick experiments</p> <p>Confident models: - When model probability distributions are very peaked - Single obvious correct answer</p>"},{"location":"decoding_strategies/greedy_decoding/#poor-use-cases","title":"\u274c Poor Use Cases","text":"<p>Creative tasks: - Story writing - Poetry generation - Brainstorming</p> <p>Conversational AI: - Chatbots - Dialogue systems - Personality-driven responses</p> <p>Long-form generation: - Articles, essays - Open-ended content</p>"},{"location":"decoding_strategies/greedy_decoding/#6-practical-considerations","title":"6. Practical Considerations","text":""},{"location":"decoding_strategies/greedy_decoding/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Time: O(T \u00d7 V) where T = sequence length, V = vocab size</li> <li>Space: O(1) for decoding state (minimal memory)</li> <li>Fastest among all decoding strategies</li> </ul>"},{"location":"decoding_strategies/greedy_decoding/#modifications","title":"Modifications","text":"<p>Greedy + repetition penalty: <pre><code>def greedy_with_penalty(logits, previous_tokens, penalty=1.2):\n    for token in previous_tokens:\n        logits[token] /= penalty  # Reduce probability of repeated tokens\n    return argmax(logits)\n</code></pre></p> <p>Greedy + length normalization: Useful when comparing sequences of different lengths (not during decoding itself).</p>"},{"location":"decoding_strategies/greedy_decoding/#7-interview-questions","title":"7. Interview Questions","text":""},{"location":"decoding_strategies/greedy_decoding/#q1-what-is-greedy-decoding-and-how-does-it-work","title":"Q1: What is greedy decoding and how does it work?","text":"<p>Answer: Greedy decoding selects the token with the highest probability at each generation step. It's deterministic and fast but locally optimal\u2014it picks the best next token without considering whether this leads to the best overall sequence.</p>"},{"location":"decoding_strategies/greedy_decoding/#q2-why-is-greedy-decoding-called-greedy","title":"Q2: Why is greedy decoding called \"greedy\"?","text":"<p>Answer: It's \"greedy\" because it makes the locally optimal choice at each step (highest probability token) without considering future consequences. Like the greedy algorithm paradigm, it optimizes immediate reward rather than global optimality.</p>"},{"location":"decoding_strategies/greedy_decoding/#q3-whats-the-main-disadvantage-of-greedy-decoding","title":"Q3: What's the main disadvantage of greedy decoding?","text":"<p>Answer: Lack of diversity and repetition. Greedy often produces repetitive, generic text because it can't explore alternative paths. Once it makes a suboptimal choice, it can't backtrack, leading to error propagation and repetitive loops.</p>"},{"location":"decoding_strategies/greedy_decoding/#q4-how-does-greedy-decoding-differ-from-beam-search","title":"Q4: How does greedy decoding differ from beam search?","text":"<p>Answer: - Greedy: Keeps only 1 hypothesis (best token at each step) - Beam search: Keeps K hypotheses (top-K sequences), explores multiple paths</p> <p>Beam search can recover from locally suboptimal choices by considering alternative sequences. Greedy cannot.</p>"},{"location":"decoding_strategies/greedy_decoding/#q5-can-greedy-decoding-produce-the-optimal-sequence","title":"Q5: Can greedy decoding produce the optimal sequence?","text":"<p>Answer: Not necessarily. Greedy finds a locally optimal sequence but not necessarily globally optimal. The highest-probability sequence might require choosing a lower-probability token early on that leads to much higher probabilities later.</p>"},{"location":"decoding_strategies/greedy_decoding/#q6-why-does-greedy-decoding-cause-repetition","title":"Q6: Why does greedy decoding cause repetition?","text":"<p>Answer: If a token or phrase has slightly higher probability than alternatives, greedy will keep selecting it. For example, in \"to be or not to be or not to be...\", if \"be\" consistently has a small probability advantage, the model gets stuck in a loop with no mechanism to escape.</p>"},{"location":"decoding_strategies/greedy_decoding/#q7-how-can-you-reduce-repetition-in-greedy-decoding","title":"Q7: How can you reduce repetition in greedy decoding?","text":"<p>Answer: Common techniques: 1. Repetition penalty: Divide logits of previously generated tokens by penalty factor (e.g., 1.2) 2. N-gram blocking: Prevent repeating same N-grams 3. Switch to sampling: Use temperature/top-p for diversity 4. Beam search: Explore multiple paths to avoid local minima</p>"},{"location":"decoding_strategies/greedy_decoding/#q8-whats-the-computational-cost-of-greedy-decoding","title":"Q8: What's the computational cost of greedy decoding?","text":"<p>Answer: Very efficient: - Per step: O(V) to find argmax over vocabulary - Total: O(T \u00d7 V) for T tokens - Memory: O(1) for decoding state - Fastest decoding strategy, no beam/sample storage overhead</p>"},{"location":"decoding_strategies/greedy_decoding/#q9-when-would-you-prefer-greedy-over-beam-search","title":"Q9: When would you prefer greedy over beam search?","text":"<p>Answer: - Latency-critical applications (greedy is much faster) - Deterministic outputs required (reproducibility) - Single clear answer (factual Q&amp;A, simple math) - Resource-constrained environments (minimal memory)</p> <p>Beam search is overkill when diversity isn't needed and the model is confident.</p>"},{"location":"decoding_strategies/greedy_decoding/#q10-how-does-greedy-decoding-interact-with-temperature","title":"Q10: How does greedy decoding interact with temperature?","text":"<p>Answer: Greedy decoding ignores temperature because it always picks argmax. Temperature only affects sampling-based methods. You can think of greedy as temperature \u2192 0 (infinitely peaked distribution where highest probability \u2192 1.0, others \u2192 0).</p>"},{"location":"decoding_strategies/greedy_decoding/#8-comparison-with-other-methods","title":"8. Comparison with Other Methods","text":"Aspect Greedy Beam Search Sampling Speed Fastest Slow (K\u00d7 cost) Fast Diversity None Low High Determinism Yes Yes No Repetition High risk Medium risk Low risk Quality Variable Higher Variable Memory Minimal O(K \u00d7 T) Minimal"},{"location":"decoding_strategies/greedy_decoding/#9-code-example","title":"9. Code Example","text":"<pre><code>import torch\n\ndef greedy_decode(model, input_ids, max_length=50):\n    \"\"\"\n    Simple greedy decoding implementation.\n\n    Args:\n        model: Language model with forward() method\n        input_ids: Starting token IDs (tensor)\n        max_length: Maximum sequence length\n\n    Returns:\n        Generated token IDs\n    \"\"\"\n    generated = input_ids.clone()\n\n    for _ in range(max_length):\n        # Get logits for next token\n        with torch.no_grad():\n            outputs = model(generated)\n            logits = outputs.logits[:, -1, :]  # Last token logits\n\n        # Greedy selection\n        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n        # Append to sequence\n        generated = torch.cat([generated, next_token], dim=1)\n\n        # Check for EOS\n        if next_token.item() == model.config.eos_token_id:\n            break\n\n    return generated\n\n# Usage\n# output = greedy_decode(model, prompt_tokens, max_length=100)\n</code></pre>"},{"location":"decoding_strategies/greedy_decoding/#10-key-takeaways-for-interviews","title":"10. Key Takeaways for Interviews","text":"<ol> <li>Definition: Always picks the highest probability token at each step</li> <li>Pros: Fast, deterministic, simple, reproducible</li> <li>Cons: No diversity, repetitive, locally optimal only</li> <li>Main problem: Repetition loops and generic outputs</li> <li>Best for: Deterministic tasks, debugging, factual Q&amp;A</li> <li>Worst for: Creative writing, conversations, long-form generation</li> <li>Complexity: O(T \u00d7 V) time, O(1) space for decoding</li> </ol>"},{"location":"decoding_strategies/greedy_decoding/#references","title":"References","text":"<ul> <li>The Curious Case of Neural Text Degeneration - Discusses repetition issues</li> <li>Hugging Face: Generation Strategies</li> </ul>"},{"location":"decoding_strategies/sampling_methods/","title":"Sampling Methods - Interview Prep Guide","text":""},{"location":"decoding_strategies/sampling_methods/#1-overview","title":"1. Overview","text":"<p>Sampling methods introduce controlled randomness into text generation by probabilistically selecting tokens rather than deterministically choosing the highest-probability token. This enables diverse, creative outputs while maintaining coherence.</p> <p>Key insight: The best sequence isn't always the highest-probability one\u2014controlled randomness can produce more natural, interesting text.</p> <p>Three main techniques: 1. Temperature sampling - Controls randomness via scaling 2. Top-k sampling - Samples from top K tokens 3. Top-p sampling (nucleus) - Samples from smallest set with cumulative probability \u2265 p</p>"},{"location":"decoding_strategies/sampling_methods/#2-temperature-sampling","title":"2. Temperature Sampling","text":""},{"location":"decoding_strategies/sampling_methods/#21-how-it-works","title":"2.1 How It Works","text":"<p>Temperature scales logits before applying softmax, controlling distribution \"sharpness\":</p> \\[\\text{P}_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\\] <p>Where: - \\(z_i\\) = logit for token i - \\(T\\) = temperature (T &gt; 0) - \\(T = 1\\) \u2192 original distribution - \\(T &lt; 1\\) \u2192 sharper (more deterministic) - \\(T &gt; 1\\) \u2192 flatter (more random)</p>"},{"location":"decoding_strategies/sampling_methods/#22-example","title":"2.2 Example","text":"<p>Original probabilities after \"The cat sat on the\":</p> Token Prob (T=1) Prob (T=0.5) Prob (T=2.0) mat 0.40 0.58 0.28 floor 0.25 0.26 0.23 sofa 0.15 0.10 0.18 bed 0.10 0.04 0.14 roof 0.05 0.01 0.09 moon 0.03 0.00 0.05 pizza 0.02 0.00 0.03 <p>T=0.5 (Low): Distribution sharpens \u2192 \"mat\" dominates \u2192 near-greedy behavior T=2.0 (High): Distribution flattens \u2192 \"moon\", \"pizza\" become viable \u2192 more random</p>"},{"location":"decoding_strategies/sampling_methods/#23-extreme-cases","title":"2.3 Extreme Cases","text":"<p>T \u2192 0: - Distribution becomes one-hot (probability \u2192 1.0 for argmax) - Equivalent to greedy decoding - Zero randomness</p> <p>T \u2192 \u221e: - Uniform distribution (all tokens equally likely) - Maximum randomness - Often produces gibberish</p> <p>Typical values: - T=0.7: Focused, coherent (factual Q&amp;A) - T=1.0: Balanced (default) - T=1.2-1.5: Creative, diverse (story writing)</p>"},{"location":"decoding_strategies/sampling_methods/#24-code-example","title":"2.4 Code Example","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\ndef temperature_sampling(logits, temperature=1.0):\n    \"\"\"\n    Sample token with temperature scaling.\n\n    Args:\n        logits: Raw model outputs [vocab_size]\n        temperature: Scaling factor (T &gt; 0)\n\n    Returns:\n        Sampled token ID\n    \"\"\"\n    # Scale logits\n    scaled_logits = logits / temperature\n\n    # Convert to probabilities\n    probs = F.softmax(scaled_logits, dim=-1)\n\n    # Sample\n    token_id = torch.multinomial(probs, num_samples=1)\n    return token_id.item()\n\n# Usage\n# next_token = temperature_sampling(model_logits, temperature=1.2)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#3-top-k-sampling","title":"3. Top-k Sampling","text":""},{"location":"decoding_strategies/sampling_methods/#31-how-it-works","title":"3.1 How It Works","text":"<p>Top-k restricts sampling to the k most probable tokens:</p> <ol> <li>Sort tokens by probability (descending)</li> <li>Keep only top-k tokens</li> <li>Set all other probabilities to zero</li> <li>Renormalize the top-k probabilities</li> <li>Sample from renormalized distribution</li> </ol>"},{"location":"decoding_strategies/sampling_methods/#32-example","title":"3.2 Example","text":"<p>Probabilities after \"The cat sat on the\":</p> Token Probability mat 0.40 floor 0.25 sofa 0.15 bed 0.10 roof 0.05 moon 0.03 pizza 0.02 <p>Top-k with k=3:</p> <p>Kept tokens: - mat: 0.40 - floor: 0.25 - sofa: 0.15</p> <p>Renormalized: - mat: 0.40/0.80 = 0.50 - floor: 0.25/0.80 = 0.31 - sofa: 0.15/0.80 = 0.19</p> <p>Removed: bed, roof, moon, pizza</p> <p>Output: One of {mat, floor, sofa} with renormalized probabilities</p>"},{"location":"decoding_strategies/sampling_methods/#33-key-limitation-fixed-k","title":"3.3 Key Limitation: Fixed K","text":"<p>Top-k doesn't adapt to distribution shape:</p> <p>Case 1: Confident model - Probabilities: [0.85, 0.07, 0.03, 0.02, 0.02, 0.01] - k=5 \u2192 keeps 5 tokens even though model is very confident - Inefficient: forces sampling from low-quality tokens</p> <p>Case 2: Uncertain model - Probabilities: [0.20, 0.20, 0.20, 0.20, 0.20] - k=3 \u2192 keeps only 3 tokens, excludes equally valid options - Too restrictive: removes valid choices</p> <p>Problem: Same k value for different distribution shapes.</p>"},{"location":"decoding_strategies/sampling_methods/#34-typical-values","title":"3.4 Typical Values","text":"<ul> <li>k=10-20: Conservative, relatively safe outputs</li> <li>k=40-50: More diverse, creative</li> <li>k=100+: Very random, may include low-quality tokens</li> </ul>"},{"location":"decoding_strategies/sampling_methods/#35-code-example","title":"3.5 Code Example","text":"<pre><code>import torch\n\ndef top_k_sampling(logits, k=50, temperature=1.0):\n    \"\"\"\n    Sample from top-k tokens.\n\n    Args:\n        logits: Raw model outputs [vocab_size]\n        k: Number of top tokens to consider\n        temperature: Optional temperature scaling\n\n    Returns:\n        Sampled token ID\n    \"\"\"\n    # Apply temperature\n    scaled_logits = logits / temperature\n\n    # Get top-k\n    top_k_logits, top_k_indices = torch.topk(scaled_logits, k)\n\n    # Softmax over top-k\n    probs = F.softmax(top_k_logits, dim=-1)\n\n    # Sample from top-k\n    sampled_idx = torch.multinomial(probs, num_samples=1)\n\n    # Map back to original vocabulary\n    token_id = top_k_indices[sampled_idx]\n    return token_id.item()\n\n# Usage\n# next_token = top_k_sampling(model_logits, k=40, temperature=0.9)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#4-top-p-sampling-nucleus-sampling","title":"4. Top-p Sampling (Nucleus Sampling)","text":""},{"location":"decoding_strategies/sampling_methods/#41-how-it-works","title":"4.1 How It Works","text":"<p>Top-p selects the smallest set of tokens whose cumulative probability \u2265 p:</p> <ol> <li>Sort tokens by probability (descending)</li> <li>Compute cumulative probability</li> <li>Keep tokens until cumulative \u2265 p</li> <li>Renormalize and sample</li> </ol> <p>Key advantage: Adaptive\u2014number of tokens varies based on distribution.</p>"},{"location":"decoding_strategies/sampling_methods/#42-example","title":"4.2 Example","text":"<p>Probabilities after \"The cat sat on the\":</p> Token Probability Cumulative mat 0.40 0.40 floor 0.25 0.65 sofa 0.15 0.80 bed 0.10 0.90 roof 0.05 0.95 moon 0.03 0.98 pizza 0.02 1.00 <p>Top-p with p=0.9: - Keep: mat, floor, sofa, bed (cumulative = 0.90) - Remove: roof, moon, pizza - Effective k = 4</p>"},{"location":"decoding_strategies/sampling_methods/#43-adaptive-behavior","title":"4.3 Adaptive Behavior","text":"<p>Case 1: Confident model <pre><code>Probabilities: [0.85, 0.07, 0.03, 0.02, 0.02, 0.01]\np=0.9 \u2192 keeps 2 tokens [0.85, 0.07]\nEffective k = 2 (adaptive reduction)\n</code></pre></p> <p>Case 2: Uncertain model <pre><code>Probabilities: [0.20, 0.20, 0.20, 0.20, 0.20]\np=0.9 \u2192 keeps 5 tokens\nEffective k = 5 (adaptive expansion)\n</code></pre></p> <p>Advantage over top-k: Automatically adjusts to model confidence.</p>"},{"location":"decoding_strategies/sampling_methods/#44-typical-values","title":"4.4 Typical Values","text":"<ul> <li>p=0.9: Standard for most applications (OpenAI default)</li> <li>p=0.95: More diverse, creative outputs</li> <li>p=0.75-0.85: More focused, conservative</li> </ul>"},{"location":"decoding_strategies/sampling_methods/#45-code-example","title":"4.5 Code Example","text":"<pre><code>import torch\n\ndef top_p_sampling(logits, p=0.9, temperature=1.0):\n    \"\"\"\n    Nucleus sampling (top-p).\n\n    Args:\n        logits: Raw model outputs [vocab_size]\n        p: Cumulative probability threshold (0 &lt; p \u2264 1)\n        temperature: Optional temperature scaling\n\n    Returns:\n        Sampled token ID\n    \"\"\"\n    # Apply temperature\n    scaled_logits = logits / temperature\n    probs = F.softmax(scaled_logits, dim=-1)\n\n    # Sort probabilities\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n\n    # Compute cumulative probabilities\n    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n\n    # Find cutoff: first position where cumulative &gt; p\n    # Include that position (so cumulative &gt;= p)\n    sorted_indices_to_remove = cumulative_probs &gt; p\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = False\n\n    # Mask out tokens to remove\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        0, sorted_indices, sorted_indices_to_remove\n    )\n    filtered_logits = scaled_logits.clone()\n    filtered_logits[indices_to_remove] = float('-inf')\n\n    # Sample from filtered distribution\n    filtered_probs = F.softmax(filtered_logits, dim=-1)\n    token_id = torch.multinomial(filtered_probs, num_samples=1)\n    return token_id.item()\n\n# Usage\n# next_token = top_p_sampling(model_logits, p=0.9, temperature=1.0)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#5-combining-sampling-methods","title":"5. Combining Sampling Methods","text":"<p>In practice, sampling methods are often combined:</p>"},{"location":"decoding_strategies/sampling_methods/#common-combination-temperature-top-p","title":"Common Combination: Temperature + Top-p","text":"<pre><code>def sample_token(logits, temperature=0.9, top_p=0.9):\n    # 1. Apply temperature first\n    scaled_logits = logits / temperature\n\n    # 2. Then apply top-p filtering\n    token = top_p_sampling(scaled_logits, p=top_p)\n    return token\n</code></pre> <p>Why combine: - Temperature controls overall randomness - Top-p prevents sampling from the very long tail - Together: controlled creativity with safety</p>"},{"location":"decoding_strategies/sampling_methods/#other-combinations","title":"Other Combinations","text":"<p>Temperature + Top-k: <pre><code># Used in some older systems\ntoken = top_k_sampling(logits, k=40, temperature=0.8)\n</code></pre></p> <p>Top-k + Top-p: <pre><code># Apply top-k first as a hard cutoff\n# Then apply top-p for adaptive filtering\n# Less common, top-p usually sufficient\n</code></pre></p>"},{"location":"decoding_strategies/sampling_methods/#6-when-to-use-each-method","title":"6. When to Use Each Method","text":""},{"location":"decoding_strategies/sampling_methods/#temperature-sampling","title":"Temperature Sampling","text":"<p>\u2705 Use when: - Need simple randomness control - Working with other filtering methods (top-k, top-p) - Want smooth transition between deterministic and random</p> <p>\u274c Avoid when: - Used alone (no filtering from tail) - Need adaptive behavior</p>"},{"location":"decoding_strategies/sampling_methods/#top-k-sampling","title":"Top-k Sampling","text":"<p>\u2705 Use when: - Need simple, predictable diversity control - Fixed computational budget (always k tokens) - Legacy systems (older standard)</p> <p>\u274c Avoid when: - Distribution shape varies significantly - Need adaptive behavior - Modern systems (top-p preferred)</p>"},{"location":"decoding_strategies/sampling_methods/#top-p-sampling","title":"Top-p Sampling","text":"<p>\u2705 Use when: - Need adaptive diversity - Model confidence varies - General-purpose text generation - Conversational AI, creative writing</p> <p>\u274c Avoid when: - Need strict determinism - Computational constraints (slightly more complex)</p>"},{"location":"decoding_strategies/sampling_methods/#7-interview-questions","title":"7. Interview Questions","text":""},{"location":"decoding_strategies/sampling_methods/#q1-what-problem-do-sampling-methods-solve","title":"Q1: What problem do sampling methods solve?","text":"<p>Answer: Greedy and beam search produce repetitive, generic text by always choosing high-probability tokens. Sampling methods introduce controlled randomness, enabling diverse, creative outputs while preventing the model from getting stuck in repetitive loops. They balance coherence with variety.</p>"},{"location":"decoding_strategies/sampling_methods/#q2-how-does-temperature-affect-the-probability-distribution","title":"Q2: How does temperature affect the probability distribution?","text":"<p>Answer: Temperature scales logits before softmax. Low temperature (T&lt;1) sharpens the distribution\u2014high-probability tokens become more dominant (near-greedy). High temperature (T&gt;1) flattens the distribution\u2014low-probability tokens become more likely (more random). At T\u21920, it becomes greedy; at T\u2192\u221e, it becomes uniform.</p>"},{"location":"decoding_strategies/sampling_methods/#q3-whats-the-main-limitation-of-top-k-sampling","title":"Q3: What's the main limitation of top-k sampling?","text":"<p>Answer: Fixed k doesn't adapt to distribution shape. When the model is very confident, k=50 wastes computation on unlikely tokens. When uncertain with many valid options, k=50 might exclude good alternatives. Top-k treats all distributions the same, ignoring the model's confidence level.</p>"},{"location":"decoding_strategies/sampling_methods/#q4-how-is-top-p-better-than-top-k","title":"Q4: How is top-p better than top-k?","text":"<p>Answer: Top-p is adaptive\u2014it automatically adjusts the number of candidate tokens based on distribution shape: - Confident model \u2192 keeps fewer tokens (smaller effective k) - Uncertain model \u2192 keeps more tokens (larger effective k)</p> <p>This makes top-p more robust across different contexts without hyperparameter tuning.</p>"},{"location":"decoding_strategies/sampling_methods/#q5-can-you-use-temperature05-with-top-p09-together","title":"Q5: Can you use temperature=0.5 with top-p=0.9 together?","text":"<p>Answer: Yes, and this is common in practice: 1. Apply temperature=0.5 first \u2192 sharpen distribution (reduce randomness) 2. Apply top-p=0.9 \u2192 filter out low-probability tail 3. Sample from filtered distribution</p> <p>Temperature controls overall randomness, top-p prevents sampling gibberish. Together they provide controlled, safe creativity.</p>"},{"location":"decoding_strategies/sampling_methods/#q6-why-not-just-use-temperature-alone","title":"Q6: Why not just use temperature alone?","text":"<p>Answer: Temperature alone doesn't filter out low-probability tokens\u2014it just reduces their probability. Even with low temperature, there's still a tiny chance of sampling nonsense tokens from the very long tail. Top-p/top-k provide a hard cutoff, ensuring we never sample from clearly bad options.</p>"},{"location":"decoding_strategies/sampling_methods/#q7-what-happens-with-very-high-temperature-t5","title":"Q7: What happens with very high temperature (T=5)?","text":"<p>Answer: The distribution becomes nearly uniform\u2014all tokens have similar probability regardless of model's original confidence. This produces incoherent gibberish:</p> <pre><code>Input: \"The capital of France is\"\nOutput: \"banana quantum seventh pencil\"\n</code></pre> <p>High temperature destroys the model's learned knowledge. Typical max: T=1.5-2.0.</p>"},{"location":"decoding_strategies/sampling_methods/#q8-how-do-you-choose-between-p09-vs-p095","title":"Q8: How do you choose between p=0.9 vs p=0.95?","text":"<p>Answer: - p=0.9: More focused, coherent (default for most applications) - p=0.95: More diverse, creative (for storytelling, brainstorming)</p> <p>Trade-off: Higher p \u2192 more diversity but higher risk of incoherence. Start with 0.9; increase for creativity, decrease for safety. Depends on task requirements.</p>"},{"location":"decoding_strategies/sampling_methods/#q9-whats-the-computational-cost-of-top-p-vs-top-k","title":"Q9: What's the computational cost of top-p vs top-k?","text":"<p>Answer: - Top-k: O(V log k) \u2014 partial sort for top-k elements - Top-p: O(V log V) \u2014 full sort to compute cumulative probabilities</p> <p>Top-p is slightly more expensive, but the difference is negligible compared to model inference cost. The adaptive benefits of top-p outweigh the small computational overhead.</p>"},{"location":"decoding_strategies/sampling_methods/#q10-why-do-modern-llms-gpt-4-claude-prefer-top-p-over-top-k","title":"Q10: Why do modern LLMs (GPT-4, Claude) prefer top-p over top-k?","text":"<p>Answer: Adaptivity and robustness. Top-p automatically adjusts to: - Different contexts (formal vs casual) - Varying model confidence - Different domains (technical vs creative)</p> <p>This makes it more reliable across diverse use cases without manual tuning. Top-k requires choosing k for each scenario, while top-p with p=0.9 works well universally.</p>"},{"location":"decoding_strategies/sampling_methods/#8-comparison-table","title":"8. Comparison Table","text":"Method Randomness Control Adaptive Prevents Tail Sampling Typical Usage Complexity Temperature Continuous (T) No No + top-p/top-k O(V) Top-k Discrete (k) No Yes Legacy, simple control O(V log k) Top-p Continuous (p) Yes Yes Modern LLMs, general use O(V log V) Greedy None N/A N/A Debugging, deterministic O(V) Beam None No N/A Translation, ASR O(K\u00d7V)"},{"location":"decoding_strategies/sampling_methods/#9-practical-guidelines","title":"9. Practical Guidelines","text":""},{"location":"decoding_strategies/sampling_methods/#for-most-applications","title":"For Most Applications","text":"<pre><code># Recommended defaults\ntemperature = 0.8-1.0\ntop_p = 0.9\n# Don't use top-k with top-p\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#for-creative-writing","title":"For Creative Writing","text":"<pre><code>temperature = 1.0-1.5\ntop_p = 0.95\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#for-factual-qa","title":"For Factual Q&amp;A","text":"<pre><code>temperature = 0.3-0.7\ntop_p = 0.9\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#for-code-generation","title":"For Code Generation","text":"<pre><code>temperature = 0.2-0.5\ntop_p = 0.9\n# Or just use greedy (temperature=0)\n</code></pre>"},{"location":"decoding_strategies/sampling_methods/#10-key-takeaways-for-interviews","title":"10. Key Takeaways for Interviews","text":"<ol> <li>Temperature: Controls randomness by scaling logits; T&lt;1 sharper, T&gt;1 flatter</li> <li>Top-k: Fixed number of candidate tokens; simple but not adaptive</li> <li>Top-p: Adaptive cumulative probability threshold; modern standard</li> <li>Combination: Use temperature + top-p together for best results</li> <li>Top-p advantage: Automatically adjusts to model confidence</li> <li>Common values: temperature=0.8-1.0, top-p=0.9</li> <li>Use case: Sampling for creative/diverse tasks; greedy/beam for deterministic tasks</li> </ol>"},{"location":"decoding_strategies/sampling_methods/#references","title":"References","text":"<ul> <li>The Curious Case of Neural Text Degeneration - Introduces nucleus (top-p) sampling</li> <li>Hugging Face: Generation Strategies</li> <li>OpenAI API: Sampling Parameters</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/","title":"Speculative Decoding - Interview Prep Guide","text":""},{"location":"decoding_strategies/speculative_decoding/#1-overview","title":"1. Overview","text":"<p>Speculative decoding reduces inference latency in autoregressive LLMs by using a small draft model to propose multiple tokens that a large target model verifies in parallel. It produces exact same output distribution as standard decoding\u2014not an approximation.</p> <p>Key insight: Instead of 1 token per expensive model call, verify K tokens in one call by having a cheap model guess ahead.</p> <p>Typical speedup: 2-3x latency reduction with no quality loss</p>"},{"location":"decoding_strategies/speculative_decoding/#2-the-problem-with-standard-decoding","title":"2. The Problem with Standard Decoding","text":""},{"location":"decoding_strategies/speculative_decoding/#sequential-bottleneck","title":"Sequential Bottleneck","text":"<p>Autoregressive generation is inherently sequential:</p> \\[ P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{&lt;t}) \\] <p>Standard decoding loop: <pre><code>for _ in range(max_length):\n    logits = large_model(tokens)      # Expensive!\n    next_token = sample(logits[-1])   # Only use last position\n    tokens.append(next_token)         # Generate 1 token\n</code></pre></p> <p>Problems: - Generate 1 token per forward pass - Model sits mostly idle (memory-bound, not compute-bound) - Latency grows linearly with output length - KV cache helps computation but doesn't break sequential dependency</p> <p>Example: For 100 tokens, need 100 expensive forward passes of the large model.</p>"},{"location":"decoding_strategies/speculative_decoding/#3-how-speculative-decoding-works","title":"3. How Speculative Decoding Works","text":""},{"location":"decoding_strategies/speculative_decoding/#core-idea","title":"Core Idea","text":"<p>Separate token proposal from verification:</p> <ol> <li>Draft model (small, fast): Proposes K tokens autoregressively</li> <li>Target model (large, expensive): Verifies all K tokens in one parallel forward pass</li> <li>Accept/reject: Use rejection sampling to maintain correct distribution</li> </ol>"},{"location":"decoding_strategies/speculative_decoding/#visual-example","title":"Visual Example","text":"<pre><code>Prompt: \"The capital of France is\"\n\nDraft model proposes:  \"Paris , which is\"  (K=4 tokens)\n                        \u2193\nTarget model verifies all 4 tokens in one pass\n                        \u2193\nAccept? [Paris: \u2713] [,: \u2713] [which: \u2713] [is: \u2717]\n                        \u2193\nOutput: \"The capital of France is Paris , which\"\nContinue from \"which\" (3 tokens in 1 target pass instead of 3!)\n</code></pre>"},{"location":"decoding_strategies/speculative_decoding/#4-step-by-step-algorithm","title":"4. Step-by-Step Algorithm","text":""},{"location":"decoding_strategies/speculative_decoding/#setup","title":"Setup","text":"<ul> <li>Draft model \\(q\\): Small, fast (e.g., 1B params)</li> <li>Target model \\(p\\): Large, accurate (e.g., 70B params)</li> <li>Draft length \\(K\\): Number of tokens to propose (typically 4-8)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#step-1-draft-proposes-k-tokens","title":"Step 1: Draft Proposes K Tokens","text":"<p>Draft model generates K tokens autoregressively:</p> \\[ y_i \\sim q(\\cdot \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1, \\dots, K \\] <p>Important: Sample tokens (don't use greedy) and record \\(q(y_i \\mid x, y_{&lt;i})\\) for each.</p> <pre><code># Draft phase (cheap, K sequential calls to small model)\ndraft_tokens = []\ndraft_probs = []\n\nfor i in range(K):\n    logits = draft_model(prompt + draft_tokens)\n    probs = softmax(logits[-1])\n\n    token = sample(probs)  # Sample, not greedy!\n    draft_tokens.append(token)\n    draft_probs.append(probs[token])  # Record q(y_i)\n</code></pre>"},{"location":"decoding_strategies/speculative_decoding/#step-2-target-verifies-all-tokens","title":"Step 2: Target Verifies All Tokens","text":"<p>Target model runs once on the entire sequence:</p> \\[ \\text{Input: } [x, y_1, y_2, \\dots, y_K] \\] <p>This produces probabilities for all draft positions:</p> \\[ p(y_i \\mid x, y_{&lt;i}) \\quad \\text{for } i = 1, \\dots, K \\] <pre><code># Verification phase (1 parallel call to large model)\nfull_sequence = prompt + draft_tokens\ntarget_logits = target_model(full_sequence)  # Shape: [seq_len, vocab_size]\n\n# Extract logits for draft positions only\ntarget_probs = []\nfor i in range(K):\n    pos = len(prompt) + i - 1  # Position before token i\n    probs = softmax(target_logits[pos])\n    target_probs.append(probs[draft_tokens[i]])  # p(y_i)\n</code></pre> <p>Key: Transformer naturally computes logits for all positions\u2014speculative decoding just uses them.</p>"},{"location":"decoding_strategies/speculative_decoding/#step-3-accept-or-reject-each-token","title":"Step 3: Accept or Reject Each Token","text":"<p>Use rejection sampling to maintain correct distribution:</p> \\[ \\alpha_i = \\min\\left(1, \\frac{p(y_i \\mid x, y_{&lt;i})}{q(y_i \\mid x, y_{&lt;i})}\\right) \\] <pre><code>accepted = []\n\nfor i in range(K):\n    # Acceptance probability\n    acceptance_prob = min(1.0, target_probs[i] / draft_probs[i])\n\n    # Random test\n    if random.random() &lt; acceptance_prob:\n        accepted.append(draft_tokens[i])\n    else:\n        # First rejection: stop here\n        break\n</code></pre> <p>Stop at first rejection (can't accept token 3 if token 2 was rejected).</p>"},{"location":"decoding_strategies/speculative_decoding/#step-4-handle-rejection","title":"Step 4: Handle Rejection","text":"<p>If token \\(j\\) is rejected: - Discard \\(y_j, y_{j+1}, \\dots, y_K\\) - Sample replacement from target model at position \\(j\\):</p> \\[ x_{\\text{next}} \\sim p(\\cdot \\mid x, y_{&lt;j}) \\] <pre><code>if len(accepted) &lt; K:  # Some token was rejected\n    # Sample next token from target model\n    rejection_pos = len(accepted)\n    next_token = sample(target_logits[rejection_pos])\n    accepted.append(next_token)\n\n# Continue with accepted tokens\nprompt = prompt + accepted\n</code></pre> <p>If all K tokens accepted, continue with next draft batch.</p>"},{"location":"decoding_strategies/speculative_decoding/#5-why-its-faster","title":"5. Why It's Faster","text":""},{"location":"decoding_strategies/speculative_decoding/#speedup-analysis","title":"Speedup Analysis","text":"<p>Standard decoding for N tokens: - N forward passes of target model - Cost: \\(N \\times C_{\\text{target}}\\)</p> <p>Speculative decoding: - Draft proposes K tokens: \\(K \\times C_{\\text{draft}}\\) (cheap) - Target verifies in 1 pass: \\(1 \\times C_{\\text{target}}\\) - If acceptance rate = \\(\\alpha\\), generate \\(\\alpha \\times K\\) tokens per cycle</p> <p>Expected tokens per target call: $$ E[\\text{tokens}] = 1 + \\sum_{i=1}^{K} \\alpha^i \\approx \\frac{1 - \\alpha^{K+1}}{1 - \\alpha} $$</p> <p>Example: - \\(K=4\\), \\(\\alpha=0.7\\) \u2192 ~2.4 tokens per target call - 2.4\u00d7 speedup (vs 1 token per call in standard decoding)</p>"},{"location":"decoding_strategies/speculative_decoding/#why-parallel-verification-works","title":"Why Parallel Verification Works","text":"<p>Transformers compute logits for all positions in one pass: <pre><code>Input:  [\"The\", \"cat\", \"sat\"]\nOutput: [logits_0, logits_1, logits_2]\n         \u2193         \u2193         \u2193\n      P(\u00b7|\"The\") P(\u00b7|\"The cat\") P(\u00b7|\"The cat sat\")\n</code></pre></p> <p>Standard decoding only uses <code>logits_2</code> (last position). Speculative decoding uses <code>logits_0</code>, <code>logits_1</code>, <code>logits_2</code> to verify multiple tokens.</p> <p>This is not new capability\u2014it's how Transformers always work during training.</p>"},{"location":"decoding_strategies/speculative_decoding/#6-why-its-exact-not-approximate","title":"6. Why It's Exact (Not Approximate)","text":""},{"location":"decoding_strategies/speculative_decoding/#rejection-sampling-proof","title":"Rejection Sampling Proof","text":"<p>The acceptance rule: $$ \\alpha_i = \\min\\left(1, \\frac{p(y_i)}{q(y_i)}\\right) $$</p> <p>is standard rejection sampling:</p> <ul> <li>If \\(p(y_i) \\geq q(y_i)\\): always accept (draft underestimated)</li> <li>If \\(p(y_i) &lt; q(y_i)\\): accept with probability \\(p/q\\) (draft overestimated)</li> </ul> <p>Result: Accepted tokens have distribution \\(p\\), not \\(q\\).</p> <p>Mathematical guarantee: Output distribution is identical to sampling from target model directly.</p>"},{"location":"decoding_strategies/speculative_decoding/#when-rejection-happens","title":"When Rejection Happens","text":"<pre><code>Draft assigns:  q(\"Paris\") = 0.8\nTarget assigns: p(\"Paris\") = 0.9\n\u2192 Accept always (\u03b1 = min(1, 0.9/0.8) = 1.0)\n\nDraft assigns:  q(\"London\") = 0.6\nTarget assigns: p(\"London\") = 0.3\n\u2192 Accept 50% of time (\u03b1 = 0.3/0.6 = 0.5)\n</code></pre> <p>Rejection corrects for draft model errors while maintaining exact target distribution.</p>"},{"location":"decoding_strategies/speculative_decoding/#7-practical-considerations","title":"7. Practical Considerations","text":""},{"location":"decoding_strategies/speculative_decoding/#draft-model-selection","title":"Draft Model Selection","text":"<p>Options: 1. Smaller version of target (e.g., 1B vs 70B parameters) 2. Quantized target model (INT8 vs FP16) 3. Distilled model trained to match target 4. Early-exit from target (use intermediate layers)</p> <p>Requirements: - Fast enough (\u226510\u00d7 faster than target) - Similar enough to target (high acceptance rate)</p>"},{"location":"decoding_strategies/speculative_decoding/#draft-length-k","title":"Draft Length K","text":"<p>Trade-off: - Small K (2-4): Lower overhead, guaranteed speedup - Large K (8-16): Higher potential speedup, but more likely to reject</p> <p>Optimal K depends on: - Acceptance rate (higher \u03b1 \u2192 larger K beneficial) - Draft/target speed ratio - Memory constraints</p> <p>Typical: K=4-5 works well in practice</p>"},{"location":"decoding_strategies/speculative_decoding/#acceptance-rate","title":"Acceptance Rate","text":"<p>Factors affecting \u03b1: - Draft-target model similarity - Task complexity (simple text \u2192 higher \u03b1) - Prompt context (more context \u2192 better draft predictions)</p> <p>Typical rates: - Good draft: \u03b1=0.7-0.9 - Poor draft: \u03b1=0.3-0.5</p> <p>Below \u03b1\u22480.3: Speculative decoding may be slower than standard (overhead dominates).</p>"},{"location":"decoding_strategies/speculative_decoding/#8-interaction-with-kv-cache","title":"8. Interaction with KV Cache","text":""},{"location":"decoding_strategies/speculative_decoding/#kv-cache-in-both-models","title":"KV Cache in Both Models","text":"<p>Draft model: - Maintains its own KV cache - Generates K tokens autoregressively (K cache updates)</p> <p>Target model: - Computes KV cache for entire draft window in one pass - Accepted tokens' KV states are kept - Rejected tokens' KV states are discarded</p>"},{"location":"decoding_strategies/speculative_decoding/#memory-considerations","title":"Memory Considerations","text":"<pre><code>Standard decoding: 1 model's KV cache\nSpeculative decoding: 2 models' KV cache (draft + target)\n</code></pre> <p>Memory cost: Minimal\u2014draft model is small, so its cache is negligible.</p>"},{"location":"decoding_strategies/speculative_decoding/#9-interview-questions","title":"9. Interview Questions","text":""},{"location":"decoding_strategies/speculative_decoding/#q1-what-is-speculative-decoding-and-why-is-it-faster","title":"Q1: What is speculative decoding and why is it faster?","text":"<p>Answer: Speculative decoding uses a small draft model to propose K tokens cheaply, then a large target model verifies all K tokens in one parallel forward pass. Since Transformers compute logits for all positions naturally, we can check multiple draft tokens together. If most are accepted (high acceptance rate), we generate multiple tokens per expensive target model call instead of 1, reducing latency by 2-3\u00d7 while maintaining exact output distribution.</p>"},{"location":"decoding_strategies/speculative_decoding/#q2-does-speculative-decoding-produce-exact-or-approximate-results","title":"Q2: Does speculative decoding produce exact or approximate results?","text":"<p>Answer: Exact. It uses rejection sampling to correct for draft model errors. The acceptance probability \u03b1 = min(1, p(y)/q(y)) ensures accepted tokens have the exact target distribution p, not the draft distribution q. The output is statistically identical to standard decoding with the target model\u2014it's a latency optimization, not an approximation.</p>"},{"location":"decoding_strategies/speculative_decoding/#q3-why-cant-you-just-run-the-target-model-in-parallel-for-k-tokens-directly","title":"Q3: Why can't you just run the target model in parallel for K tokens directly?","text":"<p>Answer: Autoregressive models have a causal dependency\u2014token t depends on all tokens before it (t-1, t-2, ...). You cannot predict token t and token t+1 independently because t+1 needs t as input. Speculative decoding works around this by having the draft model make guesses (which may be wrong), then the target model verifies them all at once (which is valid because verification only requires a single forward pass).</p>"},{"location":"decoding_strategies/speculative_decoding/#q4-how-does-the-target-model-verify-k-tokens-in-one-pass","title":"Q4: How does the target model verify K tokens in one pass?","text":"<p>Answer: Transformers naturally compute logits for all token positions in a forward pass, not just the last one. Given input [x, y\u2081, y\u2082, y\u2083], the model outputs logits for positions 0, 1, 2, 3. Standard decoding only uses the last position. Speculative decoding uses all positions to compute p(y\u2081|x), p(y\u2082|x,y\u2081), p(y\u2083|x,y\u2081,y\u2082) and compare them with the draft probabilities. This is how Transformers work during training\u2014speculative decoding just reuses this during inference.</p>"},{"location":"decoding_strategies/speculative_decoding/#q5-what-happens-when-a-draft-token-is-rejected","title":"Q5: What happens when a draft token is rejected?","text":"<p>Answer: Stop immediately at the first rejection: 1. Discard the rejected token and all subsequent draft tokens 2. Sample a replacement token from the target model at that position 3. Restart speculation from the new sequence</p> <p>For example, if tokens 1,2 are accepted but token 3 is rejected, keep 1,2, sample a new token 3 from target, discard draft tokens 4,5,...,K. This maintains causality and correctness.</p>"},{"location":"decoding_strategies/speculative_decoding/#q6-what-determines-the-acceptance-rate","title":"Q6: What determines the acceptance rate?","text":"<p>Answer: How well the draft model matches the target model's distribution: - High similarity (e.g., quantized version) \u2192 \u03b1=0.8-0.9 \u2192 2-3\u00d7 speedup - Moderate similarity (e.g., smaller architecture) \u2192 \u03b1=0.5-0.7 \u2192 1.5-2\u00d7 speedup - Low similarity \u2192 \u03b1&lt;0.3 \u2192 overhead dominates, possibly slower</p> <p>Also affected by task (simple text has higher \u03b1) and context (more prompt context helps draft predict better).</p>"},{"location":"decoding_strategies/speculative_decoding/#q7-how-do-you-choose-the-draft-length-k","title":"Q7: How do you choose the draft length K?","text":"<p>Answer: Trade-off between potential speedup and overhead: - Higher acceptance rate \u03b1 \u2192 can use larger K (more tokens likely accepted) - Lower \u03b1 \u2192 use smaller K (avoid wasting computation on rejections) - Typical: K=4-5 works well across scenarios</p> <p>Formula: Expected tokens per cycle \u2248 (1-\u03b1^(K+1))/(1-\u03b1). Diminishing returns beyond K=5-8 for most acceptance rates.</p>"},{"location":"decoding_strategies/speculative_decoding/#q8-does-speculative-decoding-reduce-total-flops","title":"Q8: Does speculative decoding reduce total FLOPs?","text":"<p>Answer: No, it increases total FLOPs slightly (due to draft model overhead and potential rejections). The speedup comes from reducing latency\u2014fewer sequential calls to the expensive target model. It's memory-bound optimization: better GPU utilization by verifying multiple tokens in parallel rather than one at a time. Wall-clock time decreases even though total compute increases.</p>"},{"location":"decoding_strategies/speculative_decoding/#q9-can-you-use-greedy-decoding-for-the-draft-model","title":"Q9: Can you use greedy decoding for the draft model?","text":"<p>Answer: No, you must sample from the draft model and record probabilities q(y_i). The rejection sampling formula requires q(y_i) to compute \u03b1 = min(1, p(y_i)/q(y_i)). Greedy decoding doesn't provide a probability distribution over sampled tokens\u2014it just picks argmax. Sampling is essential for the mathematical correctness guarantee.</p>"},{"location":"decoding_strategies/speculative_decoding/#q10-how-does-speculative-decoding-interact-with-kv-cache","title":"Q10: How does speculative decoding interact with KV cache?","text":"<p>Answer: Both models use KV cache: - Draft model: Maintains its own cache, generates K tokens autoregressively - Target model: Computes cache for entire draft window in one pass; keeps cache for accepted tokens, discards for rejected ones</p> <p>Memory overhead is minimal since the draft model is small. KV cache improves efficiency (avoids recomputing attention) but doesn't change the algorithm\u2014it's an orthogonal optimization.</p>"},{"location":"decoding_strategies/speculative_decoding/#10-variants-and-extensions","title":"10. Variants and Extensions","text":""},{"location":"decoding_strategies/speculative_decoding/#medusa-multi-head-speculation","title":"Medusa / Multi-Head Speculation","text":"<ul> <li>Add multiple prediction heads to draft model</li> <li>Each head predicts different future positions in parallel</li> <li>Tree-based verification (multiple candidate paths)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#lookahead-decoding","title":"Lookahead Decoding","text":"<ul> <li>Uses n-gram matching and Jacobi iterations</li> <li>Doesn't require separate draft model</li> <li>Lower speedup but no extra model needed</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#staged-speculative-decoding","title":"Staged Speculative Decoding","text":"<ul> <li>Multiple draft models of increasing size</li> <li>First draft proposes, second refines, target verifies</li> <li>Can achieve higher acceptance rates</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#11-key-takeaways-for-interviews","title":"11. Key Takeaways for Interviews","text":"<ol> <li>Main idea: Draft model proposes K tokens, target verifies all in one pass</li> <li>Speedup source: Multiple tokens per expensive model call (not reduced FLOPs)</li> <li>Exactness: Rejection sampling ensures identical distribution to target model</li> <li>Acceptance rate: Critical metric\u2014need \u03b1&gt;0.5 for practical speedup</li> <li>Parallel verification: Uses existing Transformer capability (logits for all positions)</li> <li>Draft model: Must be 10\u00d7+ faster, doesn't need to be very accurate</li> <li>Typical gain: 2-3\u00d7 latency reduction in production systems</li> <li>Memory: Minimal overhead (draft model is small)</li> </ol>"},{"location":"decoding_strategies/speculative_decoding/#12-when-to-use-speculative-decoding","title":"12. When to Use Speculative Decoding","text":""},{"location":"decoding_strategies/speculative_decoding/#good-use-cases","title":"\u2705 Good Use Cases","text":"<ul> <li>Latency-critical serving (chatbots, real-time applications)</li> <li>Long-form generation (more tokens \u2192 more opportunities for speedup)</li> <li>Good draft model available (smaller version, quantized, distilled)</li> <li>Inference-bound workloads (not training)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#less-beneficial","title":"\u274c Less Beneficial","text":"<ul> <li>Batch inference (already parallelized across sequences)</li> <li>Very short outputs (overhead not amortized)</li> <li>No suitable draft model (acceptance rate too low)</li> <li>Memory-constrained systems (need to load 2 models)</li> </ul>"},{"location":"decoding_strategies/speculative_decoding/#references","title":"References","text":"<ul> <li>Fast Inference from Transformers via Speculative Decoding - Original paper</li> <li>Accelerating Large Language Model Decoding with Speculative Sampling - DeepMind's version</li> <li>Medusa: Simple LLM Inference Acceleration with Multiple Decoding Heads</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/","title":"Bottleneck analysis","text":""},{"location":"fundamentals/bottleneck_analysis/#1-understanding-bottlenecks","title":"1. Understanding Bottlenecks","text":""},{"location":"fundamentals/bottleneck_analysis/#the-three-primary-bottlenecks","title":"The Three Primary Bottlenecks","text":"<p>1. Compute-Bound - GPU cores underutilized - Not enough arithmetic operations - Common in: Prefill phase, large batches</p> <p>2. Memory-Bound - GPU cores waiting for data - Memory bandwidth saturated - Common in: Decode phase, small batches</p> <p>3. Overhead-Bound - Framework/system overhead dominates - Kernel launch latency - Common in: Very small models, batch=1</p>"},{"location":"fundamentals/bottleneck_analysis/#2-roofline-model","title":"2. Roofline Model","text":"<pre><code>Attainable Performance = min(Peak Compute, Arithmetic Intensity \u00d7 Memory Bandwidth)\n\nArithmetic Intensity = FLOPs / Bytes Transferred\n\nIf Arithmetic Intensity &lt; Compute/Bandwidth ratio \u2192 Memory-Bound\nIf Arithmetic Intensity &gt; Compute/Bandwidth ratio \u2192 Compute-Bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#example-h100-gpu","title":"Example: H100 GPU","text":"<pre><code>Peak FP16 Compute: 989 TFLOPS\nMemory Bandwidth: 3,350 GB/s\nRatio: 295 FLOP/Byte\n\nOperation with AI=100 FLOP/Byte \u2192 Memory-bound\nOperation with AI=500 FLOP/Byte \u2192 Compute-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#3-identifying-bottlenecks","title":"3. Identifying Bottlenecks","text":""},{"location":"fundamentals/bottleneck_analysis/#method-1-gpu-utilization-metrics","title":"Method 1: GPU Utilization Metrics","text":"<p>Compute Utilization</p> <pre><code>nvidia-smi dmon -s u\n# SM (Streaming Multiprocessor) utilization\n\nHigh SM% (&gt;80%) \u2192 Compute-bound\nLow SM% (&lt;40%) \u2192 Memory or overhead-bound\n</code></pre> <p>Memory Utilization</p> <pre><code>nvidia-smi dmon -s m\n# Memory bandwidth utilization\n\nHigh Mem% (&gt;80%) \u2192 Memory-bound\nLow Mem% (&lt;40%) \u2192 Compute or overhead-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#method-2-profiling-tools","title":"Method 2: Profiling Tools","text":"<p>NVIDIA Nsight Compute</p> <p><pre><code>ncu --set full -o profile python inference.py\n</code></pre> - Shows compute vs memory bottleneck per kernel - Identifies optimization opportunities</p> <p>PyTorch Profiler</p> <pre><code>from torch.profiler import profile, ProfilerActivity\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(input)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n</code></pre> <p>Key Metrics to Check:</p> <ul> <li>Kernel time distribution</li> <li>Memory copy overhead</li> <li>CPU-GPU sync points</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#method-3-microbenchmarks","title":"Method 3: Microbenchmarks","text":"<p>Isolate Operations</p> <pre><code># Test prefill vs decode separately\nprefill_time = benchmark_prefill(prompt_tokens)\ndecode_time = benchmark_decode(num_output_tokens)\n\n# Test different batch sizes\nfor batch_size in [1, 4, 8, 16, 32]:\n    throughput[batch_size] = benchmark(batch_size)\n</code></pre> <p>Expected Results:</p> <ul> <li>Decode: Throughput plateaus early \u2192 Memory-bound</li> <li>Prefill: Throughput scales with batch \u2192 Compute-bound</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#4-common-bottleneck-patterns","title":"4. Common Bottleneck Patterns","text":""},{"location":"fundamentals/bottleneck_analysis/#pattern-1-decode-phase-memory-bound","title":"Pattern 1: Decode Phase (Memory-Bound)","text":"<p>Symptoms:</p> <ul> <li>Low GPU compute utilization (20-40%)</li> <li>High memory bandwidth usage</li> <li>TPOT doesn't improve with smaller model quantization</li> </ul> <p>Root Cause:</p> <pre><code>Single token generation = Load entire weight matrix\nArithmetic Intensity \u2248 1-2 FLOP/Byte (very low)\n</code></pre> <p>Solutions:</p> <ul> <li>Weight quantization (INT8/INT4) \u2192 Reduce bytes transferred</li> <li>Increase batch size \u2192 Amortize weight loading</li> <li>Use higher memory bandwidth GPU (H100 vs A100)</li> <li>Speculative decoding \u2192 Generate multiple tokens</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-2-prefill-phase-compute-bound","title":"Pattern 2: Prefill Phase (Compute-Bound)","text":"<p>Symptoms:</p> <ul> <li>High GPU compute utilization (70-90%)</li> <li>Attention computation dominates</li> <li>Scales well with batch size</li> </ul> <p>Root Cause:</p> <pre><code>Attention: O(n\u00b2d) operations\nLong sequences = Quadratic compute growth\n</code></pre> <p>Solutions:</p> <ul> <li>FlashAttention \u2192 Fused kernel, reduce memory access</li> <li>Tensor parallelism \u2192 Split across GPUs</li> <li>Reduce sequence length if possible</li> <li>Use models with sliding window attention (Mistral)</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-3-kv-cache-transfer-memory-bound","title":"Pattern 3: KV Cache Transfer (Memory-Bound)","text":"<p>Symptoms:</p> <ul> <li>Performance degrades with sequence length</li> <li>Memory copy time visible in profiler</li> </ul> <p>Root Cause:</p> <pre><code>KV cache size = 2 \u00d7 seq_len \u00d7 layers \u00d7 heads \u00d7 dim \u00d7 bytes\nLong sequences = Large cache to copy\n</code></pre> <p>Solutions:</p> <ul> <li>GQA/MQA \u2192 Reduce KV cache size</li> <li>KV cache quantization (INT8) \u2192 2x reduction</li> <li>Paged attention (vLLM) \u2192 Better memory management</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-4-kernel-launch-overhead","title":"Pattern 4: Kernel Launch Overhead","text":"<p>Symptoms:</p> <ul> <li>Low utilization despite small workload</li> <li>Many small kernels in profiler</li> <li>Performance doesn't scale with model size</li> </ul> <p>Root Cause:</p> <pre><code>Each operation launches separate kernel\nOverhead: ~5-20\u03bcs per kernel launch\n</code></pre> <p>Solutions:</p> <ul> <li>Kernel fusion (FlashAttention, torch.compile)</li> <li>Larger batch sizes</li> <li>Use CUDA graphs \u2192 Eliminate launch overhead</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#pattern-5-cpu-gpu-synchronization","title":"Pattern 5: CPU-GPU Synchronization","text":"<p>Symptoms:</p> <ul> <li>GPU idle time between operations</li> <li>High \"cudaDeviceSynchronize\" time</li> <li>Low pipeline parallelism</li> </ul> <p>Root Cause:</p> <pre><code>Explicit sync points or implicit Python overhead\nGPU waits for CPU to issue next operation\n</code></pre> <p>Solutions:</p> <ul> <li>Asynchronous operations (CUDA streams)</li> <li>Reduce Python overhead (torch.compile, C++ inference)</li> <li>Pipeline parallelism</li> </ul>"},{"location":"fundamentals/bottleneck_analysis/#5-systematic-analysis-framework","title":"5. Systematic Analysis Framework","text":""},{"location":"fundamentals/bottleneck_analysis/#step-1-measure-baseline","title":"Step 1: Measure Baseline","text":"<pre><code>Metrics to collect:\n- Total latency (TTFT + decode time)\n- Tokens per second (throughput)\n- GPU utilization (SM%, Mem%)\n- Memory usage (weights, KV cache, activations)\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#step-2-profile-critical-path","title":"Step 2: Profile Critical Path","text":"<pre><code>Use profiler to identify:\n1. Which operations take most time?\n2. Are they compute or memory-bound?\n3. Where are sync points?\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#step-3-apply-targeted-optimizations","title":"Step 3: Apply Targeted Optimizations","text":"<pre><code>If memory-bound \u2192 Reduce data movement\nIf compute-bound \u2192 Optimize kernels or reduce ops\nIf overhead-bound \u2192 Fuse kernels or increase batch\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#step-4-validate-improvement","title":"Step 4: Validate Improvement","text":"<pre><code>Measure again and compare\nCheck for regressions in quality\nEnsure optimization applies to production workload\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#6-profiling-example-llama-2-7b","title":"6. Profiling Example: LLaMA-2-7B","text":""},{"location":"fundamentals/bottleneck_analysis/#baseline-batch1-seq512","title":"Baseline (Batch=1, Seq=512)","text":"<pre><code>Operation          | Time (ms) | % Total | Bottleneck\n-------------------|-----------|---------|------------\nAttention          | 8.2       | 45%     | Memory\nFFN                | 6.5       | 35%     | Memory\nLayer Norm         | 1.8       | 10%     | Overhead\nKV Cache Update    | 1.2       | 7%      | Memory\nMisc               | 0.5       | 3%      | -\n-------------------|-----------|---------|------------\nTotal              | 18.2      | 100%    | Memory-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#after-optimization","title":"After Optimization","text":"<pre><code>Applied: FlashAttention, INT8 quantization, kernel fusion\n\nOperation          | Time (ms) | % Total | Change\n-------------------|-----------|---------|--------\nAttention (Flash)  | 4.1       | 40%     | -50%\nFFN (INT8)         | 3.8       | 37%     | -42%\nLayer Norm (fused) | 0.9       | 9%      | -50%\nKV Cache Update    | 1.0       | 10%     | -17%\nMisc               | 0.4       | 4%      | -20%\n-------------------|-----------|---------|--------\nTotal              | 10.2      | 100%    | -44%\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#7-common-interview-questions","title":"7. Common Interview Questions","text":"<p>Q: How do you determine if inference is compute or memory-bound?</p> <pre><code>1. Check GPU metrics (SM% vs Mem%)\n2. Profile with Nsight Compute (SOL Compute vs SOL Memory)\n3. Test batch size scaling:\n   - Compute-bound: Scales well with batch\n   - Memory-bound: Plateaus quickly\n4. Calculate arithmetic intensity vs hardware ratio\n</code></pre> <p>Q: GPU shows 100% utilization but throughput is low. Why?</p> <ul> <li>Could be memory-bound (100% memory utilization)</li> <li>Check if memory bandwidth saturated</li> <li>Verify you're looking at the right metric (compute vs memory)</li> <li>Could be inefficient kernels (high utilization, low throughput)</li> </ul> <p>Q: Describe how you'd optimize a memory-bound decode phase</p> <pre><code>1. Profile to confirm bottleneck (low SM%, high Mem%)\n2. Quantize weights (INT8) \u2192 2x less data to transfer\n3. Increase batch size \u2192 Better memory bandwidth utilization\n4. Use H100 instead of A100 \u2192 1.7x more bandwidth\n5. Consider speculative decoding \u2192 Reduce number of decode steps\n</code></pre> <p>Q: What's the impact of FlashAttention on prefill vs decode?</p> <pre><code>Prefill (Compute-bound):\n- Reduces memory access (no full attention matrix)\n- Enables longer sequences without OOM\n- 2-4x speedup typical\n\nDecode (Memory-bound):\n- Smaller benefit (already memory-bound on weights)\n- Still helpful for very long context\n- ~20-30% speedup\n</code></pre> <p>Q: How do you profile Python overhead vs GPU computation?</p> <pre><code># Method 1: Compare with/without CUDA sync\nimport time\n\n# With sync (includes Python overhead)\nt0 = time.time()\noutput = model(input)\ntorch.cuda.synchronize()\nt1 = time.time()\n\n# With events (pure GPU time)\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\nstart.record()\noutput = model(input)\nend.record()\ntorch.cuda.synchronize()\ngpu_time = start.elapsed_time(end)\n\npython_overhead = (t1 - t0) - (gpu_time / 1000)\n</code></pre> <p>Q: Explain the roofline model for LLM inference</p> <pre><code>Roofline: Performance = min(Compute Peak, Bandwidth \u00d7 Arithmetic Intensity)\n\nExample: Decode single token on H100\n- Matmul: [1, 4096] \u00d7 [4096, 4096]\n- FLOPs: 2 \u00d7 1 \u00d7 4096 \u00d7 4096 \u2248 33M\n- Bytes: (4096\u00d74096 + 4096\u00d74096) \u00d7 2 (FP16) \u2248 67MB\n- AI: 33M / 67M \u2248 0.5 FLOP/Byte\n\nH100: 989 TFLOPS, 3350 GB/s \u2192 295 FLOP/Byte ratio\nAI (0.5) &lt;&lt; Ratio (295) \u2192 Memory-bound\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#8-advanced-profiling","title":"8. Advanced Profiling","text":""},{"location":"fundamentals/bottleneck_analysis/#tensor-core-utilization","title":"Tensor Core Utilization","text":"<pre><code>ncu --metrics sm__sass_thread_inst_executed_op_dmma_inst,sm__sass_thread_inst_executed_op_hmma_inst\n\nCheck if matmuls use tensor cores (TC)\nLow TC usage \u2192 Not using FP16/BF16 or improper dims\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#memory-transaction-efficiency","title":"Memory Transaction Efficiency","text":"<pre><code>ncu --metrics l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum\n\nEfficiency = Sectors / (Requests \u00d7 4)\nLow efficiency \u2192 Uncoalesced memory access\n</code></pre>"},{"location":"fundamentals/bottleneck_analysis/#9-key-takeaways","title":"9. Key Takeaways","text":"<ol> <li>Always profile before optimizing - Don't guess the bottleneck</li> <li>Different phases have different bottlenecks: Prefill (compute), Decode (memory)</li> <li>Use the right metric: SM% for compute, Mem% for memory</li> <li>Batch size is a key diagnostic: Scaling behavior reveals bottleneck</li> <li>Optimization must target the actual bottleneck: Memory optimization won't help compute-bound workload</li> <li>Modern GPUs shift bottlenecks: H100's higher compute/bandwidth ratio changes optimization strategy</li> </ol>"},{"location":"fundamentals/inference_basics/","title":"Inference basics","text":""},{"location":"fundamentals/inference_basics/#1-core-concepts","title":"1. Core Concepts","text":""},{"location":"fundamentals/inference_basics/#autoregressive-generation","title":"Autoregressive Generation","text":"<ul> <li>LLMs generate tokens sequentially: P(token_t | token_1, ..., token_{t-1})</li> <li>Each token requires full model forward pass</li> <li>Output of step t becomes input for step t+1</li> </ul>"},{"location":"fundamentals/inference_basics/#two-phase-inference","title":"Two-Phase Inference","text":"<p>Prefill Phase (Prompt Processing)</p> <ul> <li>Process entire input prompt in parallel</li> <li>Compute KV cache for all input tokens</li> <li>Computationally intensive, compute-bound</li> <li>Time complexity: O(n\u00b2d) for n tokens, d dimensions</li> </ul> <p>Decode Phase (Token Generation)</p> <ul> <li>Generate one token at a time</li> <li>Reuse cached KV from previous tokens</li> <li>Memory-bound operation (fetching weights/KV cache)</li> <li>Continues until EOS token or max length</li> </ul>"},{"location":"fundamentals/inference_basics/#key-metrics","title":"Key Metrics","text":"<pre><code>Time to First Token (TTFT) = Prefill time\nTime Per Output Token (TPOT) = Average decode time per token\nTotal Latency = TTFT + (num_output_tokens \u00d7 TPOT)\n</code></pre>"},{"location":"fundamentals/inference_basics/#2-model-architecture-components","title":"2. Model Architecture Components","text":""},{"location":"fundamentals/inference_basics/#transformer-blocks","title":"Transformer Blocks","text":"<ul> <li>Multi-head self-attention: O(n\u00b2d) complexity</li> <li>Feed-forward network: O(nd_ff) where d_ff \u2248 4d</li> <li>Layer normalization</li> <li>Residual connections</li> </ul>"},{"location":"fundamentals/inference_basics/#kv-cache","title":"KV Cache","text":"<ul> <li>Stores key/value matrices from previous tokens</li> <li>Size per layer: 2 \u00d7 batch_size \u00d7 seq_len \u00d7 num_heads \u00d7 head_dim \u00d7 2 bytes (FP16)</li> <li>Example: LLaMA-2-7B with seq_len=2048, batch=1</li> <li>Per layer: 2 \u00d7 1 \u00d7 2048 \u00d7 32 \u00d7 128 \u00d7 2 \u2248 33 MB</li> <li>Total (32 layers): ~1 GB</li> </ul>"},{"location":"fundamentals/inference_basics/#3-memory-requirements","title":"3. Memory Requirements","text":"<pre><code>Total Memory = Model Weights + KV Cache + Activations + Overhead\n\nModel Weights = num_params \u00d7 bytes_per_param\n- FP32: 4 bytes, FP16: 2 bytes, INT8: 1 byte, INT4: 0.5 bytes\n\nActivations = temporary tensors during forward pass\nOverhead = CUDA context, fragmentation (~10-20%)\n</code></pre> <p>Example: LLaMA-2-7B (FP16) - Weights: 7B \u00d7 2 = 14 GB - KV cache (batch=1, seq=2048): ~1 GB - Activations: ~0.5-1 GB - Total: ~16-17 GB</p>"},{"location":"fundamentals/inference_basics/#4-common-interview-questions","title":"4. Common Interview Questions","text":"<p>Q: Why is prefill compute-bound and decode memory-bound?</p> <ul> <li>Prefill: Process many tokens in parallel \u2192 high arithmetic intensity, GPU cores saturated</li> <li>Decode: Generate 1 token \u2192 fetch entire model weights from memory, low compute utilization</li> </ul> <p>Q: How does batch size affect inference? - Prefill: Higher batch increases compute, remains compute-bound - Decode: Higher batch increases memory for KV cache, can become compute-bound with large batches - Sweet spot: Balance between throughput and latency</p> <p>Q: What limits maximum sequence length?</p> <ul> <li>KV cache memory grows linearly with sequence length</li> <li>Attention computation grows quadratically O(n\u00b2)</li> <li>GPU memory capacity is primary constraint</li> </ul> <p>Q: Calculate memory for Mistral-7B (FP16) with batch=4, seq=4096?</p> <pre><code>Weights: 7B \u00d7 2 = 14 GB\nKV cache: 2 \u00d7 4 \u00d7 4096 \u00d7 32 \u00d7 128 \u00d7 2 \u00d7 32 layers \u2248 8 GB\nTotal: ~22-24 GB\n</code></pre> <p>Q: Why can't we parallelize token generation?</p> <ul> <li>Each token depends on all previous tokens</li> <li>Autoregressive dependency prevents parallelization</li> <li>Speculative decoding attempts to work around this</li> </ul>"},{"location":"fundamentals/inference_basics/#5-modern-optimizations-2024-2025","title":"5. Modern Optimizations (2024-2025)","text":"<ul> <li>Grouped Query Attention (GQA): Reduce KV cache by sharing KV heads</li> <li>Multi-Query Attention (MQA): Single KV head for all queries</li> <li>FlashAttention-3: Fused attention kernel, 2x faster on H100</li> <li>Paged Attention (vLLM): Non-contiguous KV cache storage</li> <li>Continuous Batching: Dynamic batch assembly for throughput</li> </ul>"},{"location":"fundamentals/inference_basics/#6-key-takeaways","title":"6. Key Takeaways","text":"<ol> <li>Inference has distinct prefill (parallel) and decode (sequential) phases</li> <li>KV cache is crucial for avoiding recomputation but consumes significant memory</li> <li>Memory bandwidth is often the bottleneck during decode</li> <li>Model size, sequence length, and batch size determine memory requirements</li> <li>Understanding the compute vs memory bound distinction is critical</li> </ol>"},{"location":"fundamentals/latency_vs_throughput/","title":"Latency vs throughput","text":""},{"location":"fundamentals/latency_vs_throughput/#1-core-concepts","title":"1. Core Concepts","text":""},{"location":"fundamentals/latency_vs_throughput/#latency","title":"Latency","text":"<ul> <li>Time to complete a single request</li> <li>Measured in seconds or milliseconds</li> <li>Critical for interactive applications (chatbots, code completion)</li> <li>Key metrics: TTFT, TPOT, E2E latency</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#throughput","title":"Throughput","text":"<ul> <li>Number of requests processed per unit time</li> <li>Measured in tokens/sec or requests/sec</li> <li>Critical for batch processing, high-traffic services</li> <li>Maximize GPU utilization</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#2-the-fundamental-tradeoff","title":"2. The Fundamental Tradeoff","text":"<pre><code>Latency \u2191 as Throughput \u2191\n\nHigher batch size \u2192 Higher throughput, Higher latency per request\nLower batch size \u2192 Lower latency, Lower throughput\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#why-they-conflict","title":"Why They Conflict","text":"<p>Batching Increases Throughput</p> <ul> <li>Process multiple requests simultaneously</li> <li>Better GPU utilization (more parallel work)</li> <li>Amortize weight loading overhead</li> </ul> <p>But Hurts Latency</p> <ul> <li>Requests wait for entire batch to complete</li> <li>Queueing delays increase</li> <li>Stragglers slow down entire batch</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#3-key-metrics","title":"3. Key Metrics","text":"<pre><code>Latency = Queue Time + Processing Time\nThroughput = Batch Size / Processing Time (ignoring queue)\n\nUtilization = (Actual Throughput) / (Max Theoretical Throughput)\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#4-optimization-strategies","title":"4. Optimization Strategies","text":""},{"location":"fundamentals/latency_vs_throughput/#for-low-latency-100ms","title":"For Low Latency (&lt; 100ms)","text":"<p>Batch Size = 1 or Small</p> <ul> <li>Minimize queueing delay</li> <li>Accept lower GPU utilization</li> <li>Use smaller models (7B vs 70B)</li> <li>Quantization (INT8/INT4) for faster decode</li> </ul> <p>Prefill Optimization</p> <ul> <li>FlashAttention for faster attention</li> <li>Tensor parallelism to split model across GPUs</li> </ul> <p>Infrastructure</p> <ul> <li>Low-latency network</li> <li>GPU with high memory bandwidth (H100 &gt; A100)</li> <li>Close to users (edge deployment)</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#for-high-throughput","title":"For High Throughput","text":"<p>Large Batch Sizes</p> <ul> <li>Batch 32-128+ requests</li> <li>Maximize GPU compute utilization</li> <li>Accept seconds of latency per request</li> </ul> <p>Continuous Batching</p> <ul> <li>Don't wait for all sequences to finish</li> <li>Insert new requests as others complete</li> <li>Used by vLLM, TensorRT-LLM</li> </ul> <p>Paged Attention (vLLM)</p> <ul> <li>Reduce memory fragmentation</li> <li>Pack more sequences in memory</li> <li>Enable larger effective batch size</li> </ul> <p>Chunked Prefill</p> <ul> <li>Split long prefills into chunks</li> <li>Interleave with decode steps</li> <li>Balance latency and throughput</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#5-request-level-batching-strategies","title":"5. Request-Level Batching Strategies","text":""},{"location":"fundamentals/latency_vs_throughput/#static-batching","title":"Static Batching","text":"<ul> <li>Wait for batch to fill before processing</li> <li>Simple but high latency variance</li> <li>Wasted time if batch doesn't fill</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#continuous-batching","title":"Continuous Batching","text":"<pre><code>t=0: Start batch [A, B, C]\nt=1: A finishes \u2192 add D \u2192 [B, C, D]\nt=2: B finishes \u2192 add E \u2192 [C, D, E]\n</code></pre> <ul> <li>Dynamic batch composition</li> <li>Much better GPU utilization</li> <li>Lower average latency</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#priority-queuing","title":"Priority Queuing","text":"<ul> <li>Process short/urgent requests first</li> <li>Separate queues for interactive vs batch</li> <li>SLO-aware scheduling</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#6-hardware-considerations","title":"6. Hardware Considerations","text":""},{"location":"fundamentals/latency_vs_throughput/#a100-80gb","title":"A100 (80GB)","text":"<ul> <li>1,935 GB/s memory bandwidth</li> <li>Good for batch inference</li> <li>Throughput: ~2000 tokens/sec (LLaMA-2-7B, batch=32)</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#h100-80gb","title":"H100 (80GB)","text":"<ul> <li>3,350 GB/s memory bandwidth (1.7x A100)</li> <li>Better for both latency and throughput</li> <li>FlashAttention-3 support</li> <li>Throughput: ~3500 tokens/sec (same setup)</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#l40s-l4","title":"L40S / L4","text":"<ul> <li>Lower cost, lower bandwidth</li> <li>Good for latency-optimized serving (small batch)</li> <li>Not ideal for high throughput</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#7-common-interview-questions","title":"7. Common Interview Questions","text":"<p>Q: You have 1000 QPS (queries per second). Optimize for p99 latency &lt; 200ms. How?</p> <ul> <li>Use continuous batching (vLLM)</li> <li>Target small effective batch (4-8)</li> <li>Replica scaling with load balancer</li> <li>Monitor queue depth, scale if needed</li> </ul> <p>Q: Batch size 1 vs 32: compare latency and throughput</p> <pre><code>Batch=1:\n- Latency: ~50ms\n- Throughput: ~20 tokens/sec\n- GPU utilization: ~15%\n\nBatch=32:\n- Latency: ~800ms (includes queueing)\n- Throughput: ~500 tokens/sec\n- GPU utilization: ~80%\n</code></pre> <p>Q: How does continuous batching improve over static?</p> <ul> <li>No waiting for batch to fill</li> <li>No wasted cycles when sequences finish at different times</li> <li>Typically, 2-3x better throughput at similar latency</li> </ul> <p>Q: When would you choose latency over throughput?</p> <ul> <li>Real-time chat applications</li> <li>Code completion (100-200ms target)</li> <li>Interactive agents</li> <li>Premium API tiers</li> </ul> <p>Q: When would you choose throughput over latency?</p> <ul> <li>Offline batch processing</li> <li>Data labeling/annotation</li> <li>Embedding generation</li> <li>Document summarization at scale</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#8-production-patterns-2024-2025","title":"8. Production Patterns (2024-2025)","text":""},{"location":"fundamentals/latency_vs_throughput/#multi-tier-serving","title":"Multi-Tier Serving","text":"<pre><code>Tier 1 (Latency): Small models, batch=1-4, edge deployment\nTier 2 (Balanced): Medium models, continuous batching, batch=8-16\nTier 3 (Throughput): Large models, large batches, datacenter\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#speculative-decoding","title":"Speculative Decoding","text":"<ul> <li>Draft model generates multiple tokens</li> <li>Target model verifies in parallel</li> <li>2-3x speedup with same latency</li> <li>Best for latency-sensitive scenarios</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#disaggregated-serving-splitwise","title":"Disaggregated Serving (Splitwise)","text":"<ul> <li>Separate prefill and decode clusters</li> <li>Prefill: GPU compute optimized (A100)</li> <li>Decode: Memory bandwidth optimized (H100)</li> <li>Transfer KV cache between clusters</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#9-key-metrics-to-monitor","title":"9. Key Metrics to Monitor","text":"<pre><code>P50, P95, P99 Latency - Distribution matters\nThroughput (tokens/sec) - Absolute capacity\nQueue Depth - Leading indicator of overload\nGPU Utilization - Efficiency metric\nCost per 1M tokens - Business metric\n</code></pre>"},{"location":"fundamentals/latency_vs_throughput/#10-benchmarking-tips","title":"10. Benchmarking Tips","text":"<ul> <li>Measure real production traffic patterns</li> <li>Include cold start times if relevant</li> <li>Test at different concurrency levels</li> <li>Monitor long-tail latency (p99, p99.9)</li> <li>Account for sequence length variance</li> </ul>"},{"location":"fundamentals/latency_vs_throughput/#11-key-takeaways","title":"11. Key Takeaways","text":"<ol> <li>Latency and throughput are inversely related via batching</li> <li>Continuous batching is standard for production (vLLM, TRT-LLM)</li> <li>Different use cases need different optimization targets</li> <li>Hardware choice matters: H100 better for both metrics vs A100</li> <li>Monitor distributions (p99), not just averages</li> </ol>"},{"location":"fundamentals/memory_compute_tradeoffs/","title":"Memory compute tradeoffs","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#1-the-core-tradeoff","title":"1. The Core Tradeoff","text":"<p>Memory Savings \u2192 Compute Overhead (Usually)</p> <p>Techniques that reduce memory often require:</p> <ul> <li>Additional computation (quantization/dequantization)</li> <li>Recomputation instead of caching</li> <li>More complex kernels</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#2-memory-bottlenecks-in-llm-inference","title":"2. Memory Bottlenecks in LLM Inference","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#1-model-weights-static","title":"1. Model Weights (Static)","text":"<ul> <li>70B model in FP16: 140 GB</li> <li>Must fit in GPU memory</li> <li>Loaded repeatedly during decode (memory bandwidth bound)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#2-kv-cache-dynamic","title":"2. KV Cache (Dynamic)","text":"<ul> <li>Grows with sequence length and batch size</li> <li>Often largest memory consumer in production</li> <li>Formula: <code>2 \u00d7 B \u00d7 S \u00d7 L \u00d7 H \u00d7 D \u00d7 bytes</code></li> <li>B=batch, S=seq_len, L=layers, H=heads, D=head_dim</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#3-activations-temporary","title":"3. Activations (Temporary)","text":"<ul> <li>Intermediate tensors during forward pass</li> <li>Recomputed in inference (no backprop needed)</li> <li>~5-10% of total memory</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#3-quantization-trading-precision-for-memory","title":"3. Quantization: Trading Precision for Memory","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#weight-quantization","title":"Weight Quantization","text":"<p>FP16 \u2192 INT8 (8-bit)</p> <ul> <li>2x memory reduction (2 bytes \u2192 1 byte)</li> <li>Minimal accuracy loss (&lt;1% typically)</li> <li>Faster on hardware with INT8 support (Tensor Cores)</li> <li>Compute: Dequantize to FP16 for matmul (overhead ~10%)</li> </ul> <p>FP16 \u2192 INT4 (4-bit)</p> <ul> <li>4x memory reduction</li> <li>Quality degradation possible (1-3% on benchmarks)</li> <li>Requires calibration data</li> <li>Compute: More dequant overhead (~20-30%)</li> </ul> <p>Techniques:</p> <pre><code>Per-Tensor: Single scale for entire tensor\nPer-Channel: Scale per output channel (better quality)\nGroup Quantization: Scale per 128 elements (GPTQ, AWQ)\n\nGPTQ: Layer-wise quantization, minimizes error\nAWQ: Activation-aware, protects important weights\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#kv-cache-quantization","title":"KV Cache Quantization","text":"<ul> <li>KV cache in INT8 instead of FP16</li> <li>2x memory savings \u2192 2x larger batch or sequence length</li> <li>Quality loss typically &lt;0.5%</li> <li>Growing adoption in production (2024+)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#mixed-precision","title":"Mixed Precision","text":"<ul> <li>Keep critical layers in FP16 (first/last, attention)</li> <li>Quantize FFN layers to INT4</li> <li>Balance quality and memory</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#4-kv-cache-optimization","title":"4. KV Cache Optimization","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#multi-query-attention-mqa","title":"Multi-Query Attention (MQA)","text":"<pre><code>Standard: num_kv_heads = num_query_heads (e.g., 32)\nMQA: num_kv_heads = 1\n\nMemory reduction: 32x fewer KV parameters\nTradeoff: Slight quality degradation\nUsed in: Falcon, StarCoder\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#grouped-query-attention-gqa","title":"Grouped Query Attention (GQA)","text":"<pre><code>num_kv_heads &lt; num_query_heads\nExample: 8 KV heads, 32 query heads (4 queries per KV)\n\nMemory reduction: 4x fewer KV parameters\nTradeoff: Minimal quality loss\nUsed in: LLaMA-2, Mistral, GPT-4 (rumored)\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#paged-attention-vllm","title":"Paged Attention (vLLM)","text":"<ul> <li>KV cache in non-contiguous \"pages\" (like OS virtual memory)</li> <li>Eliminates fragmentation</li> <li>Enables ~2x higher batch size for same memory</li> <li>Compute: Slight overhead for page lookup</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#multi-token-prediction","title":"Multi-Token Prediction","text":"<ul> <li>Cache prefixes for common prompts</li> <li>Reduces redundant computation</li> <li>Memory: Store prompt KV cache (shared across requests)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#4-recomputation-vs-caching","title":"4. Recomputation vs Caching","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#activation-checkpointing-training","title":"Activation Checkpointing (Training)","text":"<ul> <li>Not used in inference (no backprop)</li> <li>Mentioned for completeness</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#selective-recomputation","title":"Selective Recomputation","text":"<ul> <li>Recompute cheap operations instead of storing</li> <li>Example: Recompute layer norm instead of caching</li> <li>Memory savings: ~10-20%</li> <li>Compute overhead: ~5-10%</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#5-model-architecture-choices","title":"5. Model Architecture Choices","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#width-vs-depth","title":"Width vs Depth","text":"<pre><code>Wide: More hidden dimensions, fewer layers\n- More memory for weights\n- Less memory for KV cache (fewer layers)\n\nDeep: More layers, smaller hidden dimensions\n- Less memory for weights  \n- More memory for KV cache (more layers)\n</code></pre>"},{"location":"fundamentals/memory_compute_tradeoffs/#ffn-expansion-ratio","title":"FFN Expansion Ratio","text":"<ul> <li>Standard: <code>d_ff = 4 \u00d7 d_model</code></li> <li>Smaller ratio (2x or 3x): Less memory, potential quality loss</li> <li>MoE: Sparse activation, more parameters but same compute</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#6-hardware-specific-tradeoffs","title":"6. Hardware-Specific Tradeoffs","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#memory-bandwidth-vs-compute","title":"Memory Bandwidth vs Compute","text":"<pre><code>A100: 1,935 GB/s bandwidth, 312 TFLOPS (FP16)\nH100: 3,350 GB/s bandwidth, 989 TFLOPS (FP16)\n\nBandwidth-to-Compute Ratio:\nA100: 6.2 GB/s per TFLOP\nH100: 3.4 GB/s per TFLOP\n</code></pre> <p>Implication: H100 relatively more compute-bound, benefits more from quantization compute overhead</p>"},{"location":"fundamentals/memory_compute_tradeoffs/#tensor-core-utilization","title":"Tensor Core Utilization","text":"<ul> <li>FP16: Full tensor core speed</li> <li>INT8: 2x faster on Ampere/Hopper with DP4A</li> <li>INT4: 4x faster (requires specialized kernels)</li> </ul> <p>Tradeoff: Quantization compute overhead offset by faster matmul</p>"},{"location":"fundamentals/memory_compute_tradeoffs/#7-memory-compute-decision-matrix","title":"7. Memory-Compute Decision Matrix","text":"Technique Memory Saved Compute Overhead Quality Impact INT8 Quantization 2x +10% &lt;1% INT4 Quantization 4x +30% 1-3% GQA (4:1) 4x KV cache Minimal &lt;0.5% MQA 32x KV cache Minimal 1-2% KV Cache INT8 2x KV cache +5% &lt;0.5% FlashAttention Minimal -30% latency None"},{"location":"fundamentals/memory_compute_tradeoffs/#8-common-interview-questions","title":"8. Common Interview Questions","text":"<p>Q: You have a 70B model but only 40GB GPU memory. What do you do?</p> <pre><code>Options:\n1. INT4 quantization: 140GB \u2192 35GB \u2713\n2. INT8 + model parallelism across 2 GPUs\n3. Offload layers to CPU (slow, not recommended)\n4. Use smaller model variant (13B/7B)\n</code></pre> <p>Q: Explain the tradeoff in GQA (Grouped Query Attention)</p> <ul> <li>Save memory: Fewer KV heads \u2192 smaller KV cache</li> <li>Minimal compute overhead: Attention computation slightly changes</li> <li>Quality: Negligible impact (&lt;0.5% on benchmarks)</li> <li>Production: Widely adopted (Mistral, LLaMA-2)</li> </ul> <p>Q: Why is decode phase memory-bound?</p> <ul> <li>Single token generation: Low arithmetic intensity</li> <li>Must fetch entire weight matrix from memory</li> <li>Memory bandwidth saturated, compute underutilized</li> <li>Arithmetic Intensity: FLOPs / Bytes Loaded \u2248 1-2 (very low)</li> </ul> <p>Q: When does quantization hurt performance?</p> <ul> <li>Small batch size: Dequant overhead dominates</li> <li>Compute-bound workloads: Adding compute makes it worse</li> <li>Old hardware: No INT8 tensor core support</li> <li>Generally: Decode phase on modern GPUs (H100) benefits from quantization</li> </ul> <p>Q: Calculate KV cache size: LLaMA-2-70B, batch=16, seq=4096, FP16</p> <pre><code>GQA with 8 KV heads (70B uses this)\n2 \u00d7 16 \u00d7 4096 \u00d7 80_layers \u00d7 8_heads \u00d7 128_dim \u00d7 2_bytes\n= 2 \u00d7 16 \u00d7 4096 \u00d7 80 \u00d7 8 \u00d7 128 \u00d7 2\n= 1,073,741,824 bytes \u2248 1 GB per sample \u00d7 16 = 16 GB total\n\n(If standard MHA with 64 heads: 128 GB - impractical!)\n</code></pre> <p>Q: How does FlashAttention affect memory-compute tradeoff?</p> <ul> <li>Reduces memory: Avoids materializing full attention matrix</li> <li>Reduces compute time: Fused kernel, better cache locality</li> <li>Win-win: Memory AND compute improvement</li> <li>No quality impact (mathematically equivalent)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#9-modern-techniques-2024-2025","title":"9. Modern Techniques (2024-2025)","text":""},{"location":"fundamentals/memory_compute_tradeoffs/#awq-activation-aware-weight-quantization","title":"AWQ (Activation-Aware Weight Quantization)","text":"<ul> <li>Protect weights with high activation magnitude</li> <li>Better quality than naive INT4</li> <li>Used in production (Hugging Face TGI)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#smoothquant","title":"SmoothQuant","text":"<ul> <li>Migrate difficulty from weights to activations</li> <li>Enables better INT8 quantization</li> <li>Particularly for older models not trained for quantization</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#fp8-h100","title":"FP8 (H100)","text":"<ul> <li>Native FP8 support on Hopper</li> <li>2x memory saving vs FP16</li> <li>Minimal quality loss</li> <li>Compute: Faster than FP16 (2x with tensor cores)</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#quip-aqlm","title":"QuIP# / AQLM","text":"<ul> <li>Extreme quantization (2-3 bits)</li> <li>Lattice-based, better than naive 2-bit</li> <li>Research stage, not production yet</li> </ul>"},{"location":"fundamentals/memory_compute_tradeoffs/#10-practical-guidelines","title":"10. Practical Guidelines","text":"<ol> <li>Start with INT8: Minimal quality loss, 2x memory saving</li> <li>Use GQA architecture: If designing new models</li> <li>Enable KV cache quantization: Production-ready in vLLM</li> <li>FlashAttention is mandatory: No downside</li> <li>INT4 for large models: When GPU memory is the constraint</li> <li>Monitor quality: Always benchmark on your task</li> </ol>"},{"location":"fundamentals/memory_compute_tradeoffs/#11-key-takeaways","title":"11. Key Takeaways","text":"<ol> <li>Most memory optimizations have negligible compute cost (GQA, FlashAttention)</li> <li>Quantization is a clear win on modern hardware (INT8 tensor cores)</li> <li>KV cache often dominates memory in long-context scenarios</li> <li>Decode phase is memory-bound: Reducing memory access helps latency</li> <li>Hardware matters: H100 handles quantization overhead better than A100</li> </ol>"},{"location":"quantization/awq/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/awq/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/awq/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/gguf_ggml/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/gguf_ggml/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/gguf_ggml/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/gptq/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/gptq/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/gptq/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/int4_quantization/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/int4_quantization/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/int4_quantization/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/int8_quantization/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/int8_quantization/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/int8_quantization/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/quantization_basics/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/quantization_basics/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/quantization_basics/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/quantization_tradeoffs/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/quantization_tradeoffs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/quantization_tradeoffs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"quantization/smoothquant/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"quantization/smoothquant/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"quantization/smoothquant/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"serving_frameworks/deepspeed_inference/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"serving_frameworks/deepspeed_inference/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"serving_frameworks/deepspeed_inference/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"serving_frameworks/tensorrt_llm/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"serving_frameworks/tensorrt_llm/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"serving_frameworks/tensorrt_llm/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"serving_frameworks/text_generation_inference/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"serving_frameworks/text_generation_inference/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"serving_frameworks/text_generation_inference/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"serving_frameworks/triton_inference_server/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"serving_frameworks/triton_inference_server/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"serving_frameworks/triton_inference_server/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"serving_frameworks/vllm/","title":"Vllm","text":""},{"location":"serving_frameworks/vllm/#1-overview","title":"1. Overview","text":"<p>vLLM is an open-source inference engine optimized for high-throughput, low-latency serving of large language models. Its core innovation, PagedAttention, rethinks KV cache management by applying principles similar to virtual memory systems in operating systems.</p> <p>The key contribution of vLLM is not faster kernels alone, but dramatically improved GPU memory utilization, enabling higher batch sizes, better multi-tenancy, and more predictable latency under load.</p>"},{"location":"serving_frameworks/vllm/#2-the-kv-cache-bottleneck-in-llm-inference","title":"2. The KV Cache Bottleneck in LLM Inference","text":""},{"location":"serving_frameworks/vllm/#21-why-the-kv-cache-dominates","title":"2.1 Why the KV Cache Dominates","text":"<p>During autoregressive decoding, each generated token appends Key and Value tensors for every transformer layer. Over long sequences or many concurrent requests, the KV cache quickly becomes the dominant consumer of GPU memory.</p> <p>Key properties: - Grows linearly with sequence length - Must be retained across decoding steps - Is memory-bandwidth bound during decode</p>"},{"location":"serving_frameworks/vllm/#3-the-core-technology-pagedattention","title":"3. The Core Technology: PagedAttention","text":"<p>The KV cache (the memory storing previous tokens to predict the next one) is the primary bottleneck in scaling LLMs.</p>"},{"location":"serving_frameworks/vllm/#the-problem-fragmentation","title":"The Problem: Fragmentation","text":"<p>Standard inference engines allocate a contiguous \"max-length\" chunk of memory for every request.</p> <ul> <li>Over-reservation: Reserving 2048 tokens for a request that only generates 50.</li> <li>Internal Fragmentation: Memory wasted inside the reserved block.</li> <li>Waste: Up to 60-80% of VRAM is often left unused but \"reserved.\"</li> </ul>"},{"location":"serving_frameworks/vllm/#the-solution-pagedattention","title":"The Solution: PagedAttention","text":"<p>vLLM breaks the KV cache into fixed-size blocks (pages).</p> <ul> <li>Logical Blocks: Sequential tokens in the prompt.</li> <li>Physical Blocks: Non-contiguous memory addresses on the GPU.</li> <li>Block Table: A mapping system that allows the model to access these blocks as if they were one continuous string.</li> </ul> <p>Result: Waste is reduced to &lt;4%, allowing for significantly larger batch sizes.</p>"},{"location":"serving_frameworks/vllm/#4-scheduling-continuous-batching","title":"4. Scheduling: Continuous Batching","text":"<p>Traditional engines use \"Static Batching,\" where the entire batch must finish before new requests start.</p> <ul> <li>Iteration-Level Scheduling: vLLM schedules at the level of individual iterations. </li> <li>Mechanism: As soon as one sequence in a batch hits an <code>&lt;EOS&gt;</code> (End of Sentence) token, a new request from the queue is inserted into its spot in the next iteration.</li> <li>Outcome: Eliminates \"bubbles\" (idle time) in GPU utilization.</li> </ul>"},{"location":"serving_frameworks/vllm/#5-modern-2025-2026-advanced-features","title":"5. Modern (2025-2026) Advanced Features","text":""},{"location":"serving_frameworks/vllm/#a-speculative-decoding","title":"A. Speculative Decoding","text":"<p>vLLM implements speculative decoding where a smaller draft model (e.g., a 100M parameter model) predicts several tokens, and a larger target model (e.g., Llama 3 70B) verifies them in a single pass. * Benefit: Reduces latency by 2-3x for heavy models.</p>"},{"location":"serving_frameworks/vllm/#b-automatic-prefix-caching-apc","title":"B. Automatic Prefix Caching (APC)","text":"<p>For RAG or multi-turn chat, vLLM caches the KV blocks of common prefixes (like system prompts). * If two users share the same 1,000-token system prompt, vLLM stores it once in physical memory, and both requests point to the same blocks.</p>"},{"location":"serving_frameworks/vllm/#c-multi-lora-support","title":"C. Multi-LoRA Support","text":"<p>vLLM can serve one base model with hundreds of different fine-tuned \"adapters\" (LoRAs) simultaneously.  * It uses specialized SGMV (Shrink-Generalized Matrix-Vector) kernels to compute multiple different LoRAs in a single batch without a significant performance hit.</p>"},{"location":"serving_frameworks/vllm/#6-prefill-vs-decode-performance-characteristics","title":"6. Prefill vs Decode: Performance Characteristics","text":"Phase Characteristics Bottleneck Prefill Parallel over tokens Compute-bound Decode Sequential token-by-token Memory-bandwidth-bound <p>vLLM introduces Chunked Prefill to prevent long prompts from blocking decode for other users.</p>"},{"location":"serving_frameworks/vllm/#8-memory-pressure-and-preemption","title":"8. Memory Pressure and Preemption","text":"<p>When GPU memory becomes constrained, vLLM supports preemption strategies:</p> <ul> <li>Swap: Move KV blocks to CPU memory</li> <li>Recompute: Drop KV blocks and recompute them later</li> </ul> <p>Recompute trades extra compute for lower memory pressure and is often preferred on fast GPUs.</p>"},{"location":"serving_frameworks/vllm/#9-architectural-comparison","title":"9. Architectural Comparison","text":"Feature vLLM Hugging Face TGI NVIDIA TensorRT-LLM Memory Mgmt PagedAttention FlashAttention Paged KV Cache Ease of Use High (Pythonic) Medium (Rust/Go) Low (Complex Build) Best For General Throughput Stability/HF ecosystem Peak NVIDIA Perf"},{"location":"serving_frameworks/vllm/#10-useful-memory-approximation","title":"10. Useful Memory Approximation","text":"<p>Approximate KV cache memory usage in bytes:</p> \\[ \\text{KV Memory} \\;\\approx\\; 2 \\times L \\times T \\times H \\times D_h \\times B \\] <p>Where: - 2 accounts for Keys and Values - \\(L\\) is the number of layers - \\(H\\) is the number of attention heads  - \\(B\\) is the number of bytes per element (2 for FP16, 1 for FP8) - \\(D_h\\) is the per-head hidden dimension - \\(T\\) is the sequence length (number of cached tokens)</p> <p>Example:</p> <p>For Llama-3 8B:</p> <ul> <li>\\(L = 32\\)</li> <li>\\(D_{\\text{model}} = 4096\\)</li> <li>\\(B = 2\\) (FP16)</li> </ul> <p>Per token KV cache memory:</p> \\[ 2 \\times 32 \\times 4096 \\times 2 \\approx 524{,}288 \\text{ bytes} \\approx 0.5 \\text{ MB} \\] <p>Approximate totals: - 2k tokens \u2192 ~1 GB KV cache - 8k tokens \u2192 ~4 GB KV cache</p> <p>This linear scaling with sequence length explains why KV cache memory becomes the dominant bottleneck and motivates techniques such as PagedAttention in vLLM.</p>"},{"location":"serving_frameworks/vllm/#q-as","title":"Q &amp; As","text":"<p>Q1: How does vLLM handle a situation where the GPU runs out of memory during a request? * A: It uses Preemption. It can either \"Swap\" (move blocks to CPU RAM) or \"Recompute\" (drop the blocks and re-calculate them later when memory is free).</p> <p>Q2: What is the difference between the 'Prefill' and 'Decode' phases? * A: Prefill processes the input prompt (parallel/compute-bound). Decode generates tokens one by one (sequential/memory-bound). vLLM uses Chunked Prefill to prevent large prompts from stalling the generation of other users.</p> <p>Q3: Why is vLLM better for multi-tenant SaaS? * A: Because of PagedAttention and Multi-LoRA support. It allows hosting many different \"specialized\" models on a single GPU cluster with minimal overhead.</p>"}]}