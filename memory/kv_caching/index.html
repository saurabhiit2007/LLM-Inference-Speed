<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Kv caching - My Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Welcome to MkDocs</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Decoding</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../decoding/decoding_strategies/" class="dropdown-item">Decoding strategies</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Memory</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">Kv caching</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Speed</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../speed/flash_attention/" class="dropdown-item">Flash Attention</a>
</li>
                                    
<li>
    <a href="../../speed/speculative_decoding/" class="dropdown-item">Speculative decoding</a>
</li>
                                    
<li>
    <a href="../../speed/vllm/" class="dropdown-item">Vllm</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../decoding/decoding_strategies/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../speed/flash_attention/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            
            
            
            
            
            
            
            
            
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h3 id="1-self-attention-recap">ðŸ“¦1. Self Attention Recap</h3>
<p>Given hidden states $X \in \mathbb{R}^{T \times d}$:</p>
<p>$$
Q = XW_Q,\quad K = XW_K,\quad V = XW_V
$$</p>
<p>Per head attention:</p>
<p>$$
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_h}}\right)V
$$</p>
<p>Autoregressive decoding generates one token at a time with causal masking.</p>
<hr />
<h3 id="2-why-kv-cache-is-needed">ðŸ“¦2. Why KV Cache Is Needed</h3>
<p>At decoding step $t$, keys and values for tokens $1 \ldots t-1$ are unchanged but would be recomputed without caching.</p>
<p>This repeated computation dominates inference latency and wastes FLOPs.</p>
<hr />
<h3 id="3-kv-cache-mechanism">ðŸ“¦3. KV Cache Mechanism</h3>
<p>For each transformer layer $\ell$:</p>
<p>$$
\text{KVCache}<em>\ell = {K</em>\ell^{1:t}, V_\ell^{1:t}}
$$</p>
<p>At decoding step $t$:</p>
<ul>
<li>Compute $Q_t, K_t, V_t$</li>
<li>Append $K_t, V_t$ to the cache</li>
<li>Attend over all cached keys and values</li>
</ul>
<p>$$
\text{Attn}<em>t = \text{softmax}\left(\frac{Q_t K</em>{1:t}^T}{\sqrt{d_h}}\right)V_{1:t}
$$</p>
<p>Only the current token requires new computation.</p>
<hr />
<h3 id="4-toy-example">ðŸ“¦4. Toy Example</h3>
<p>Prompt: "I like neural"</p>
<p>Step 1: generate <code>"networks"</code></p>
<ul>
<li>Compute and cache keys and values for the prompt</li>
<li>Attend to all cached tokens</li>
</ul>
<p>Step 2: generate <code>"models"</code></p>
<ul>
<li>Reuse cached keys and values</li>
<li>Compute keys and values only for <code>"networks"</code></li>
</ul>
<p>Previously generated tokens are never recomputed.</p>
<hr />
<h3 id="5-complexity-analysis">ðŸ“¦5. Complexity Analysis</h3>
<h4 id="notation">Notation</h4>
<ul>
<li>$T$: number of generated tokens</li>
<li>$L$: number of transformer layers</li>
<li>$H$: number of attention heads</li>
<li>$d_h$: head dimension</li>
</ul>
<h4 id="without-kv-cache">Without KV Cache</h4>
<p>At each decoding step, attention is recomputed for all previous tokens:</p>
<p>$$
O(L \cdot H \cdot d_h \cdot T^3)
$$</p>
<h4 id="with-kv-cache">With KV Cache</h4>
<p>Only attention against cached keys and values is computed:</p>
<p>$$
O(L \cdot H \cdot d_h \cdot T^2)
$$</p>
<p>KV caching removes one full factor of $T$ from decoding complexity.</p>
<hr />
<h3 id="6-memory-cost">ðŸ“¦6. Memory Cost</h3>
<p>Each layer stores:</p>
<p>$$
K, V \in \mathbb{R}^{H \times T \times d_h}
$$</p>
<p>Total KV cache memory across all layers:</p>
<p>$$
O(L \cdot H \cdot T \cdot d_h)
$$</p>
<p>For long context inference, KV cache memory is often the dominant bottleneck.</p>
<h3 id="7-inference-vs-training-usage">ðŸ“¦7. Inference v/s Training Usage</h3>
<h4 id="71-during-inference">7.1 During Inference</h4>
<p>This is the most common and important usage.</p>
<p><strong>Inference Workflow</strong></p>
<ul>
<li>Encode prompt</li>
<li>Initialize empty KV cache per layer</li>
<li>For each generated token:<ul>
<li>Compute $Q_t, K_t, V_t$</li>
<li>Append $K_t, V_t$ to cache</li>
</ul>
</li>
<li>Compute attention using cached tensors</li>
</ul>
<p><strong>Practical Benefits</strong></p>
<ul>
<li>Faster decoding</li>
<li>Lower FLOPs</li>
<li>Enables long context generation</li>
<li>Essential for streaming and chat systems</li>
</ul>
<h4 id="72-during-training">7.2 During Training</h4>
<p>KV caching is not used in standard full sequence training.</p>
<p><strong>Why?</strong></p>
<ul>
<li>Training processes full sequences in parallel</li>
<li>All tokens attend to each other simultaneously</li>
<li>No repeated computation across steps</li>
</ul>
<hr />
<h3 id="8-scaling-kv-cache-for-long-context">ðŸ“¦8. Scaling KV Cache for Long Context</h3>
<p>Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.</p>
<h4 id="81-sliding-window-attention">8.1 Sliding Window Attention</h4>
<p>Only retain keys and values for the most recent $W$ tokens:</p>
<p>$$
K_{t-W:t}, V_{t-W:t}
$$</p>
<p>This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.</p>
<hr />
<h4 id="82-kv-cache-quantization">8.2 KV Cache Quantization</h4>
<p>KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.</p>
<h4 id="what-gets-quantized">What Gets Quantized</h4>
<p>Both keys and values can be quantized, but they have different sensitivity:</p>
<ul>
<li><strong>Keys (K)</strong> directly affect attention scores $QK^T$</li>
<li><strong>Values (V)</strong> affect the weighted sum after softmax</li>
</ul>
<p>As a result:</p>
<ul>
<li>Keys usually require higher precision</li>
<li>Values tolerate more aggressive quantization</li>
</ul>
<hr />
<h4 id="common-quantization-schemes">Common Quantization Schemes</h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Typical Format</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Keys</td>
<td>FP16 / BF16</td>
<td>Preserves attention score stability</td>
</tr>
<tr>
<td>Values</td>
<td>INT8</td>
<td>Large memory reduction with minimal quality loss</td>
</tr>
<tr>
<td>Both</td>
<td>INT8 or INT4</td>
<td>Used for extreme long context scenarios</td>
</tr>
</tbody>
</table>
<p>Mixed precision KV cache is widely used in practice.</p>
<h4 id="quantization-granularity">Quantization Granularity</h4>
<p>KV cache quantization can be applied at different levels:</p>
<ul>
<li><strong>Per tensor</strong>: One scale for entire K or V tensor</li>
<li><strong>Per head</strong>: Separate scale per attention head</li>
<li><strong>Per channel</strong>: Separate scale per head dimension</li>
</ul>
<p>Finer granularity improves accuracy but increases metadata and compute overhead.</p>
<h4 id="dequantization-during-attention">Dequantization During Attention</h4>
<p>At decoding step $t$:</p>
<ol>
<li>Load quantized $K, V$ from cache</li>
<li>Dequantize to FP16 or BF16</li>
<li>Compute attention normally:</li>
</ol>
<p>$$
\text{softmax}\left(\frac{Q_t K_{1:t}^T}{\sqrt{d_h}}\right)V_{1:t}
$$</p>
<p>Dequantization cost is small compared to memory bandwidth savings.</p>
<h4 id="impact-on-performance">Impact on Performance</h4>
<p>Benefits:</p>
<ul>
<li>2x to 4x KV memory reduction</li>
<li>Higher batch size and longer context</li>
<li>Improved inference throughput due to reduced memory traffic</li>
</ul>
<p>Tradeoffs:</p>
<ul>
<li>Slight loss in generation quality</li>
<li>Additional dequantization overhead</li>
</ul>
<p>In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.</p>
<h4 id="interaction-with-other-optimizations">Interaction with Other Optimizations</h4>
<ul>
<li><strong>GQA</strong> further reduces KV cache size and works well with quantization</li>
<li><strong>Paged KV cache</strong> benefits from smaller KV blocks</li>
<li><strong>FlashAttention</strong> amortizes dequantization overhead inside fused kernels</li>
</ul>
<hr />
<h4 id="83-prefix-caching">8.3 Prefix Caching</h4>
<p>When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.</p>
<hr />
<h4 id="84-paged-kv-cache">8.4 Paged KV Cache</h4>
<p>KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.</p>
<hr />
<h3 id="9-grouped-query-attention-gqa">ðŸ“¦9. Grouped Query Attention (GQA)</h3>
<p>Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.</p>
<h4 id="91-head-configuration">9.1 Head Configuration</h4>
<p>$$
H_q &gt; H_k = H_v
$$</p>
<p>Example:</p>
<ul>
<li>Query heads $H_q = 32$</li>
<li>Key value heads $H_k = 8$</li>
</ul>
<p>This reduces KV cache memory by a factor of $H_q / H_k$.</p>
<h4 id="92-qk-computation-with-mismatched-heads">9.2 QK Computation with Mismatched Heads</h4>
<p>Each key value head is shared by a fixed group of query heads.</p>
<p>Let:</p>
<p>$$
g = \frac{H_q}{H_k}
$$</p>
<p>Each key value head serves $g$ query heads.</p>
<p>For query head $i$, the corresponding key value head index is:</p>
<p>$$
\left\lfloor \frac{i}{g} \right\rfloor
$$</p>
<p>The attention computation becomes:</p>
<p>$$
\text{Attn}<em>i = \text{softmax}\left(\frac{Q_i K</em>{\left\lfloor i/g \right\rfloor}^T}{\sqrt{d_h}}\right)V_{\left\lfloor i/g \right\rfloor}
$$</p>
<p>Keys and values are reused directly without additional projection or averaging.</p>
<h4 id="93-why-gqa-is-effective">9.3 Why GQA Is Effective</h4>
<ul>
<li>Query heads retain expressive power</li>
<li>Keys and values capture shared context</li>
<li>KV cache size and memory bandwidth are significantly reduced</li>
</ul>
<p>GQA is widely used in production LLMs.</p>
<hr />
<h3 id="10-other-common-optimizations">ðŸ“¦10. Other Common Optimizations</h3>
<h4 id="flashattention">FlashAttention</h4>
<p>FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.</p>
<h4 id="chunked-prefill">Chunked Prefill</h4>
<p>Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.</p>
<h4 id="speculative-decoding">Speculative Decoding</h4>
<p>Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
