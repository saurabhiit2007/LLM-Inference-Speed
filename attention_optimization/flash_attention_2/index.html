
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Flash attention 2 - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Flash attention 2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Fundamentals
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/inference_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/bottleneck_analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bottleneck Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/latency_vs_throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latency vs Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/memory_compute_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Compute Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Attention Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Attention Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kv_caching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vllm.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Decoding Strategies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/greedy_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Greedy Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/beam_search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beam Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/sampling_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sampling Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speculative_decoding.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../batching_strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Batching Strategies
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Quantization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Quantization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/quantization_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/int8_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    INT8 Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/int4_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    INT4 Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/gptq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPTQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/gguf_ggml/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GGUF GGML
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/awq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/smoothquant/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SmoothQuant
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/quantization_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Serving Frameworks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Serving Frameworks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/deepspeed_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepSpeed Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/tensorrt_llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TensorRT LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/vllm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/text_generation_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text Generation Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/triton_inference_server/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Triton Inference Server
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-what-was-wrong-with-flashattention-1" class="md-nav__link">
    <span class="md-ellipsis">
      2. What Was Wrong with FlashAttention-1?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. What Was Wrong with FlashAttention-1?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-1-poor-work-partitioning" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 1: Poor Work Partitioning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-2-non-coalesced-memory-accesses" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 2: Non-Coalesced Memory Accesses
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-3-limited-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 3: Limited Parallelism
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-key-improvements-in-flashattention-2" class="md-nav__link">
    <span class="md-ellipsis">
      3. Key Improvements in FlashAttention-2
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Key Improvements in FlashAttention-2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-better-parallelism-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Better Parallelism Strategy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-improved-work-partitioning-within-thread-blocks" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Improved Work Partitioning Within Thread Blocks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-memory-access-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Memory Access Optimizations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-performance-impact" class="md-nav__link">
    <span class="md-ellipsis">
      4. Performance Impact
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Performance Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#speedup-over-flashattention-1" class="md-nav__link">
    <span class="md-ellipsis">
      Speedup Over FlashAttention-1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-utilization" class="md-nav__link">
    <span class="md-ellipsis">
      GPU Utilization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Efficiency
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      5. Implementation Details
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Implementation Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#thread-block-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Thread Block Structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#synchronization" class="md-nav__link">
    <span class="md-ellipsis">
      Synchronization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-interview-questions" class="md-nav__link">
    <span class="md-ellipsis">
      6. Interview Questions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Interview Questions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1-whats-the-main-difference-between-flashattention-1-and-flashattention-2" class="md-nav__link">
    <span class="md-ellipsis">
      Q1: What's the main difference between FlashAttention-1 and FlashAttention-2?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2-does-flashattention-2-change-the-algorithm-or-just-the-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Q2: Does FlashAttention-2 change the algorithm or just the implementation?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3-why-does-parallelizing-across-kv-tiles-improve-performance" class="md-nav__link">
    <span class="md-ellipsis">
      Q3: Why does parallelizing across KV tiles improve performance?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q4-whats-the-trade-off-with-this-increased-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Q4: What's the trade-off with this increased parallelism?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q5-how-does-flashattention-2-handle-the-accumulation-of-partial-results" class="md-nav__link">
    <span class="md-ellipsis">
      Q5: How does FlashAttention-2 handle the accumulation of partial results?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q6-what-gpu-features-does-flashattention-2-rely-on-more-heavily" class="md-nav__link">
    <span class="md-ellipsis">
      Q6: What GPU features does FlashAttention-2 rely on more heavily?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q7-why-doesnt-flashattention-2-achieve-100-of-peak-flops" class="md-nav__link">
    <span class="md-ellipsis">
      Q7: Why doesn't FlashAttention-2 achieve 100% of peak FLOPS?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q8-how-does-sequence-length-affect-fa-2s-speedup-over-fa-1" class="md-nav__link">
    <span class="md-ellipsis">
      Q8: How does sequence length affect FA-2's speedup over FA-1?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q9-can-flashattention-2s-techniques-be-applied-to-other-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Q9: Can FlashAttention-2's techniques be applied to other operations?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q10-whats-the-memory-complexity-of-flashattention-2" class="md-nav__link">
    <span class="md-ellipsis">
      Q10: What's the memory complexity of FlashAttention-2?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-flashattention-1-vs-flashattention-2-summary" class="md-nav__link">
    <span class="md-ellipsis">
      7. FlashAttention-1 vs FlashAttention-2 Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-key-takeaways-for-interviews" class="md-nav__link">
    <span class="md-ellipsis">
      8. Key Takeaways for Interviews
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Flash attention 2</h1>

<h2 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h2>
<p>FlashAttention-2 is an improved version of FlashAttention that achieves <strong>2x speedup</strong> over FlashAttention-1 through better GPU utilization. It maintains the same exact attention computation while being even faster and more efficient.</p>
<p><strong>Key improvement:</strong> Better work partitioning across GPU threads to reduce idle time and maximize hardware utilization.</p>
<hr />
<hr />
<h2 id="2-what-was-wrong-with-flashattention-1">2. What Was Wrong with FlashAttention-1?<a class="headerlink" href="#2-what-was-wrong-with-flashattention-1" title="Permanent link">&para;</a></h2>
<p>Despite being much faster than standard attention, FlashAttention-1 had suboptimal GPU utilization:</p>
<h3 id="problem-1-poor-work-partitioning">Problem 1: Poor Work Partitioning<a class="headerlink" href="#problem-1-poor-work-partitioning" title="Permanent link">&para;</a></h3>
<ul>
<li>Each thread block processed one query tile across all key/value tiles</li>
<li>Led to <strong>unbalanced workload</strong> and thread block idle time</li>
<li>Didn't fully saturate GPU compute resources</li>
</ul>
<hr />
<h3 id="problem-2-non-coalesced-memory-accesses">Problem 2: Non-Coalesced Memory Accesses<a class="headerlink" href="#problem-2-non-coalesced-memory-accesses" title="Permanent link">&para;</a></h3>
<ul>
<li>Memory accesses weren't optimally aligned for GPU memory coalescing</li>
<li>Caused unnecessary memory bandwidth waste</li>
</ul>
<hr />
<h3 id="problem-3-limited-parallelism">Problem 3: Limited Parallelism<a class="headerlink" href="#problem-3-limited-parallelism" title="Permanent link">&para;</a></h3>
<ul>
<li>Parallelism was only across batch, heads, and query sequence</li>
<li>Didn't parallelize across key/value sequence dimension</li>
</ul>
<hr />
<hr />
<h2 id="3-key-improvements-in-flashattention-2">3. Key Improvements in FlashAttention-2<a class="headerlink" href="#3-key-improvements-in-flashattention-2" title="Permanent link">&para;</a></h2>
<h3 id="31-better-parallelism-strategy">3.1 Better Parallelism Strategy<a class="headerlink" href="#31-better-parallelism-strategy" title="Permanent link">&para;</a></h3>
<p><strong>FlashAttention-1:</strong> Parallelize over <code>(batch, heads, query_tiles)</code>
<div class="highlight"><pre><span></span><code>Thread Block 1 → processes Q_tile_1 across all K,V tiles
Thread Block 2 → processes Q_tile_2 across all K,V tiles
</code></pre></div></p>
<p><strong>FlashAttention-2:</strong> Parallelize over <code>(batch, heads, query_tiles, kv_tiles)</code>
<div class="highlight"><pre><span></span><code>Thread Block 1 → processes (Q_tile_1, K_tile_1, V_tile_1)
Thread Block 2 → processes (Q_tile_1, K_tile_2, V_tile_2)
Thread Block 3 → processes (Q_tile_2, K_tile_1, V_tile_1)
</code></pre></div></p>
<p><strong>Benefit:</strong> More thread blocks doing work simultaneously → better GPU occupancy → less idle time</p>
<hr />
<h3 id="32-improved-work-partitioning-within-thread-blocks">3.2 Improved Work Partitioning Within Thread Blocks<a class="headerlink" href="#32-improved-work-partitioning-within-thread-blocks" title="Permanent link">&para;</a></h3>
<p><strong>FlashAttention-1:</strong> Each warp handled different queries within a tile
- Led to imbalanced work when softmax required different amounts of computation</p>
<p><strong>FlashAttention-2:</strong> Each warp handles same query, split across K dimension
- More balanced work distribution
- Better load balancing across warps</p>
<hr />
<h3 id="33-memory-access-optimizations">3.3 Memory Access Optimizations<a class="headerlink" href="#33-memory-access-optimizations" title="Permanent link">&para;</a></h3>
<ul>
<li>Improved memory coalescing patterns</li>
<li>Better cache utilization</li>
<li>Reduced redundant memory loads</li>
</ul>
<hr />
<hr />
<h2 id="4-performance-impact">4. Performance Impact<a class="headerlink" href="#4-performance-impact" title="Permanent link">&para;</a></h2>
<h3 id="speedup-over-flashattention-1">Speedup Over FlashAttention-1<a class="headerlink" href="#speedup-over-flashattention-1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>~2x faster</strong> on average for typical sequence lengths</li>
<li>Up to <strong>2.3x</strong> on A100 GPUs for long sequences</li>
<li>Better scaling with sequence length</li>
</ul>
<h3 id="gpu-utilization">GPU Utilization<a class="headerlink" href="#gpu-utilization" title="Permanent link">&para;</a></h3>
<ul>
<li>FlashAttention-1: ~35-50% of peak FLOPS</li>
<li>FlashAttention-2: ~50-70% of peak FLOPS</li>
</ul>
<h3 id="memory-efficiency">Memory Efficiency<a class="headerlink" href="#memory-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li>Same <span class="arithmatex">\(O(N \cdot B)\)</span> memory complexity</li>
<li>Better bandwidth utilization due to improved access patterns</li>
</ul>
<hr />
<hr />
<h2 id="5-implementation-details">5. Implementation Details<a class="headerlink" href="#5-implementation-details" title="Permanent link">&para;</a></h2>
<h3 id="thread-block-structure">Thread Block Structure<a class="headerlink" href="#thread-block-structure" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Conceptual partitioning</span>
<span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">q_tile_idx</span> <span class="ow">in</span> <span class="n">query_tiles</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">kv_tile_idx</span> <span class="ow">in</span> <span class="n">kv_tiles</span><span class="p">:</span>  <span class="c1"># NEW: also parallelize here</span>
                <span class="c1"># Each (q_tile, kv_tile) pair gets its own thread block</span>
                <span class="n">thread_block</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">q_tile_idx</span><span class="p">],</span> <span class="n">K</span><span class="p">[</span><span class="n">kv_tile_idx</span><span class="p">],</span> <span class="n">V</span><span class="p">[</span><span class="n">kv_tile_idx</span><span class="p">])</span>
                <span class="c1"># Accumulate partial results</span>
</code></pre></div>
<hr />
<h3 id="synchronization">Synchronization<a class="headerlink" href="#synchronization" title="Permanent link">&para;</a></h3>
<ul>
<li>Requires careful synchronization when accumulating partial outputs</li>
<li>Uses atomic operations or reduction trees to combine results from different KV tiles</li>
</ul>
<hr />
<hr />
<h2 id="6-interview-questions">6. Interview Questions<a class="headerlink" href="#6-interview-questions" title="Permanent link">&para;</a></h2>
<h3 id="q1-whats-the-main-difference-between-flashattention-1-and-flashattention-2">Q1: What's the main difference between FlashAttention-1 and FlashAttention-2?<a class="headerlink" href="#q1-whats-the-main-difference-between-flashattention-1-and-flashattention-2" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> FlashAttention-2 improves <strong>parallelism</strong> by also parallelizing across the key/value sequence dimension, not just query sequence. This means more thread blocks work simultaneously, reducing idle time and achieving ~2x speedup while computing the exact same result.</p>
<hr />
<h3 id="q2-does-flashattention-2-change-the-algorithm-or-just-the-implementation">Q2: Does FlashAttention-2 change the algorithm or just the implementation?<a class="headerlink" href="#q2-does-flashattention-2-change-the-algorithm-or-just-the-implementation" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> It's purely an <strong>implementation improvement</strong>. The algorithm (tiling, online softmax, kernel fusion) remains the same. FlashAttention-2 just distributes work more efficiently across GPU threads to maximize hardware utilization.</p>
<hr />
<h3 id="q3-why-does-parallelizing-across-kv-tiles-improve-performance">Q3: Why does parallelizing across KV tiles improve performance?<a class="headerlink" href="#q3-why-does-parallelizing-across-kv-tiles-improve-performance" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> In FlashAttention-1, each thread block processes one Q tile sequentially across all KV tiles. This limits parallelism. FlashAttention-2 launches separate thread blocks for each (Q_tile, KV_tile) pair, enabling many more blocks to run concurrently, better saturating the GPU's compute resources.</p>
<hr />
<h3 id="q4-whats-the-trade-off-with-this-increased-parallelism">Q4: What's the trade-off with this increased parallelism?<a class="headerlink" href="#q4-whats-the-trade-off-with-this-increased-parallelism" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> More synchronization overhead. Since multiple thread blocks now compute partial outputs for the same query tile (from different KV tiles), we need to carefully accumulate and normalize these partial results. However, the performance gain from parallelism far outweighs this cost.</p>
<hr />
<h3 id="q5-how-does-flashattention-2-handle-the-accumulation-of-partial-results">Q5: How does FlashAttention-2 handle the accumulation of partial results?<a class="headerlink" href="#q5-how-does-flashattention-2-handle-the-accumulation-of-partial-results" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Each thread block computes a partial attention output for its (Q_tile, KV_tile) pair along with partial softmax statistics (max, sum). These partials are then combined using:
- Atomic operations, or
- Reduction trees, or
- Final pass to accumulate stored partials</p>
<p>The online softmax technique ensures correct normalization.</p>
<hr />
<h3 id="q6-what-gpu-features-does-flashattention-2-rely-on-more-heavily">Q6: What GPU features does FlashAttention-2 rely on more heavily?<a class="headerlink" href="#q6-what-gpu-features-does-flashattention-2-rely-on-more-heavily" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong>
- <strong>High thread block occupancy</strong> - needs many concurrent blocks
- <strong>Fast atomic operations</strong> - for accumulating partials
- <strong>Shared memory bandwidth</strong> - still crucial like FA-1
- <strong>Warp-level primitives</strong> - for efficient intra-block communication</p>
<p>Modern GPUs (A100, H100) have better support for these, maximizing FA-2's benefits.</p>
<hr />
<h3 id="q7-why-doesnt-flashattention-2-achieve-100-of-peak-flops">Q7: Why doesn't FlashAttention-2 achieve 100% of peak FLOPS?<a class="headerlink" href="#q7-why-doesnt-flashattention-2-achieve-100-of-peak-flops" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Several factors:
- Memory bandwidth still matters (can't fully hide all memory latency)
- Synchronization overhead from accumulating partials
- Load imbalance across thread blocks (some finish before others)
- Non-uniform work per tile (softmax computation varies)</p>
<p>50-70% utilization is actually quite good for memory-intensive operations.</p>
<hr />
<h3 id="q8-how-does-sequence-length-affect-fa-2s-speedup-over-fa-1">Q8: How does sequence length affect FA-2's speedup over FA-1?<a class="headerlink" href="#q8-how-does-sequence-length-affect-fa-2s-speedup-over-fa-1" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> FlashAttention-2's advantage <strong>increases</strong> with sequence length:
- Longer sequences → more tiles → more parallelism opportunities
- Better amortization of synchronization overhead
- At very short sequences, FA-1 and FA-2 are similar (not enough parallelism to exploit)</p>
<hr />
<h3 id="q9-can-flashattention-2s-techniques-be-applied-to-other-operations">Q9: Can FlashAttention-2's techniques be applied to other operations?<a class="headerlink" href="#q9-can-flashattention-2s-techniques-be-applied-to-other-operations" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Yes! The key insight—parallelizing over both input and computation dimensions—applies to any operation that processes tiles/blocks:
- Other attention variants (sparse, local attention)
- Convolutions with tiling
- Matrix multiplications with block processing</p>
<p>The principle is: maximize parallel work to reduce GPU idle time.</p>
<hr />
<h3 id="q10-whats-the-memory-complexity-of-flashattention-2">Q10: What's the memory complexity of FlashAttention-2?<a class="headerlink" href="#q10-whats-the-memory-complexity-of-flashattention-2" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Same as FlashAttention-1: <strong><span class="arithmatex">\(O(N \cdot B)\)</span></strong> where <span class="arithmatex">\(B\)</span> is tile size. The improvement is in speed (better parallelism and memory access patterns), not memory usage. Both avoid materializing the full <span class="arithmatex">\(N \times N\)</span> attention matrix.</p>
<hr />
<hr />
<h2 id="7-flashattention-1-vs-flashattention-2-summary">7. FlashAttention-1 vs FlashAttention-2 Summary<a class="headerlink" href="#7-flashattention-1-vs-flashattention-2-summary" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>FlashAttention-1</th>
<th>FlashAttention-2</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Parallelism</strong></td>
<td>Over batch, heads, query tiles</td>
<td>Over batch, heads, query tiles, <strong>KV tiles</strong></td>
</tr>
<tr>
<td><strong>Work per block</strong></td>
<td>One Q tile × all KV tiles</td>
<td>One (Q tile, KV tile) pair</td>
</tr>
<tr>
<td><strong>GPU utilization</strong></td>
<td>35-50% of peak FLOPS</td>
<td>50-70% of peak FLOPS</td>
</tr>
<tr>
<td><strong>Speedup vs standard</strong></td>
<td>2-4x</td>
<td>4-8x</td>
</tr>
<tr>
<td><strong>Speedup vs FA-1</strong></td>
<td>-</td>
<td>~2x</td>
</tr>
<tr>
<td><strong>Memory complexity</strong></td>
<td><span class="arithmatex">\(O(N \cdot B)\)</span></td>
<td><span class="arithmatex">\(O(N \cdot B)\)</span></td>
</tr>
<tr>
<td><strong>Algorithm</strong></td>
<td>Tiling + online softmax + fusion</td>
<td>Same</td>
</tr>
<tr>
<td><strong>Synchronization</strong></td>
<td>Simpler</td>
<td>More complex (atomic/reduction)</td>
</tr>
</tbody>
</table>
<hr />
<hr />
<h2 id="8-key-takeaways-for-interviews">8. Key Takeaways for Interviews<a class="headerlink" href="#8-key-takeaways-for-interviews" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Main idea:</strong> Same algorithm, better parallelism by also parallelizing across KV dimension</li>
<li><strong>Performance:</strong> ~2x faster than FA-1 through better GPU utilization</li>
<li><strong>Trade-off:</strong> More synchronization overhead, but worth it for the speedup</li>
<li><strong>Memory:</strong> Same <span class="arithmatex">\(O(N \cdot B)\)</span> complexity, just faster execution</li>
<li><strong>When it matters most:</strong> Long sequences where more parallelism can be exploited</li>
<li><strong>Hardware dependency:</strong> Benefits scale with GPU's ability to run many thread blocks concurrently</li>
</ol>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>