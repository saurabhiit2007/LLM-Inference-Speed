
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Paged attention - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Paged attention
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Fundamentals
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/inference_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/bottleneck_analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bottleneck Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/latency_vs_throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latency vs Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/memory_compute_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Compute Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Attention Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Attention Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kv_caching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vllm.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Decoding Strategies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/greedy_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Greedy Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/beam_search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beam Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/sampling_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sampling Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speculative_decoding.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../batching_strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Batching Strategies
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Quantization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Quantization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/quantization_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/int8_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    INT8 Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/int4_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    INT4 Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/gptq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPTQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/awq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/smoothquant/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SmoothQuant
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/gguf_ggml/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GGUF GGML
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/quantization_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Serving Frameworks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Serving Frameworks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/tensorrt_llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TensorRT LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/text_generation_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text Generation Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/deepspeed_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepSpeed Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/triton_inference_server/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Triton Inference Server
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/vllm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/framework_comparison/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Framework Comparison
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-kv-cache-memory-problem" class="md-nav__link">
    <span class="md-ellipsis">
      2. The KV Cache Memory Problem
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The KV Cache Memory Problem">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      What is KV Cache?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problems-with-traditional-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      Problems with Traditional KV Cache
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Problems with Traditional KV Cache">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-1-memory-fragmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 1: Memory Fragmentation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-2-no-memory-sharing" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 2: No Memory Sharing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-3-static-allocation" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 3: Static Allocation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-how-pagedattention-works" class="md-nav__link">
    <span class="md-ellipsis">
      3. How PagedAttention Works
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. How PagedAttention Works">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-core-concept-paging" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Core Concept: Paging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-block-table" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Block Table
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-dynamic-allocation" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Dynamic Allocation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-memory-sharing-via-copy-on-write" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Memory Sharing via Copy-on-Write
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-attention-computation-with-paging" class="md-nav__link">
    <span class="md-ellipsis">
      4. Attention Computation with Paging
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-performance-impact" class="md-nav__link">
    <span class="md-ellipsis">
      5. Performance Impact
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Performance Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Efficiency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#throughput-improvement" class="md-nav__link">
    <span class="md-ellipsis">
      Throughput Improvement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latency" class="md-nav__link">
    <span class="md-ellipsis">
      Latency
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-interview-questions" class="md-nav__link">
    <span class="md-ellipsis">
      6. Interview Questions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Interview Questions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1-what-problem-does-pagedattention-solve" class="md-nav__link">
    <span class="md-ellipsis">
      Q1: What problem does PagedAttention solve?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2-how-is-pagedattention-similar-to-os-virtual-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Q2: How is PagedAttention similar to OS virtual memory?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3-whats-a-typical-block-size-and-why" class="md-nav__link">
    <span class="md-ellipsis">
      Q3: What's a typical block size and why?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q4-how-does-pagedattention-enable-memory-sharing" class="md-nav__link">
    <span class="md-ellipsis">
      Q4: How does PagedAttention enable memory sharing?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q5-whats-the-overhead-of-block-table-lookups" class="md-nav__link">
    <span class="md-ellipsis">
      Q5: What's the overhead of block table lookups?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q6-how-does-pagedattention-improve-throughput" class="md-nav__link">
    <span class="md-ellipsis">
      Q6: How does PagedAttention improve throughput?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q7-what-happens-when-we-run-out-of-physical-blocks" class="md-nav__link">
    <span class="md-ellipsis">
      Q7: What happens when we run out of physical blocks?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q8-can-pagedattention-work-with-flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      Q8: Can PagedAttention work with FlashAttention?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q9-whats-the-difference-between-block-size-and-tile-size" class="md-nav__link">
    <span class="md-ellipsis">
      Q9: What's the difference between block size and tile size?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q10-what-are-the-limitations-of-pagedattention" class="md-nav__link">
    <span class="md-ellipsis">
      Q10: What are the limitations of PagedAttention?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-pagedattention-in-practice-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      7. PagedAttention in Practice (vLLM)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. PagedAttention in Practice (vLLM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-cases-where-it-shines" class="md-nav__link">
    <span class="md-ellipsis">
      Use Cases Where It Shines
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-key-takeaways-for-interviews" class="md-nav__link">
    <span class="md-ellipsis">
      8. Key Takeaways for Interviews
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-comparison-table" class="md-nav__link">
    <span class="md-ellipsis">
      9. Comparison Table
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Paged attention</h1>

<h2 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h2>
<p>PagedAttention is a memory management technique for efficient LLM serving that stores KV cache in non-contiguous memory blocks (pages). It's the core innovation behind vLLM, enabling <strong>near-zero waste</strong> in KV cache memory and higher throughput.</p>
<p><strong>Key insight:</strong> Treat KV cache like virtual memory in operating systems—use paging to eliminate fragmentation and enable flexible memory sharing.</p>
<hr />
<hr />
<h2 id="2-the-kv-cache-memory-problem">2. The KV Cache Memory Problem<a class="headerlink" href="#2-the-kv-cache-memory-problem" title="Permanent link">&para;</a></h2>
<h3 id="what-is-kv-cache">What is KV Cache?<a class="headerlink" href="#what-is-kv-cache" title="Permanent link">&para;</a></h3>
<p>In autoregressive generation, transformers reuse Key and Value tensors from previous tokens:
- Without cache: Recompute K, V for all previous tokens at each step (wasteful)
- With cache: Store K, V tensors and only compute for new token</p>
<p>For a sequence of length <span class="arithmatex">\(N\)</span> with <span class="arithmatex">\(L\)</span> layers, <span class="arithmatex">\(H\)</span> heads, and dimension <span class="arithmatex">\(d\)</span>:
$$
\text{KV cache size} = 2 \times N \times L \times H \times d
$$</p>
<p><strong>Example:</strong> LLaMA-13B with 2048 tokens ≈ 800 MB per sequence</p>
<hr />
<h3 id="problems-with-traditional-kv-cache">Problems with Traditional KV Cache<a class="headerlink" href="#problems-with-traditional-kv-cache" title="Permanent link">&para;</a></h3>
<h4 id="problem-1-memory-fragmentation">Problem 1: Memory Fragmentation<a class="headerlink" href="#problem-1-memory-fragmentation" title="Permanent link">&para;</a></h4>
<p><strong>Issue:</strong> Must pre-allocate contiguous memory for maximum sequence length</p>
<div class="highlight"><pre><span></span><code>Sequence 1: [████████░░░░] (800 tokens, allocated for 2048)
Sequence 2: [███░░░░░░░░░] (300 tokens, allocated for 2048)
Sequence 3: [██████░░░░░░] (600 tokens, allocated for 2048)
</code></pre></div>
<ul>
<li>Actual usage: 1700 tokens</li>
<li>Allocated: 6144 slots (3 × 2048)</li>
<li><strong>Waste: 72% of memory unused!</strong></li>
</ul>
<hr />
<h4 id="problem-2-no-memory-sharing">Problem 2: No Memory Sharing<a class="headerlink" href="#problem-2-no-memory-sharing" title="Permanent link">&para;</a></h4>
<ul>
<li>Cannot share KV cache across sequences (even with identical prompts)</li>
<li>Parallel sampling requires duplicating entire cache</li>
<li>Beam search creates multiple copies</li>
</ul>
<hr />
<h4 id="problem-3-static-allocation">Problem 3: Static Allocation<a class="headerlink" href="#problem-3-static-allocation" title="Permanent link">&para;</a></h4>
<ul>
<li>Must allocate for worst case (max sequence length)</li>
<li>Can't dynamically adjust based on actual needs</li>
<li>Limits batch size and throughput</li>
</ul>
<hr />
<hr />
<h2 id="3-how-pagedattention-works">3. How PagedAttention Works<a class="headerlink" href="#3-how-pagedattention-works" title="Permanent link">&para;</a></h2>
<h3 id="31-core-concept-paging">3.1 Core Concept: Paging<a class="headerlink" href="#31-core-concept-paging" title="Permanent link">&para;</a></h3>
<p>Divide KV cache into fixed-size <strong>blocks</strong> (pages), similar to OS virtual memory:</p>
<div class="highlight"><pre><span></span><code>Logical sequence: [Token 0, Token 1, ..., Token N]
                         ↓
Physical memory:  [Block 0] → [Block 5] → [Block 2] (non-contiguous)
</code></pre></div>
<p><strong>Key properties:</strong>
- Block size: Typically 16-64 tokens
- Blocks can be anywhere in physical memory
- Mapping tracked via <strong>block table</strong> (like OS page table)</p>
<hr />
<h3 id="32-block-table">3.2 Block Table<a class="headerlink" href="#32-block-table" title="Permanent link">&para;</a></h3>
<p>Each sequence has a block table mapping logical blocks to physical blocks:</p>
<div class="highlight"><pre><span></span><code>Sequence 1:
  Logical Block 0 → Physical Block 3
  Logical Block 1 → Physical Block 7
  Logical Block 2 → Physical Block 1

Sequence 2:
  Logical Block 0 → Physical Block 3  (shared with Seq 1!)
  Logical Block 1 → Physical Block 9
</code></pre></div>
<hr />
<h3 id="33-dynamic-allocation">3.3 Dynamic Allocation<a class="headerlink" href="#33-dynamic-allocation" title="Permanent link">&para;</a></h3>
<p>Blocks are allocated <strong>on-demand</strong> as sequences grow:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Conceptual allocation</span>
<span class="k">def</span> <span class="nf">generate_token</span><span class="p">(</span><span class="n">sequence</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">sequence</span><span class="o">.</span><span class="n">last_block_is_full</span><span class="p">():</span>
        <span class="n">new_block</span> <span class="o">=</span> <span class="n">allocate_free_block</span><span class="p">()</span>
        <span class="n">sequence</span><span class="o">.</span><span class="n">block_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_block</span><span class="p">)</span>

    <span class="c1"># Compute attention using block table</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">paged_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">sequence</span><span class="o">.</span><span class="n">block_table</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p><strong>Benefits:</strong>
- Only allocate what's actually used
- No pre-allocation for max length
- Memory freed immediately when sequence completes</p>
<hr />
<h3 id="34-memory-sharing-via-copy-on-write">3.4 Memory Sharing via Copy-on-Write<a class="headerlink" href="#34-memory-sharing-via-copy-on-write" title="Permanent link">&para;</a></h3>
<p>Multiple sequences can share blocks (read-only):</p>
<div class="highlight"><pre><span></span><code>Prompt: &quot;Translate to French: &quot;
         ↓
[Block 0: &quot;Translate to French: &quot;] ← Shared by all sequences

Seq 1: [Block 0] → [Block 3: &quot;Hello → &quot;]
Seq 2: [Block 0] → [Block 5: &quot;Goodbye → &quot;]
</code></pre></div>
<p>When modifying a shared block → <strong>copy-on-write</strong>:
1. Allocate new physical block
2. Copy contents
3. Update block table
4. Modify the copy</p>
<hr />
<hr />
<h2 id="4-attention-computation-with-paging">4. Attention Computation with Paging<a class="headerlink" href="#4-attention-computation-with-paging" title="Permanent link">&para;</a></h2>
<p>Standard attention accesses KV cache contiguously. PagedAttention accesses via block table:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Simplified PagedAttention</span>
<span class="k">def</span> <span class="nf">paged_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">block_table</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">logical_idx</span><span class="p">,</span> <span class="n">physical_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">block_table</span><span class="p">):</span>
        <span class="c1"># Fetch K, V from physical block</span>
        <span class="n">K_block</span> <span class="o">=</span> <span class="n">K_blocks</span><span class="p">[</span><span class="n">physical_idx</span><span class="p">]</span>
        <span class="n">V_block</span> <span class="o">=</span> <span class="n">V_blocks</span><span class="p">[</span><span class="n">physical_idx</span><span class="p">]</span>

        <span class="c1"># Compute attention for this block</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K_block</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">V_block</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p><strong>Key insight:</strong> The indirection (block table lookup) has minimal overhead compared to memory savings.</p>
<hr />
<hr />
<h2 id="5-performance-impact">5. Performance Impact<a class="headerlink" href="#5-performance-impact" title="Permanent link">&para;</a></h2>
<h3 id="memory-efficiency">Memory Efficiency<a class="headerlink" href="#memory-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Traditional:</strong> 20-40% KV cache utilization (60-80% waste)</li>
<li><strong>PagedAttention:</strong> 80-95% utilization (5-20% waste)</li>
<li><strong>2-3x more sequences</strong> in same memory</li>
</ul>
<h3 id="throughput-improvement">Throughput Improvement<a class="headerlink" href="#throughput-improvement" title="Permanent link">&para;</a></h3>
<ul>
<li>vLLM with PagedAttention: <strong>2-4x higher throughput</strong> vs traditional serving</li>
<li>Batch size limited by memory → bigger batches with less waste</li>
</ul>
<h3 id="latency">Latency<a class="headerlink" href="#latency" title="Permanent link">&para;</a></h3>
<ul>
<li>Minimal overhead from block table lookups (&lt;5%)</li>
<li>Often better latency due to higher batch efficiency</li>
</ul>
<hr />
<hr />
<h2 id="6-interview-questions">6. Interview Questions<a class="headerlink" href="#6-interview-questions" title="Permanent link">&para;</a></h2>
<h3 id="q1-what-problem-does-pagedattention-solve">Q1: What problem does PagedAttention solve?<a class="headerlink" href="#q1-what-problem-does-pagedattention-solve" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> PagedAttention solves memory fragmentation and waste in KV cache management. Traditional approaches pre-allocate contiguous memory for max sequence length, wasting 60-80% of memory. PagedAttention uses non-contiguous blocks allocated on-demand, achieving 80-95% utilization and enabling 2-4x higher throughput.</p>
<hr />
<h3 id="q2-how-is-pagedattention-similar-to-os-virtual-memory">Q2: How is PagedAttention similar to OS virtual memory?<a class="headerlink" href="#q2-how-is-pagedattention-similar-to-os-virtual-memory" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Both use paging:
- <strong>Virtual memory:</strong> Maps virtual addresses to physical pages via page table
- <strong>PagedAttention:</strong> Maps logical KV cache positions to physical blocks via block table</p>
<p>Both enable non-contiguous allocation, on-demand paging, and copy-on-write sharing.</p>
<hr />
<h3 id="q3-whats-a-typical-block-size-and-why">Q3: What's a typical block size and why?<a class="headerlink" href="#q3-whats-a-typical-block-size-and-why" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Typically <strong>16-64 tokens</strong>. Trade-offs:
- <strong>Too small:</strong> High block table overhead, more lookups during attention
- <strong>Too large:</strong> Internal fragmentation (wasted space within partially-filled blocks)
- <strong>Sweet spot:</strong> 16-64 balances overhead vs. fragmentation (similar to OS page sizes like 4KB)</p>
<hr />
<h3 id="q4-how-does-pagedattention-enable-memory-sharing">Q4: How does PagedAttention enable memory sharing?<a class="headerlink" href="#q4-how-does-pagedattention-enable-memory-sharing" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Multiple sequences can point to the same physical blocks (read-only). Common use cases:
- <strong>Shared prompts:</strong> All sequences share blocks containing the same prompt
- <strong>Parallel sampling:</strong> Multiple outputs from same prompt share prefix blocks
- <strong>Beam search:</strong> Different beams share common prefix</p>
<p>When a shared block needs modification → <strong>copy-on-write</strong>: allocate new block, copy contents, update that sequence's block table.</p>
<hr />
<h3 id="q5-whats-the-overhead-of-block-table-lookups">Q5: What's the overhead of block table lookups?<a class="headerlink" href="#q5-whats-the-overhead-of-block-table-lookups" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Minimal (&lt;5% typically) because:
- Block tables are small (fits in cache)
- Lookups are simple integer indexing
- Attention computation dominates (matrix ops on blocks)
- Modern GPUs handle indirection efficiently</p>
<p>The memory savings far outweigh this small overhead.</p>
<hr />
<h3 id="q6-how-does-pagedattention-improve-throughput">Q6: How does PagedAttention improve throughput?<a class="headerlink" href="#q6-how-does-pagedattention-improve-throughput" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> By reducing memory waste:
1. Traditional: Can fit 10 sequences (60% waste)
2. PagedAttention: Can fit 25 sequences (10% waste) in same memory
3. Bigger batches → better GPU utilization → higher throughput</p>
<p>Typical improvement: <strong>2-4x</strong> more requests/second.</p>
<hr />
<h3 id="q7-what-happens-when-we-run-out-of-physical-blocks">Q7: What happens when we run out of physical blocks?<a class="headerlink" href="#q7-what-happens-when-we-run-out-of-physical-blocks" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Memory management strategies:
- <strong>Preemption:</strong> Evict lower-priority sequences, save their state
- <strong>Swapping:</strong> Move blocks to CPU memory (like OS swap)
- <strong>Recomputation:</strong> Drop blocks and recompute if needed
- <strong>Blocking:</strong> Wait until blocks free up</p>
<p>vLLM typically uses preemption for fairness and efficiency.</p>
<hr />
<h3 id="q8-can-pagedattention-work-with-flashattention">Q8: Can PagedAttention work with FlashAttention?<a class="headerlink" href="#q8-can-pagedattention-work-with-flashattention" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Yes! They're complementary:
- <strong>FlashAttention:</strong> Optimizes attention computation (tiling, kernel fusion)
- <strong>PagedAttention:</strong> Optimizes KV cache memory management (paging, sharing)</p>
<p>You can use both together: FlashAttention for fast computation, PagedAttention for efficient memory. vLLM does exactly this.</p>
<hr />
<h3 id="q9-whats-the-difference-between-block-size-and-tile-size">Q9: What's the difference between block size and tile size?<a class="headerlink" href="#q9-whats-the-difference-between-block-size-and-tile-size" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong>
- <strong>Block size (PagedAttention):</strong> Memory management granularity (16-64 tokens)
  - Determines allocation unit for KV cache storage
- <strong>Tile size (FlashAttention):</strong> Computation granularity (128-256 tokens)
  - Determines how much data loads into shared memory at once</p>
<p>They're independent concepts operating at different levels (memory management vs computation).</p>
<hr />
<h3 id="q10-what-are-the-limitations-of-pagedattention">Q10: What are the limitations of PagedAttention?<a class="headerlink" href="#q10-what-are-the-limitations-of-pagedattention" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong>
- <strong>Complexity:</strong> More complex implementation than contiguous allocation
- <strong>Indirection overhead:</strong> Small cost from block table lookups
- <strong>GPU kernel changes:</strong> Requires custom attention kernels that understand block tables
- <strong>Internal fragmentation:</strong> Last block in sequence may be partially empty</p>
<p>Despite these, benefits (2-4x throughput) far outweigh costs for LLM serving.</p>
<hr />
<hr />
<h2 id="7-pagedattention-in-practice-vllm">7. PagedAttention in Practice (vLLM)<a class="headerlink" href="#7-pagedattention-in-practice-vllm" title="Permanent link">&para;</a></h2>
<h3 id="key-features">Key Features<a class="headerlink" href="#key-features" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># vLLM with PagedAttention</span>
<span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-2-7b&quot;</span><span class="p">)</span>

<span class="c1"># Automatic memory management</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
<span class="c1"># - Blocks allocated on-demand</span>
<span class="c1"># - Shared prompts reuse blocks</span>
<span class="c1"># - Memory freed automatically</span>
</code></pre></div>
<hr />
<h3 id="use-cases-where-it-shines">Use Cases Where It Shines<a class="headerlink" href="#use-cases-where-it-shines" title="Permanent link">&para;</a></h3>
<p>✅ High-throughput serving (many concurrent requests)<br />
✅ Long sequences (less pre-allocation waste)<br />
✅ Parallel sampling / beam search (shared prefixes)<br />
✅ Shared system prompts across requests  </p>
<p>❌ Single-sequence inference (no sharing benefits)<br />
❌ Very short sequences (overhead not amortized)  </p>
<hr />
<hr />
<h2 id="8-key-takeaways-for-interviews">8. Key Takeaways for Interviews<a class="headerlink" href="#8-key-takeaways-for-interviews" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Main idea:</strong> Treat KV cache like OS virtual memory—use paging for efficient, flexible allocation</li>
<li><strong>Problem solved:</strong> Memory fragmentation (60-80% waste → 5-20% waste)</li>
<li><strong>Mechanism:</strong> Block table maps logical positions to physical memory blocks</li>
<li><strong>Sharing:</strong> Copy-on-write enables multiple sequences to share read-only blocks</li>
<li><strong>Performance:</strong> 2-4x throughput improvement in LLM serving</li>
<li><strong>Complementary:</strong> Works alongside FlashAttention (computation vs memory optimization)</li>
</ol>
<hr />
<hr />
<h2 id="9-comparison-table">9. Comparison Table<a class="headerlink" href="#9-comparison-table" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Traditional KV Cache</th>
<th>PagedAttention</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Allocation</strong></td>
<td>Contiguous, pre-allocated</td>
<td>Non-contiguous, on-demand</td>
</tr>
<tr>
<td><strong>Memory waste</strong></td>
<td>60-80%</td>
<td>5-20%</td>
</tr>
<tr>
<td><strong>Max sequences</strong></td>
<td>Limited by pre-allocation</td>
<td>2-4x more in same memory</td>
</tr>
<tr>
<td><strong>Sharing</strong></td>
<td>No sharing</td>
<td>Copy-on-write sharing</td>
</tr>
<tr>
<td><strong>Complexity</strong></td>
<td>Simple</td>
<td>More complex</td>
</tr>
<tr>
<td><strong>Overhead</strong></td>
<td>None</td>
<td>&lt;5% (block lookups)</td>
</tr>
<tr>
<td><strong>Throughput</strong></td>
<td>Baseline</td>
<td>2-4x higher</td>
</tr>
</tbody>
</table>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>