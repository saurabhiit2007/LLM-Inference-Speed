
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A technical reference for LoRA, QLoRA, and other fine-tuning techniques.">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../flash_attention_2/">
      
      
        <link rel="next" href="../paged_attention/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>KV Caching - Fine-Tuning Methods Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-self-attention-recap" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-header__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Fine-Tuning Methods Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              KV Caching
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Fine-Tuning Methods Documentation" class="md-nav__button md-logo" aria-label="Fine-Tuning Methods Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Fine-Tuning Methods Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Finetuning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Fundamentals
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/inference_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/bottleneck_analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bottleneck Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/latency_vs_throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latency vs Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fundamentals/memory_compute_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Compute Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Attention Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Attention Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flash_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flash_attention_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flash Attention 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    KV Caching
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-self-attention-recap" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦1. Self Attention Recap
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-why-kv-cache-is-needed" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦2. Why KV Cache Is Needed
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-kv-cache-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦3. KV Cache Mechanism
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-toy-example" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦4. Toy Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-complexity-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦5. Complexity Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦5. Complexity Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#notation" class="md-nav__link">
    <span class="md-ellipsis">
      Notation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#without-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      Without KV Cache
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      With KV Cache
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-memory-cost" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦6. Memory Cost
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-inference-vs-training-usage" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦7. Inference v/s Training Usage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦7. Inference v/s Training Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-during-inference" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 During Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-during-training" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 During Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-scaling-kv-cache-for-long-context" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦8. Scaling KV Cache for Long Context
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦8. Scaling KV Cache for Long Context">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-sliding-window-attention" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Sliding Window Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-kv-cache-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 KV Cache Quantization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-gets-quantized" class="md-nav__link">
    <span class="md-ellipsis">
      What Gets Quantized
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-quantization-schemes" class="md-nav__link">
    <span class="md-ellipsis">
      Common Quantization Schemes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantization-granularity" class="md-nav__link">
    <span class="md-ellipsis">
      Quantization Granularity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dequantization-during-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Dequantization During Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#impact-on-performance" class="md-nav__link">
    <span class="md-ellipsis">
      Impact on Performance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interaction-with-other-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      Interaction with Other Optimizations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-prefix-caching" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Prefix Caching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#84-paged-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      8.4 Paged KV Cache
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦9. Grouped Query Attention (GQA)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦9. Grouped Query Attention (GQA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-head-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 Head Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-qk-computation-with-mismatched-heads" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 QK Computation with Mismatched Heads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-why-gqa-is-effective" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 Why GQA Is Effective
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-other-common-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦10. Other Common Optimizations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦10. Other Common Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      FlashAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chunked-prefill" class="md-nav__link">
    <span class="md-ellipsis">
      Chunked Prefill
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speculative-decoding" class="md-nav__link">
    <span class="md-ellipsis">
      Speculative Decoding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../paged_attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paged Attention
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Decoding Strategies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/greedy_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Greedy Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/beam_search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Beam Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/sampling_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sampling Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding_strategies/speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../batching_strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Batching Strategies
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Quantization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Quantization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/quantization_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/int8_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    INT8 Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/int4_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    INT4 Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/gptq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPTQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/awq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/smoothquant/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SmoothQuant
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/gguf_ggml/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GGUF GGML
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantization/quantization_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Serving Frameworks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Serving Frameworks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/tensorrt_llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TensorRT LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/text_generation_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text Generation Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/deepspeed_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepSpeed Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/triton_inference_server/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Triton Inference Server
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/vllm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../serving_frameworks/framework_comparison/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Framework Comparison
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-self-attention-recap" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦1. Self Attention Recap
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-why-kv-cache-is-needed" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦2. Why KV Cache Is Needed
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-kv-cache-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦3. KV Cache Mechanism
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-toy-example" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦4. Toy Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-complexity-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦5. Complexity Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦5. Complexity Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#notation" class="md-nav__link">
    <span class="md-ellipsis">
      Notation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#without-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      Without KV Cache
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      With KV Cache
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-memory-cost" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦6. Memory Cost
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-inference-vs-training-usage" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦7. Inference v/s Training Usage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦7. Inference v/s Training Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-during-inference" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 During Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-during-training" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 During Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-scaling-kv-cache-for-long-context" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦8. Scaling KV Cache for Long Context
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦8. Scaling KV Cache for Long Context">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-sliding-window-attention" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Sliding Window Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-kv-cache-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 KV Cache Quantization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-gets-quantized" class="md-nav__link">
    <span class="md-ellipsis">
      What Gets Quantized
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-quantization-schemes" class="md-nav__link">
    <span class="md-ellipsis">
      Common Quantization Schemes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantization-granularity" class="md-nav__link">
    <span class="md-ellipsis">
      Quantization Granularity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dequantization-during-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Dequantization During Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#impact-on-performance" class="md-nav__link">
    <span class="md-ellipsis">
      Impact on Performance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interaction-with-other-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      Interaction with Other Optimizations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-prefix-caching" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Prefix Caching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#84-paged-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      8.4 Paged KV Cache
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦9. Grouped Query Attention (GQA)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦9. Grouped Query Attention (GQA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-head-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 Head Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-qk-computation-with-mismatched-heads" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 QK Computation with Mismatched Heads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-why-gqa-is-effective" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 Why GQA Is Effective
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-other-common-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      ðŸ“¦10. Other Common Optimizations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ðŸ“¦10. Other Common Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      FlashAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chunked-prefill" class="md-nav__link">
    <span class="md-ellipsis">
      Chunked Prefill
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speculative-decoding" class="md-nav__link">
    <span class="md-ellipsis">
      Speculative Decoding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>KV Caching</h1>

<h3 id="1-self-attention-recap">ðŸ“¦1. Self Attention Recap<a class="headerlink" href="#1-self-attention-recap" title="Permanent link">&para;</a></h3>
<p>Given hidden states <span class="arithmatex">\(X \in \mathbb{R}^{T \times d}\)</span>:</p>
<div class="arithmatex">\[
Q = XW_Q,\quad K = XW_K,\quad V = XW_V
\]</div>
<p>Per head attention:</p>
<div class="arithmatex">\[
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_h}}\right)V
\]</div>
<p>Autoregressive decoding generates one token at a time with causal masking.</p>
<hr />
<h3 id="2-why-kv-cache-is-needed">ðŸ“¦2. Why KV Cache Is Needed<a class="headerlink" href="#2-why-kv-cache-is-needed" title="Permanent link">&para;</a></h3>
<p>At decoding step <span class="arithmatex">\(t\)</span>, keys and values for tokens <span class="arithmatex">\(1 \ldots t-1\)</span> are unchanged but would be recomputed without caching.</p>
<p>This repeated computation dominates inference latency and wastes FLOPs.</p>
<hr />
<h3 id="3-kv-cache-mechanism">ðŸ“¦3. KV Cache Mechanism<a class="headerlink" href="#3-kv-cache-mechanism" title="Permanent link">&para;</a></h3>
<p>For each transformer layer <span class="arithmatex">\(\ell\)</span>:</p>
<div class="arithmatex">\[
\text{KVCache}_\ell = \{K_\ell^{1:t}, V_\ell^{1:t}\}
\]</div>
<p>At decoding step <span class="arithmatex">\(t\)</span>:</p>
<ul>
<li>Compute <span class="arithmatex">\(Q_t, K_t, V_t\)</span></li>
<li>Append <span class="arithmatex">\(K_t, V_t\)</span> to the cache</li>
<li>Attend over all cached keys and values</li>
</ul>
<div class="arithmatex">\[
\text{Attn}_t = \text{softmax}\left(\frac{Q_t K_{1:t}^T}{\sqrt{d_h}}\right)V_{1:t}
\]</div>
<p>Only the current token requires new computation.</p>
<hr />
<h3 id="4-toy-example">ðŸ“¦4. Toy Example<a class="headerlink" href="#4-toy-example" title="Permanent link">&para;</a></h3>
<p>Prompt: "I like neural"</p>
<p>Step 1: generate <code>"networks"</code></p>
<ul>
<li>Compute and cache keys and values for the prompt</li>
<li>Attend to all cached tokens</li>
</ul>
<p>Step 2: generate <code>"models"</code></p>
<ul>
<li>Reuse cached keys and values</li>
<li>Compute keys and values only for <code>"networks"</code></li>
</ul>
<p>Previously generated tokens are never recomputed.</p>
<hr />
<h3 id="5-complexity-analysis">ðŸ“¦5. Complexity Analysis<a class="headerlink" href="#5-complexity-analysis" title="Permanent link">&para;</a></h3>
<h4 id="notation">Notation<a class="headerlink" href="#notation" title="Permanent link">&para;</a></h4>
<ul>
<li><span class="arithmatex">\(T\)</span>: number of generated tokens</li>
<li><span class="arithmatex">\(L\)</span>: number of transformer layers</li>
<li><span class="arithmatex">\(H\)</span>: number of attention heads</li>
<li><span class="arithmatex">\(d_h\)</span>: head dimension</li>
</ul>
<h4 id="without-kv-cache">Without KV Cache<a class="headerlink" href="#without-kv-cache" title="Permanent link">&para;</a></h4>
<p>At each decoding step, attention is recomputed for all previous tokens:</p>
<div class="arithmatex">\[
O(L \cdot H \cdot d_h \cdot T^3)
\]</div>
<h4 id="with-kv-cache">With KV Cache<a class="headerlink" href="#with-kv-cache" title="Permanent link">&para;</a></h4>
<p>Only attention against cached keys and values is computed:</p>
<div class="arithmatex">\[
O(L \cdot H \cdot d_h \cdot T^2)
\]</div>
<p>KV caching removes one full factor of <span class="arithmatex">\(T\)</span> from decoding complexity.</p>
<hr />
<h3 id="6-memory-cost">ðŸ“¦6. Memory Cost<a class="headerlink" href="#6-memory-cost" title="Permanent link">&para;</a></h3>
<p>Each layer stores:</p>
<div class="arithmatex">\[
K, V \in \mathbb{R}^{H \times T \times d_h}
\]</div>
<p>Total KV cache memory across all layers:</p>
<div class="arithmatex">\[
O(L \cdot H \cdot T \cdot d_h)
\]</div>
<p>For long context inference, KV cache memory is often the dominant bottleneck.</p>
<h3 id="7-inference-vs-training-usage">ðŸ“¦7. Inference v/s Training Usage<a class="headerlink" href="#7-inference-vs-training-usage" title="Permanent link">&para;</a></h3>
<h4 id="71-during-inference">7.1 During Inference<a class="headerlink" href="#71-during-inference" title="Permanent link">&para;</a></h4>
<p>This is the most common and important usage.</p>
<p><strong>Inference Workflow</strong></p>
<ul>
<li>Encode prompt</li>
<li>Initialize empty KV cache per layer</li>
<li>For each generated token:<ul>
<li>Compute <span class="arithmatex">\(Q_t, K_t, V_t\)</span></li>
<li>Append <span class="arithmatex">\(K_t, V_t\)</span> to cache</li>
</ul>
</li>
<li>Compute attention using cached tensors</li>
</ul>
<p><strong>Practical Benefits</strong></p>
<ul>
<li>Faster decoding</li>
<li>Lower FLOPs</li>
<li>Enables long context generation</li>
<li>Essential for streaming and chat systems</li>
</ul>
<h4 id="72-during-training">7.2 During Training<a class="headerlink" href="#72-during-training" title="Permanent link">&para;</a></h4>
<p>KV caching is not used in standard full sequence training.</p>
<p><strong>Why?</strong></p>
<ul>
<li>Training processes full sequences in parallel</li>
<li>All tokens attend to each other simultaneously</li>
<li>No repeated computation across steps</li>
</ul>
<hr />
<h3 id="8-scaling-kv-cache-for-long-context">ðŸ“¦8. Scaling KV Cache for Long Context<a class="headerlink" href="#8-scaling-kv-cache-for-long-context" title="Permanent link">&para;</a></h3>
<p>Long context inference is primarily limited by KV cache memory, which grows linearly with sequence length.</p>
<h4 id="81-sliding-window-attention">8.1 Sliding Window Attention<a class="headerlink" href="#81-sliding-window-attention" title="Permanent link">&para;</a></h4>
<p>Only retain keys and values for the most recent <span class="arithmatex">\(W\)</span> tokens:</p>
<div class="arithmatex">\[
K_{t-W:t}, V_{t-W:t}
\]</div>
<p>This bounds memory usage and is commonly used in streaming and chat applications. Older context is no longer directly accessible.</p>
<hr />
<h4 id="82-kv-cache-quantization">8.2 KV Cache Quantization<a class="headerlink" href="#82-kv-cache-quantization" title="Permanent link">&para;</a></h4>
<p>KV cache quantization reduces memory usage and memory bandwidth by storing cached keys and values in lower precision formats. This is especially important for long context inference, where KV cache memory dominates total GPU usage.</p>
<h4 id="what-gets-quantized">What Gets Quantized<a class="headerlink" href="#what-gets-quantized" title="Permanent link">&para;</a></h4>
<p>Both keys and values can be quantized, but they have different sensitivity:</p>
<ul>
<li><strong>Keys (K)</strong> directly affect attention scores <span class="arithmatex">\(QK^T\)</span></li>
<li><strong>Values (V)</strong> affect the weighted sum after softmax</li>
</ul>
<p>As a result:</p>
<ul>
<li>Keys usually require higher precision</li>
<li>Values tolerate more aggressive quantization</li>
</ul>
<hr />
<h4 id="common-quantization-schemes">Common Quantization Schemes<a class="headerlink" href="#common-quantization-schemes" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Typical Format</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Keys</td>
<td>FP16 / BF16</td>
<td>Preserves attention score stability</td>
</tr>
<tr>
<td>Values</td>
<td>INT8</td>
<td>Large memory reduction with minimal quality loss</td>
</tr>
<tr>
<td>Both</td>
<td>INT8 or INT4</td>
<td>Used for extreme long context scenarios</td>
</tr>
</tbody>
</table>
<p>Mixed precision KV cache is widely used in practice.</p>
<h4 id="quantization-granularity">Quantization Granularity<a class="headerlink" href="#quantization-granularity" title="Permanent link">&para;</a></h4>
<p>KV cache quantization can be applied at different levels:</p>
<ul>
<li><strong>Per tensor</strong>: One scale for entire K or V tensor</li>
<li><strong>Per head</strong>: Separate scale per attention head</li>
<li><strong>Per channel</strong>: Separate scale per head dimension</li>
</ul>
<p>Finer granularity improves accuracy but increases metadata and compute overhead.</p>
<h4 id="dequantization-during-attention">Dequantization During Attention<a class="headerlink" href="#dequantization-during-attention" title="Permanent link">&para;</a></h4>
<p>At decoding step <span class="arithmatex">\(t\)</span>:</p>
<ol>
<li>Load quantized <span class="arithmatex">\(K, V\)</span> from cache</li>
<li>Dequantize to FP16 or BF16</li>
<li>Compute attention normally:</li>
</ol>
<div class="arithmatex">\[
\text{softmax}\left(\frac{Q_t K_{1:t}^T}{\sqrt{d_h}}\right)V_{1:t}
\]</div>
<p>Dequantization cost is small compared to memory bandwidth savings.</p>
<h4 id="impact-on-performance">Impact on Performance<a class="headerlink" href="#impact-on-performance" title="Permanent link">&para;</a></h4>
<p>Benefits:</p>
<ul>
<li>2x to 4x KV memory reduction</li>
<li>Higher batch size and longer context</li>
<li>Improved inference throughput due to reduced memory traffic</li>
</ul>
<p>Tradeoffs:</p>
<ul>
<li>Slight loss in generation quality</li>
<li>Additional dequantization overhead</li>
</ul>
<p>In practice, value quantization has minimal impact on quality, while aggressive key quantization requires careful tuning.</p>
<h4 id="interaction-with-other-optimizations">Interaction with Other Optimizations<a class="headerlink" href="#interaction-with-other-optimizations" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>GQA</strong> further reduces KV cache size and works well with quantization</li>
<li><strong>Paged KV cache</strong> benefits from smaller KV blocks</li>
<li><strong>FlashAttention</strong> amortizes dequantization overhead inside fused kernels</li>
</ul>
<hr />
<h4 id="83-prefix-caching">8.3 Prefix Caching<a class="headerlink" href="#83-prefix-caching" title="Permanent link">&para;</a></h4>
<p>When multiple requests share a common prompt prefix, the KV cache for that prefix is computed once and reused across requests. This improves throughput in serving systems with templated prompts.</p>
<hr />
<h4 id="84-paged-kv-cache">8.4 Paged KV Cache<a class="headerlink" href="#84-paged-kv-cache" title="Permanent link">&para;</a></h4>
<p>KV cache blocks can be moved between GPU and CPU or NVMe memory. This enables extremely long context lengths while trading off additional latency for cache paging.</p>
<hr />
<h3 id="9-grouped-query-attention-gqa">ðŸ“¦9. Grouped Query Attention (GQA)<a class="headerlink" href="#9-grouped-query-attention-gqa" title="Permanent link">&para;</a></h3>
<p>Grouped Query Attention reduces KV cache size by using fewer key value heads than query heads.</p>
<h4 id="91-head-configuration">9.1 Head Configuration<a class="headerlink" href="#91-head-configuration" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
H_q &gt; H_k = H_v
\]</div>
<p>Example:</p>
<ul>
<li>Query heads <span class="arithmatex">\(H_q = 32\)</span></li>
<li>Key value heads <span class="arithmatex">\(H_k = 8\)</span></li>
</ul>
<p>This reduces KV cache memory by a factor of <span class="arithmatex">\(H_q / H_k\)</span>.</p>
<h4 id="92-qk-computation-with-mismatched-heads">9.2 QK Computation with Mismatched Heads<a class="headerlink" href="#92-qk-computation-with-mismatched-heads" title="Permanent link">&para;</a></h4>
<p>Each key value head is shared by a fixed group of query heads.</p>
<p>Let:</p>
<div class="arithmatex">\[
g = \frac{H_q}{H_k}
\]</div>
<p>Each key value head serves <span class="arithmatex">\(g\)</span> query heads.</p>
<p>For query head <span class="arithmatex">\(i\)</span>, the corresponding key value head index is:</p>
<div class="arithmatex">\[
\left\lfloor \frac{i}{g} \right\rfloor
\]</div>
<p>The attention computation becomes:</p>
<div class="arithmatex">\[
\text{Attn}_i = \text{softmax}\left(\frac{Q_i K_{\left\lfloor i/g \right\rfloor}^T}{\sqrt{d_h}}\right)V_{\left\lfloor i/g \right\rfloor}
\]</div>
<p>Keys and values are reused directly without additional projection or averaging.</p>
<h4 id="93-why-gqa-is-effective">9.3 Why GQA Is Effective<a class="headerlink" href="#93-why-gqa-is-effective" title="Permanent link">&para;</a></h4>
<ul>
<li>Query heads retain expressive power</li>
<li>Keys and values capture shared context</li>
<li>KV cache size and memory bandwidth are significantly reduced</li>
</ul>
<p>GQA is widely used in production LLMs.</p>
<hr />
<h3 id="10-other-common-optimizations">ðŸ“¦10. Other Common Optimizations<a class="headerlink" href="#10-other-common-optimizations" title="Permanent link">&para;</a></h3>
<h4 id="flashattention">FlashAttention<a class="headerlink" href="#flashattention" title="Permanent link">&para;</a></h4>
<p>FlashAttention optimizes the attention kernel to reduce memory reads and improve numerical stability. It is complementary to KV caching and often used together.</p>
<h4 id="chunked-prefill">Chunked Prefill<a class="headerlink" href="#chunked-prefill" title="Permanent link">&para;</a></h4>
<p>Long prompts are processed in chunks to incrementally build the KV cache. This avoids GPU out of memory errors during prefill.</p>
<h4 id="speculative-decoding">Speculative Decoding<a class="headerlink" href="#speculative-decoding" title="Permanent link">&para;</a></h4>
<p>Both draft and target models maintain KV caches. When draft tokens are accepted, the target model reuses its cached keys and values, avoiding recomputation and increasing decoding throughput.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>