<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Vllm - My Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Welcome to MkDocs</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Decoding</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../decoding/decoding_strategies/" class="dropdown-item">Decoding strategies</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Memory</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../memory/kv_caching/" class="dropdown-item">Kv caching</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Speed</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../flash_attention/" class="dropdown-item">Flash Attention</a>
</li>
                                    
<li>
    <a href="../speculative_decoding/" class="dropdown-item">Speculative decoding</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">Vllm</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../speculative_decoding/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" class="nav-link disabled">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="2"><a href="#1-overview" class="nav-link">1. Overview</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#2-the-kv-cache-bottleneck-in-llm-inference" class="nav-link">2. The KV Cache Bottleneck in LLM Inference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#3-the-core-technology-pagedattention" class="nav-link">3. The Core Technology: PagedAttention</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#4-scheduling-continuous-batching" class="nav-link">4. Scheduling: Continuous Batching</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#5-modern-2025-2026-advanced-features" class="nav-link">5. Modern (2025-2026) Advanced Features</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#6-prefill-vs-decode-performance-characteristics" class="nav-link">6. Prefill vs Decode: Performance Characteristics</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#8-memory-pressure-and-preemption" class="nav-link">8. Memory Pressure and Preemption</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#9-architectural-comparison" class="nav-link">9. Architectural Comparison</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#10-useful-memory-approximation" class="nav-link">10. Useful Memory Approximation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-bs-level="2"><a href="#q-as" class="nav-link">Q &amp; As</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h2 id="1-overview">1. Overview</h2>
<p><strong>vLLM</strong> is an open-source inference engine optimized for high-throughput, low-latency serving of large language models. Its core innovation, <strong>PagedAttention</strong>, rethinks KV cache management by applying principles similar to virtual memory systems in operating systems.</p>
<p>The key contribution of vLLM is not faster kernels alone, but <strong>dramatically improved GPU memory utilization</strong>, enabling higher batch sizes, better multi-tenancy, and more predictable latency under load.</p>
<hr />
<hr />
<h2 id="2-the-kv-cache-bottleneck-in-llm-inference">2. The KV Cache Bottleneck in LLM Inference</h2>
<h3 id="21-why-the-kv-cache-dominates">2.1 Why the KV Cache Dominates</h3>
<p>During autoregressive decoding, each generated token appends <strong>Key and Value tensors</strong> for every transformer layer. Over long sequences or many concurrent requests, the KV cache quickly becomes the dominant consumer of GPU memory.</p>
<p>Key properties:
- Grows linearly with sequence length
- Must be retained across decoding steps
- Is memory-bandwidth bound during decode</p>
<hr />
<hr />
<h2 id="3-the-core-technology-pagedattention">3. The Core Technology: PagedAttention</h2>
<p>The KV cache (the memory storing previous tokens to predict the next one) is the primary bottleneck in scaling LLMs.</p>
<h3 id="the-problem-fragmentation">The Problem: Fragmentation</h3>
<p>Standard inference engines allocate a contiguous "max-length" chunk of memory for every request.</p>
<ul>
<li><strong>Over-reservation:</strong> Reserving 2048 tokens for a request that only generates 50.</li>
<li><strong>Internal Fragmentation:</strong> Memory wasted inside the reserved block.</li>
<li><strong>Waste:</strong> Up to 60-80% of VRAM is often left unused but "reserved."</li>
</ul>
<h3 id="the-solution-pagedattention">The Solution: PagedAttention</h3>
<p>vLLM breaks the KV cache into fixed-size <strong>blocks</strong> (pages).</p>
<ul>
<li><strong>Logical Blocks:</strong> Sequential tokens in the prompt.</li>
<li><strong>Physical Blocks:</strong> Non-contiguous memory addresses on the GPU.</li>
<li><strong>Block Table:</strong> A mapping system that allows the model to access these blocks as if they were one continuous string.</li>
</ul>
<p><strong>Result:</strong> Waste is reduced to <strong>&lt;4%</strong>, allowing for significantly larger batch sizes.</p>
<hr />
<hr />
<h2 id="4-scheduling-continuous-batching">4. Scheduling: Continuous Batching</h2>
<p>Traditional engines use "Static Batching," where the entire batch must finish before new requests start.</p>
<ul>
<li><strong>Iteration-Level Scheduling:</strong> vLLM schedules at the level of individual iterations. </li>
<li><strong>Mechanism:</strong> As soon as one sequence in a batch hits an <code>&lt;EOS&gt;</code> (End of Sentence) token, a new request from the queue is inserted into its spot in the next iteration.</li>
<li><strong>Outcome:</strong> Eliminates "bubbles" (idle time) in GPU utilization.</li>
</ul>
<hr />
<hr />
<h2 id="5-modern-2025-2026-advanced-features">5. Modern (2025-2026) Advanced Features</h2>
<h3 id="a-speculative-decoding">A. Speculative Decoding</h3>
<p>vLLM implements speculative decoding where a <strong>smaller draft model</strong> (e.g., a 100M parameter model) predicts several tokens, and a <strong>larger target model</strong> (e.g., Llama 3 70B) verifies them in a single pass.
* <strong>Benefit:</strong> Reduces latency by 2-3x for heavy models.</p>
<h3 id="b-automatic-prefix-caching-apc">B. Automatic Prefix Caching (APC)</h3>
<p>For RAG or multi-turn chat, vLLM caches the KV blocks of common prefixes (like system prompts).
* If two users share the same 1,000-token system prompt, vLLM stores it <strong>once</strong> in physical memory, and both requests point to the same blocks.</p>
<h3 id="c-multi-lora-support">C. Multi-LoRA Support</h3>
<p>vLLM can serve one base model with hundreds of different fine-tuned "adapters" (LoRAs) simultaneously. 
* It uses specialized <strong>SGMV (Shrink-Generalized Matrix-Vector)</strong> kernels to compute multiple different LoRAs in a single batch without a significant performance hit.</p>
<hr />
<hr />
<h2 id="6-prefill-vs-decode-performance-characteristics">6. Prefill vs Decode: Performance Characteristics</h2>
<table>
<thead>
<tr>
<th>Phase</th>
<th>Characteristics</th>
<th>Bottleneck</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prefill</td>
<td>Parallel over tokens</td>
<td>Compute-bound</td>
</tr>
<tr>
<td>Decode</td>
<td>Sequential token-by-token</td>
<td>Memory-bandwidth-bound</td>
</tr>
</tbody>
</table>
<p>vLLM introduces <strong>Chunked Prefill</strong> to prevent long prompts from blocking decode for other users.</p>
<hr />
<hr />
<h2 id="8-memory-pressure-and-preemption">8. Memory Pressure and Preemption</h2>
<p>When GPU memory becomes constrained, vLLM supports <strong>preemption</strong> strategies:</p>
<ul>
<li><strong>Swap:</strong> Move KV blocks to CPU memory</li>
<li><strong>Recompute:</strong> Drop KV blocks and recompute them later</li>
</ul>
<p>Recompute trades extra compute for lower memory pressure and is often preferred on fast GPUs.</p>
<hr />
<hr />
<h2 id="9-architectural-comparison">9. Architectural Comparison</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">vLLM</th>
<th style="text-align: left;">Hugging Face TGI</th>
<th style="text-align: left;">NVIDIA TensorRT-LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Memory Mgmt</strong></td>
<td style="text-align: left;">PagedAttention</td>
<td style="text-align: left;">FlashAttention</td>
<td style="text-align: left;">Paged KV Cache</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Ease of Use</strong></td>
<td style="text-align: left;">High (Pythonic)</td>
<td style="text-align: left;">Medium (Rust/Go)</td>
<td style="text-align: left;">Low (Complex Build)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Best For</strong></td>
<td style="text-align: left;">General Throughput</td>
<td style="text-align: left;">Stability/HF ecosystem</td>
<td style="text-align: left;">Peak NVIDIA Perf</td>
</tr>
</tbody>
</table>
<hr />
<hr />
<h2 id="10-useful-memory-approximation">10. Useful Memory Approximation</h2>
<p>Approximate KV cache memory usage in bytes:</p>
<p>$$
\text{KV Memory} \;\approx\; 2 \times L \times T \times H \times D_h \times B
$$</p>
<p>Where:
- 2 accounts for <strong>Keys and Values</strong>
- $L$ is the number of layers<br />
- $H$ is the number of attention heads 
- $B$ is the number of bytes per element (2 for FP16, 1 for FP8)
- $D_h$ is the per-head hidden dimension
- $T$ is the sequence length (number of cached tokens)</p>
<p><strong>Example:</strong></p>
<p>For <strong>Llama-3 8B</strong>:</p>
<ul>
<li>$L = 32$</li>
<li>$D_{\text{model}} = 4096$</li>
<li>$B = 2$ (FP16)</li>
</ul>
<p>Per token KV cache memory:</p>
<p>$$
2 \times 32 \times 4096 \times 2
\approx 524{,}288 \text{ bytes} \approx 0.5 \text{ MB}
$$</p>
<p>Approximate totals:
- 2k tokens → ~1 GB KV cache
- 8k tokens → ~4 GB KV cache</p>
<p>This linear scaling with sequence length explains why <strong>KV cache memory becomes the dominant bottleneck</strong> and motivates techniques such as <strong>PagedAttention</strong> in vLLM.</p>
<hr />
<hr />
<h2 id="q-as">Q &amp; As</h2>
<p><strong>Q1: How does vLLM handle a situation where the GPU runs out of memory during a request?</strong>
* <strong>A:</strong> It uses <strong>Preemption</strong>. It can either "Swap" (move blocks to CPU RAM) or "Recompute" (drop the blocks and re-calculate them later when memory is free).</p>
<p><strong>Q2: What is the difference between the 'Prefill' and 'Decode' phases?</strong>
* <strong>A:</strong> <strong>Prefill</strong> processes the input prompt (parallel/compute-bound). <strong>Decode</strong> generates tokens one by one (sequential/memory-bound). vLLM uses <strong>Chunked Prefill</strong> to prevent large prompts from stalling the generation of other users.</p>
<p><strong>Q3: Why is vLLM better for multi-tenant SaaS?</strong>
* <strong>A:</strong> Because of PagedAttention and Multi-LoRA support. It allows hosting many different "specialized" models on a single GPU cluster with minimal overhead.</p>
<hr /></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
