<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Speculative decoding - My Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Welcome to MkDocs</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Decoding</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../decoding/decoding_strategies/" class="dropdown-item">Decoding strategies</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Memory</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../memory/kv_caching/" class="dropdown-item">Kv caching</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Speed</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../flash_attention/" class="dropdown-item">Flash Attention</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">Speculative decoding</a>
</li>
                                    
<li>
    <a href="../vllm/" class="dropdown-item">Vllm</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../flash_attention/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../vllm/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            
            
            
            
            
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h3 id="1-speculative-decoding-overview">ðŸ“¦1. Speculative Decoding: Overview</h3>
<p>Speculative decoding reduces <strong>inference latency</strong> in decoder-only LLMs while preserving the <strong>exact output distribution</strong> of a large target model.</p>
<h4 id="11-why-standard-decoding-is-slow">1.1 Why standard decoding is slow</h4>
<p>In standard autoregressive decoding:</p>
<ul>
<li>The target model generates <strong>one token per forward pass</strong></li>
<li>The model is large and expensive</li>
<li>Latency grows linearly with output length</li>
</ul>
<p>KV cache reduces computation but does not remove the sequential bottleneck.</p>
<h4 id="12-core-idea">1.2 Core idea</h4>
<p>Speculative decoding separates <strong>token proposal</strong> from <strong>token verification</strong>:</p>
<ul>
<li>A <strong>draft model</strong> proposes multiple tokens cheaply</li>
<li>A <strong>target model</strong> verifies them efficiently</li>
</ul>
<p>If most draft tokens are accepted, multiple tokens are generated per expensive target model forward pass.</p>
<hr />
<h3 id="2-background-how-decoding-works-in-decoder-only-llms">ðŸ“¦2. Background: How Decoding Works in Decoder-Only LLMs</h3>
<p>Before speculative decoding, it is critical to understand standard autoregressive decoding.</p>
<h4 id="21-autoregressive-modeling-assumption">2.1 Autoregressive modeling assumption</h4>
<p>A decoder-only LLM models a sequence of tokens using the following factorization:</p>
<p>$$
P(x_1, x_2, \dots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, \dots, x_{t-1})
$$</p>
<p>Key points:</p>
<ul>
<li>Tokens are generated <strong>left to right</strong></li>
<li>Each new token depends on <strong>all previous tokens</strong></li>
<li>There is no notion of predicting multiple future tokens independently</li>
</ul>
<h4 id="22-what-happens-in-one-forward-pass">2.2 What happens in one forward pass</h4>
<p>Assume the current sequence length is $T$.</p>
<h4 id="step-1-embedding">Step 1: Embedding</h4>
<p>Each token $x_t$ is mapped to a vector representation:</p>
<p>$$
\mathbf{e}_t = \text{TokenEmbed}(x_t) + \text{PosEmbed}(t)
$$</p>
<h4 id="step-2-masked-self-attention">Step 2: Masked self-attention</h4>
<p>For each token position $t$:</p>
<p>$$
\mathbf{q}_t = \mathbf{e}_t W_Q,\quad
\mathbf{k}_t = \mathbf{e}_t W_K,\quad
\mathbf{v}_t = \mathbf{e}_t W_V
$$</p>
<p>Attention scores are computed as:</p>
<p>$$
\alpha_{t,i} = \frac{\mathbf{q}_t \cdot \mathbf{k}_i}{\sqrt{d_k}}
$$</p>
<p>A <strong>causal mask</strong> ensures token $t$ can only attend to tokens $i \le t$.</p>
<p>The attended representation is:</p>
<p>$$
\mathbf{a}<em>t = \sum</em>{i=1}^{t} \text{softmax}(\alpha_{t,i}) \mathbf{v}_i
$$</p>
<p>This is followed by a linear projection:</p>
<p>$$
\mathbf{o}_t = \mathbf{a}_t W_O
$$</p>
<h4 id="step-3-feed-forward-network-ffn">Step 3: Feed Forward Network (FFN)</h4>
<p>Each token is processed independently:</p>
<p>$$
\mathbf{h}_t = \text{FFN}(\mathbf{o}_t)
$$</p>
<p>Residual connections and layer normalization are applied around both attention and FFN blocks.</p>
<p>This completes one decoder layer. The same process repeats across multiple stacked layers.</p>
<h4 id="23-computing-logits-and-selecting-the-next-token">2.3 Computing logits and selecting the next token</h4>
<p>After the final decoder layer, each token position has a hidden state $\mathbf{h}_t$.</p>
<p>These are projected to vocabulary logits:</p>
<p>$$
\mathbf{z}<em>t = \mathbf{h}_t W</em>{\text{vocab}}
\quad \text{where } \mathbf{z}_t \in \mathbb{R}^{|V|}
$$</p>
<p>Important clarification:</p>
<ul>
<li>Logits are computed <strong>for every token position</strong></li>
<li>Softmax is applied <strong>over the vocabulary</strong></li>
<li>During decoding, <strong>only the last position is used</strong></li>
</ul>
<p>$$
P(x_{T+1} \mid x_{\le T}) = \text{softmax}(\mathbf{z}_T)
$$</p>
<p>A token is selected using greedy decoding or sampling and appended to the sequence.</p>
<h4 id="24-autoregressive-decoding-loop">2.4 Autoregressive decoding loop</h4>
<p>For each generated token:</p>
<ol>
<li>Run the Transformer forward pass</li>
<li>Take logits from the last token position</li>
<li>Apply softmax over the vocabulary</li>
<li>Select one token</li>
<li>Append it to the sequence</li>
<li>Repeat</li>
</ol>
<p>This process continues until an end-of-sequence token is produced or a maximum length is reached.</p>
<h4 id="25-key-limitation-of-standard-decoding">2.5 Key limitation of standard decoding</h4>
<ul>
<li>Each generated token requires a new forward pass of the model</li>
<li>Latency grows linearly with output l</li>
</ul>
<h3 id="3-step-by-step-algorithm-for-speculative-decoding">ðŸ“¦3. Step-by-Step Algorithm for Speculative Decoding</h3>
<p>This section describes the speculative decoding algorithm precisely, step by step, focusing on what each model does and why it is needed.</p>
<p>Assume:</p>
<ul>
<li>Prompt tokens: $x$</li>
<li>Draft model: $q$</li>
<li>Target model: $p$</li>
<li>Draft length: $k$</li>
<li>Drafted tokens: $y_1, y_2, \dots, y_k$</li>
</ul>
<h4 id="step-1-draft-model-proposes-tokens">Step 1: Draft model proposes tokens</h4>
<p>The draft model generates tokens autoregressively, starting from the prompt:</p>
<p>$$
y_i \sim q(\cdot \mid x, y_{&lt;i}) \quad \text{for } i = 1 \dots k
$$</p>
<p>Key points:</p>
<ul>
<li>This step is fast because the draft model is small</li>
<li>Tokens are sampled, not greedily selected</li>
<li>The draft model also records the probability of each sampled token</li>
</ul>
<h4 id="step-2-target-model-verifies-the-draft">Step 2: Target model verifies the draft</h4>
<p>The target model is run <strong>once</strong> on the combined sequence: $[x, y1, y2, ..., yk]$</p>
<p>This produces target model probabilities:</p>
<p>$$
p(y_i \mid x, y_{&lt;i}) \quad \text{for } i = 1 \dots k
$$</p>
<p>Important clarification:</p>
<ul>
<li>The target model naturally computes logits for all positions</li>
<li>Only logits corresponding to the drafted tokens are used</li>
<li>Logits for the prompt tokens are ignored</li>
</ul>
<h4 id="step-3-acceptance-test">Step 3: Acceptance test</h4>
<p>Each drafted token is accepted independently using:</p>
<p>$$
\alpha_i = \min\left(1, \frac{p(y_i \mid x, y_{&lt;i})}{q(y_i \mid x, y_{&lt;i})}\right)
$$</p>
<p>Procedure:</p>
<ol>
<li>Sample $u \sim \text{Uniform}(0, 1)$</li>
<li>Accept token $y_i$ if $u \le \alpha_i$</li>
<li>Stop at the first rejected token</li>
</ol>
<h4 id="step-4-rejection-handling-and-fallback">Step 4: Rejection handling and fallback</h4>
<p>If token $y_j$ is rejected:</p>
<ul>
<li>Tokens $y_j, y_{j+1}, \dots, y_k$ are discarded</li>
<li>The next token is sampled directly from the target model:
  $$
  x_{\text{next}} \sim p(\cdot \mid x, y_{&lt;j})
  $$</li>
<li>Speculative decoding restarts from the new prefix</li>
</ul>
<p>If no token is rejected, all $k$ draft tokens are accepted.</p>
<h4 id="why-this-preserves-correctness">Why this preserves correctness</h4>
<ul>
<li>The acceptance rule implements rejection sampling</li>
<li>Bias introduced by the draft model is corrected</li>
<li>The final output distribution exactly matches the target model</li>
</ul>
<p>This guarantees that speculative decoding is statistically equivalent to standard decoding with the target model.</p>
<h3 id="4-why-speculative-decoding-is-faster">ðŸ“¦4. Why Speculative Decoding Is Faster</h3>
<p>Speculative decoding reduces inference latency by decreasing how often the expensive target model must be executed.</p>
<h4 id="41-standard-decoding-vs-speculative-decoding">4.1 Standard decoding vs speculative decoding</h4>
<p><strong>Standard decoding</strong></p>
<ul>
<li>One target model forward pass per generated token</li>
<li>For $N$ tokens, $N$ forward passes are required</li>
</ul>
<p><strong>Speculative decoding</strong></p>
<ul>
<li>The draft model proposes $k$ tokens cheaply</li>
<li>A single target model forward pass verifies up to $k$ tokens</li>
<li>Multiple tokens can be generated per target model invocation</li>
</ul>
<h4 id="42-source-of-the-speedup">4.2 Source of the speedup</h4>
<p>The speedup comes from two properties:</p>
<ul>
<li>The draft model is significantly cheaper than the target model</li>
<li>The target model can evaluate multiple draft tokens in parallel</li>
</ul>
<p>If the acceptance rate is high, the target model is called much less frequently.</p>
<h4 id="43-what-speculative-decoding-does-not-optimize">4.3 What speculative decoding does not optimize</h4>
<p>Speculative decoding does not reduce:</p>
<ul>
<li>The total number of FLOPs in the target model</li>
<li>The per-token computation inside the Transformer</li>
</ul>
<p>It primarily reduces <strong>latency</strong>, not theoretical compute.</p>
<h4 id="44-practical-speedups">4.4 Practical speedups</h4>
<p>In practice, speculative decoding often achieves:</p>
<ul>
<li>1.5x to 3x latency improvement</li>
<li>Higher gains when draft and target distributions are close</li>
</ul>
<p>Actual speedup depends on model sizes, hardware, and acceptance rate.</p>
<h3 id="5-relationship-to-logits-for-all-tokens">5. Relationship to Logits for All Tokens</h3>
<p>Speculative decoding does not introduce a new requirement to compute logits for all tokens.</p>
<h4 id="51-standard-transformer-behavior">5.1 Standard Transformer behavior</h4>
<p>A Transformer forward pass naturally produces:</p>
<ul>
<li>One hidden state per token position</li>
<li>One vocabulary logits vector per token position</li>
</ul>
<p>This is true during both training and inference.</p>
<h4 id="52-how-logits-are-used-in-speculative-decoding">5.2 How logits are used in speculative decoding</h4>
<p>During verification:</p>
<ul>
<li>The target model is run once on the prompt plus draft tokens</li>
<li>Logits for prompt tokens are ignored</li>
<li>Only logits corresponding to the draft token positions are used</li>
</ul>
<p>Speculative decoding simply reuses standard per-position logits.</p>
<h4 id="53-what-speculative-decoding-does-not-do">5.3 What speculative decoding does not do</h4>
<p>Speculative decoding does not:</p>
<ul>
<li>Perform softmax over token positions</li>
<li>Predict future tokens independently</li>
<li>Generate tokens in parallel from the target model</li>
</ul>
<p>The target model still defines an autoregressive distribution.</p>
<h3 id="6-interaction-with-kv-cache">6. Interaction with KV Cache</h3>
<p>KV cache improves performance in speculative decoding but does not change the algorithm.</p>
<h4 id="61-kv-cache-in-the-draft-model">6.1 KV cache in the draft model</h4>
<ul>
<li>The draft model maintains its own KV cache</li>
<li>Draft tokens are generated autoregressively</li>
<li>KV cache allows fast token proposal</li>
</ul>
<h4 id="62-kv-cache-in-the-target-model">6.2 KV cache in the target model</h4>
<ul>
<li>The target model computes KV cache for the entire speculative window</li>
<li>KV states corresponding to accepted tokens are reused</li>
<li>KV states for rejected tokens are discarded</li>
</ul>
<h4 id="63-why-kv-cache-matters">6.3 Why KV cache matters</h4>
<p>KV cache:</p>
<ul>
<li>Avoids recomputing attention for previously processed tokens</li>
<li>Reduces per-step computation</li>
<li>Improves practical throughput</li>
</ul>
<p>KV cache affects efficiency only, not correctness.</p>
<hr /></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
